{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142493e9",
   "metadata": {
    "id": "142493e9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.integrate\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import diffrax\n",
    "from jax.scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d59cf27-de74-4dbd-846d-ad7dcfd915bd",
   "metadata": {
    "id": "0d59cf27-de74-4dbd-846d-ad7dcfd915bd"
   },
   "outputs": [],
   "source": [
    "import jaxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9P6weRnbrVZ8",
   "metadata": {
    "id": "9P6weRnbrVZ8"
   },
   "source": [
    "define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb2960e-c42a-4f33-b134-0449f9b1bb1e",
   "metadata": {
    "id": "5cb2960e-c42a-4f33-b134-0449f9b1bb1e",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class SP_XY_Network:\n",
    "    \n",
    "    '''\n",
    "    This is aimed to implement a neural network consisting of XY oscillators with bias term.\n",
    "    We only define the network. The datat need to be got from outside.\n",
    "\n",
    "\n",
    "    Terms: \n",
    "        data: original data\n",
    "        phase: a configuration of every cells\n",
    "    \n",
    "    ===========================================\n",
    "    \n",
    "    Variables: \n",
    "    \n",
    "        Parameters: \n",
    "        \n",
    "            (set in __init__())\n",
    "            N: number of neurons (including input and output cells)\n",
    "            N_ev: maximum number of updating when looking for the equilibrium (free and with cost function)\n",
    "            N_epoch: maximum number of iteration when doing gradient descent for the weights\n",
    "            dt: time step for evolution to find equilibrium\n",
    "            tol_eq: tolerance for searcheng for the equilibrium\n",
    "            tol_W: tolerance for optimizing weights\n",
    "            N_input: number of input cells\n",
    "            N_output: number of output cells\n",
    "            \n",
    "            (set in get_training/test_data)\n",
    "            N_sample: number of samples for training\n",
    "            N_test: number of data sets fot validation test\n",
    "            \n",
    "            (set to be 0 in __init__ and updated when getting data)\n",
    "            wieghts_0: initiate value for couplings\n",
    "            phase_0: initiate configuration of all cells\n",
    "            \n",
    "        --------------------------\n",
    "            \n",
    "        External Data:\n",
    "        \n",
    "            (set in __init__())\n",
    "            input_index: indices of input cells\n",
    "            output_index: indices of output cells\n",
    "            \n",
    "            (set to be 0 in __init__(), get from get_xxx_data)\n",
    "            training_data: data to input cells for training\n",
    "            test_data: data to input cells for validation test\n",
    "            \n",
    "            training_target: data to output cells for training\n",
    "            target_output: desired output of output cells, used for validation tests\n",
    "            \n",
    "        ---------------------------\n",
    "        \n",
    "        Internal Variables:\n",
    "            \n",
    "            weights: weights, couplings between cells. N x N\n",
    "            bias: set to be [[h0,h1,h2...h_N],[psi_0,psi_1,psi_2...psi_N]]. \n",
    "            h: strength of local field\n",
    "            psi: direction of the local field \n",
    "            \n",
    "            equi_free: configuration at free equilibrium (all cells). Used for training\n",
    "            equi_nudge: configuration at total equilibrium (all cells). Used for training\n",
    "            test_result_phase: phase figuration for output in validation test (all cells)\n",
    "            test_result_data: data to output cells for validation test (output cells)\n",
    "            \n",
    "            input_data: ...\n",
    "            output_data: ...\n",
    "            \n",
    "            validity_training: ......\n",
    "            validity_test: ......\n",
    "            \n",
    "            -----------------------\n",
    "        \n",
    "        Functions: \n",
    "            \n",
    "            Setting the Network: \n",
    "                \n",
    "                __init__: set the number of cells, determine input and output cells, set all data to be zero\n",
    "                get_training_data(input_data, output_data): input the training data, prepare for the training stage\n",
    "                get_test_data(input_data, target_output): \n",
    "                random_state_initiation(): initiate the phase_0 and weights_0 randomly for the cells other than input and output cells\n",
    "                \n",
    "            -----------------------\n",
    "            \n",
    "            Calculation of Internal Properties:\n",
    "            \n",
    "                internal_energy(self,...): calculate the internal energy: E=1/2 \\sum_{ij} W_{ij} cos(\\phi_i-\\phi_j)\n",
    "                bias_term(self,...): calculate the bias term\n",
    "                internal_energy(self,...): internal_energy+bias_term\n",
    "                \n",
    "                sinlge_cost(self,phase,target_phase): calculate the cost function for single phases\n",
    "                total_energy(self,...): internal_energy+single_cost\n",
    "                cost_function(self, output, target_output): calculate cost function for outputs and target outputs\n",
    "                \n",
    "                internal_force: gradient of free term of internal energy\n",
    "                bias_force: gradient of bias term of internal energy\n",
    "                cost_force: ......\n",
    "                internal_force: internal_force+bias_force\n",
    "                total_force: internal_force+cost_force\n",
    "                \n",
    "                ...force45: used for 4th Runge Kutte method\n",
    "                \n",
    "                weights_gradient(self,...): calculate the gradient for training\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #------------- Setting the system ------------------\n",
    "    \n",
    "    def __init__(self, N, N_ev, dt, input_index, output_index):\n",
    "        \n",
    "        # Setting parameters from the input\n",
    "        self.N=N\n",
    "        self.N_ev=N_ev\n",
    "        self.dt=dt\n",
    "        self.input_index=input_index\n",
    "        self.output_index=output_index\n",
    "        \n",
    "        self.variable_index = np.delete(np.arange(0,N), input_index)\n",
    "        \n",
    "        self.N_input=len(input_index)\n",
    "        self.N_output=len(output_index)\n",
    "        self.T=self.dt*self.N_ev\n",
    "        \n",
    "        # Initalize all the other parameters. Default values are all zero\n",
    "        self.N_sample=0\n",
    "        self.N_test=0\n",
    "        \n",
    "        self.weights=np.zeros([N,N])\n",
    "        self.weights_0=np.zeros([N,N])\n",
    "\n",
    "        self.bias=np.zeros([2,N])\n",
    "        self.bias_0=np.zeros([2,N])\n",
    "        \n",
    "        self.beta=0.001\n",
    "        \n",
    "    #---------------------------Initiation Module: initiate the network---------------------------------\n",
    "        \n",
    "    def get_initial_state(self, weights_0,phase_0,bias_0):\n",
    "        # Set weights_0 and phase_0 manually\n",
    "        \n",
    "        self.weights_0=weights_0\n",
    "        self.weights=weights_0\n",
    "        self.phase_0=phase_0\n",
    "        self.bias=bias_0\n",
    "        self.bias_0=bias_0\n",
    "\n",
    "        \n",
    "    def random_state_initiation(self):\n",
    "        # Randomly set the weights and the phase_0 and bias\n",
    "        \n",
    "        # Set weights, ~N(0,1)\n",
    "        self.weights_0=np.random.randn(self.N, self.N)\n",
    "        for k in range(0,self.N):\n",
    "            self.weights_0[k,k]=0\n",
    "        self.weights_0=(self.weights_0+np.transpose(self.weights_0))/2\n",
    "        \n",
    "        self.weights=self.weights_0\n",
    "        \n",
    "        #Set bias, ~U(-0.5,0.5),U(-pi,pi)\n",
    "        bias=np.random.rand(2,self.N)\n",
    "        bias[0,:]=bias[0,:]-0.5\n",
    "        bias[1,:]=2*np.pi*(bias[1,:]-0.5)\n",
    "        self.bias_0=bias\n",
    "        self.bias=bias\n",
    "\n",
    "        # Set phase_0\n",
    "        self.phase_0=np.pi*np.random.rand(self.N)-np.pi/2*np.ones(self.N)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_beta(self,beta):\n",
    "        \n",
    "        self.beta=beta\n",
    "        \n",
    "    #--------------------------Energy Module: Calculate the energy and cost functions--------------------------\n",
    "    ''' \n",
    "        Functions here are design to simultaneousle deal with a set of phase. \n",
    "        The value will be returned and used for other calculations such as assess the reliability of the system.  \n",
    "    '''\n",
    "    def dphase(self,phase):\n",
    "        # Calculate dphase[i,j]=phase[i]-phase[j]\n",
    "        \n",
    "        aux_ones=np.ones(self.N)\n",
    "        phase_mat=np.tensordot(aux_ones,phase,0)\n",
    "        phase_i=np.transpose(phase_mat,(1,2,0))\n",
    "        phase_j=np.transpose(phase_i,(0,2,1))\n",
    "        dphase=phase_i-phase_j\n",
    "        \n",
    "        return dphase\n",
    "\n",
    "    def internal_energy(self,W,phase):\n",
    "        # Calculate free term of internal energy for a set of phase\n",
    "        # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "        dphase=self.dphase(phase)\n",
    "        E_list=0.5*np.sum(W*np.cos(dphase),(1,2))\n",
    "        \n",
    "        return E_list\n",
    "    \n",
    "    def bias_term(self,bias,phase):\n",
    "        # Calculate bias term of internal energy for a set of phase\n",
    "        # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input. \n",
    "        \n",
    "        h=bias[0,:]\n",
    "        psi=bias[1,:]\n",
    "        N_data=np.shape(phase)[0]\n",
    "\n",
    "        aux_ones=np.ones(N_data)\n",
    "        psi_mat=np.tensordot(aux_ones,psi,0)\n",
    "        E_list=np.sum(h*np.cos(phase-psi_mat),axis=1)\n",
    "        return E_list\n",
    "    \n",
    "    def cost_function(self,phase,target):\n",
    "        # Calculate the cost function for each sample\n",
    "        # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "        doutput=phase[:,self.output_index]-target\n",
    "        cost_mat=np.ones(np.shape(doutput))-np.cos(1*doutput)\n",
    "        cost_list=np.sum(cost_mat,1)\n",
    "        \n",
    "        return cost_list\n",
    "\n",
    "    def total_energy(self, W, bias, phase, target):\n",
    "\n",
    "        return self.internal_energy(W,phase)+self.bias_term(bias,phase)+self.cost_function(phase,target)\n",
    "        \n",
    "    #----------------------Force Module: Calculate the force induced by the energy for evolution----------------------------\n",
    "    \n",
    "    def internal_force(self,W,phase):\n",
    "        # Calculate the force induced by the free term of energy\n",
    "        # Here is a sign problem\n",
    "        \n",
    "        dphase=self.dphase(phase)\n",
    "        F_list=np.sum(W*np.sin(dphase),2)\n",
    "        return F_list\n",
    "    \n",
    "    def bias_force(self,bias,phase):\n",
    "        # Calculate the force induced by bias term \n",
    "        \n",
    "        h=bias[0,:]\n",
    "        psi=bias[1,:]\n",
    "        N_data=np.shape(phase)[0]\n",
    "        psi=np.tensordot(np.ones(N_data),psi,0)\n",
    "        F_list=-h*np.sin(phase-psi)\n",
    "        return F_list\n",
    "    \n",
    "    def cost_force(self, phase, target):\n",
    "        \n",
    "        #print(phase,target)\n",
    "        F=np.zeros(np.shape(phase))\n",
    "        F[:,self.output_index]=-np.sin(1*(phase[:,self.output_index]-target))\n",
    "        return F\n",
    "\n",
    "    def reg_force(self, phase, target):\n",
    "        \n",
    "        #print(phase,target)\n",
    "        F=np.zeros(np.shape(phase))\n",
    "        F[:,self.output_index]=-np.sin(0.5*(phase[:,self.output_index]-target))\n",
    "        return F\n",
    "    \n",
    "    def total_force(self, t, con_phase, W, bias, target, beta):\n",
    "        '''\n",
    "        Give the total force under different requirement: \n",
    "        bias_flag==0: no bias.\n",
    "        bias_flag==1: with bias.\n",
    "        '''\n",
    "        \n",
    "        Nh=int(len(con_phase)/self.N)\n",
    "        phase=np.reshape(con_phase,(Nh,self.N))\n",
    "\n",
    "        F0=self.internal_force(W,phase)\n",
    "        F1=self.bias_force(bias,phase)\n",
    "\n",
    "        F2 = beta*self.cost_force(phase,target)\n",
    "\n",
    "        F3 = beta*beta*self.reg_force(phase,target)\n",
    "        F = -F0+F1+F2+F3\n",
    "        F[:,self.input_index]=0\n",
    "        \n",
    "        return np.concatenate(F)\n",
    "        \n",
    "    \n",
    "    #-----------------------Evolution Module: Do evolutions and find the equilibrium ---------------------\n",
    "        \n",
    "\n",
    "    def run_network(self, W, bias, phase_0, target, beta, T):\n",
    "        \n",
    "        # Use scipy.integrate.solve_ivp\n",
    "        N_data = np.shape(phase_0)[0]\n",
    "        con_phase=np.concatenate(phase_0)\n",
    "        t_span=[0,T]\n",
    "        temp_model=sp.integrate.solve_ivp(self.total_force,t_span,con_phase,method='RK45',args=[W,bias,target,beta])\n",
    "        \n",
    "        L=len(temp_model.t)\n",
    "\n",
    "        phase = np.zeros([L, N_data, self.N])\n",
    "\n",
    "        for k in range(0,L): \n",
    "            phase[k,:,:] = np.reshape(temp_model.y[:,k], [N_data, self.N])\n",
    "        \n",
    "        #return temp_model.t, phase\n",
    "\n",
    "        return phase[L-1]\n",
    "\n",
    "\n",
    "    #------------------------Inverse Evolution-----------------------------\n",
    "\n",
    "    def inverse_force(self, t, con_phase, W, bias, target, beta):\n",
    "        \n",
    "        return -self.total_force(t, con_phase, W, bias, target, beta)\n",
    "\n",
    "    def inverse_evolution(self, W, bias, phase_0, phase_drift, target, T):\n",
    "\n",
    "        # Here phase_0 should be a stable equilibrium\n",
    "        # phase drift is some random perturbatio of the equilibrium so that the evolution could start. \n",
    "\n",
    "        N_data = np.shape(phase_0)[0]\n",
    "        phase_0 = phase_0 + phase_drift\n",
    "        con_phase=np.concatenate(phase_0)\n",
    "        t_span=[0,T]\n",
    "        temp_model=sp.integrate.solve_ivp(self.inverse_force,t_span,con_phase,method='RK45',args=[W,bias,target,0])\n",
    "        \n",
    "        L=len(temp_model.t)\n",
    "\n",
    "        phase = np.zeros([L, N_data, self.N])\n",
    "\n",
    "        for k in range(0,L): \n",
    "            phase[k,:,:] = np.reshape(temp_model.y[:,k], [N_data, self.N])\n",
    "        \n",
    "        return temp_model.t, phase\n",
    "\n",
    "\n",
    "    #=========================Calculate Exact Gradient==========================\n",
    "\n",
    "\n",
    "    def prod_phase(self,phase):\n",
    "        \n",
    "        N_data=np.shape(phase)[0]\n",
    "        \n",
    "        # Calculate the kroneck product prod_phase_ij = phase_i * phase_j\n",
    "        prod_phase=np.tensordot(phase,phase,0)\n",
    "        prod_phase=np.diagonal(prod_phase,axis1=0,axis2=2)\n",
    "        prod_phase=np.transpose(prod_phase,[2,0,1])\n",
    "        \n",
    "        return prod_phase\n",
    "    \n",
    "    def merge_weights(self,M_IS,M_SI,M_SS):\n",
    "        dim=len(np.shape(M_IS))\n",
    "        shape=np.shape(M_IS)[0:dim-2]\n",
    "\n",
    "        N_I=np.shape(M_IS)[dim-2]\n",
    "        N_S=np.shape(M_IS)[dim-1]\n",
    "        \n",
    "        M_II=np.zeros(np.concatenate((shape,[N_I,N_I]),axis=0))\n",
    "        M_up=np.concatenate((M_II,M_IS),axis=dim-1)\n",
    "        M_down=np.concatenate((M_SI,M_SS),axis=dim-1)\n",
    "        M=np.concatenate((M_up,M_down),axis=dim-2)\n",
    "        \n",
    "        return M\n",
    "        \n",
    "    \n",
    "    def destruct_weights(self,W):\n",
    "        \n",
    "        input_index=self.input_index\n",
    "        free_index=list(set(range(0,self.N))-set(input_index))\n",
    "        M_II=(W[...,input_index,:])[...,input_index]\n",
    "        M_IS=(W[...,input_index,:])[...,free_index]\n",
    "        M_SI=(W[...,free_index,:])[...,input_index]\n",
    "        M_SS=(W[...,free_index,:])[...,free_index]\n",
    "        \n",
    "        return M_II,M_IS,M_SI,M_SS\n",
    "        \n",
    "    def data_prod(self, A, B):\n",
    "        # This is to calculate M_nij=A_ni * B_nj. Here i and j can be either single or multiple index. \n",
    "        # We first do tensor product P_nimj=A_ni * B_mj. Then take the diagonal over index m and n. Then do the transpose. \n",
    "\n",
    "        dim_A=len(np.shape(A))\n",
    "        dim_B=len(np.shape(B))\n",
    "        M=np.tensordot(A,B,0)\n",
    "        T_prod=np.diagonal(M,axis1=0,axis2=dim_A)\n",
    "        prod=np.transpose(T_prod,[dim_A+dim_B-2]+list(range(0,dim_A+dim_B-2)))\n",
    "\n",
    "        return prod\n",
    "\n",
    "\n",
    "    def E_2nd_derivatives(self,W,bias,phase):\n",
    "        \n",
    "        # This calculate the dependence tensor to calculate the exact gradient. \n",
    "        \n",
    "        N_data=np.shape(phase)[0]\n",
    "        h=np.tensordot(np.ones(N_data),bias[0,:],0)\n",
    "        psi=np.tensordot(np.ones(N_data),bias[1,:],0)\n",
    "\n",
    "        prod_phase=self.prod_phase(phase)\n",
    "        diff_phase=self.dphase(phase)\n",
    "        Id=np.eye(self.N)\n",
    "        \n",
    "        # Calculate Hessian martix H and its inverse A=pinv(H). \n",
    "        # H_nij= (d^2E/(dx_i dx_j))_n = \\sum_k W_ik cos(x_ni - x_nk) + h_ni cos(x_ni-psi_ni) \\delta_ij - W_ij*cos(x_ni-x_nj)\n",
    "        M=W*np.cos(diff_phase)\n",
    "        B=h*np.cos(phase-psi)\n",
    "        G=np.sum(M,axis=2)+B\n",
    "        \n",
    "        H_diagonal=np.tensordot(G,Id,0)\n",
    "        H_diagonal=np.diagonal(H_diagonal,axis1=1,axis2=2)\n",
    "        H_diagonal=np.transpose(H_diagonal,[0,2,1])\n",
    "        \n",
    "        Hess=H_diagonal-M\n",
    "        #print(\"shape of A is: \", np.shape(A))\n",
    "\n",
    "        # Calculate half of matrix dEdW_nikl = d^2 E/(dx_i dW_kl)_n = (RW_nikl + RW_nilk)/2\n",
    "        RW=np.tensordot(np.sin(diff_phase),Id,0)\n",
    "        RW=np.diagonal(RW,axis1=2,axis2=4)\n",
    "        RW=np.transpose(RW,[0,2,1,3])/2\n",
    "        # Reconsturct the blocks to guarantee the boundary condition\n",
    "        '''\n",
    "        dExW_II, dExW_IS, dExW_SI, dExW_SS = self.destruct_weights(RW)\n",
    "        print(dExW_II, dExW_IS, dExW_SI, dExW_SS)\n",
    "        dExW_II = 0*dExW_II\n",
    "        '''\n",
    "        dExW = (RW + np.transpose(RW, [0,1,3,2]))/2\n",
    "        #dExW = self.merge_weights(dExW_IS, dExW_SI, dExW_SS)\n",
    "        #print(\"shape of RW is: \", np.shape(RW))\n",
    "        #print(\"RW= \\n\", RW)\n",
    "        #print(\"dExW= \\n\", dExW)\n",
    "        \n",
    "\n",
    "        # Calculate dExh_ik = d^2E/(dh_k dx_i) = sin(phi_k-psi_k) * delta_ik\n",
    "        Rh=-np.tensordot(np.sin(phase-psi),Id,0)\n",
    "        dExh=np.diagonal(Rh,axis1=1,axis2=3)\n",
    "        #dExh[:,:,self.input_index]=0\n",
    "        #dExh[:,self.input_index,:]=0\n",
    "        #print(\"shape of Rh is: \", np.shape(Rh))\n",
    "        \n",
    "        \n",
    "        # Calculate dExp_ik = d^2E/(dpsi_k dx_i) = -h_k * cos(phi_k-psi_k) * delta_ik\n",
    "        RP = -h*np.cos(phase-psi)\n",
    "        RP = np.tensordot(RP, Id, 0)\n",
    "        RP = np.diagonal(RP, axis1=1,axis2=2)\n",
    "        dExP = np.transpose(RP, [0,2,1])\n",
    "        #dExP[:,:,self.input_index]=0\n",
    "        #dExP[:,self.input_index,:]=0\n",
    "\n",
    "        return Hess, dExW, dExh, dExP\n",
    "\n",
    "\n",
    "    def x_dep(self,W,bias,phase):\n",
    "        # Calculate x dependence over internal parameters dx/dW, dx/dh, dx/dpsi\n",
    "        # dx_i/d W_k = sum_j inv(Hess)_ij dExW_k\n",
    "\n",
    "        Hess, dExW, dExh, dExP = self.E_2nd_derivatives(W,bias,phase) \n",
    "        # Calculate inverse Hessian: A=H^-1\n",
    "\n",
    "        A=np.linalg.pinv(Hess)\n",
    "        print(\"A=\",A)\n",
    "        A_II, A_IS, A_SI, A_SS = self.destruct_weights(A)\n",
    "        A=self.merge_weights(A_IS*0, A_SI*0, A_SS)\n",
    "        print(\"A=\",A)\n",
    "        dxdW=self.data_prod(A,dExW)\n",
    "        dxdW=np.diagonal(dxdW,axis1=2, axis2=3)\n",
    "        dxdW=np.sum(dxdW,axis=4)\n",
    "\n",
    "\n",
    "        dxdh=self.data_prod(A,dExh)\n",
    "        dxdh=np.diagonal(dxdh,axis1=2, axis2=3)\n",
    "        dxdh=np.sum(dxdh,axis=3)\n",
    "\n",
    "        dxdP=self.data_prod(A,dExP)\n",
    "        dxdP=np.diagonal(dxdP,axis1=2, axis2=3)\n",
    "        dxdP=np.sum(dxdP,axis=3)\n",
    "        \n",
    "        return dxdW, dxdh, dxdP\n",
    "    \n",
    "    def exact_gradient(self,W,h,phase,target):\n",
    "        \n",
    "        # This calculate the exact gradient of loss function over weights and bias with linear self-consistent function. \n",
    "        \n",
    "        # Calculate the naive gradient\n",
    "        NG=np.zeros(np.shape(phase))\n",
    "        NG[:,self.output_index] = -np.sin(phase[:,self.output_index]-target)\n",
    "        \n",
    "        dxdW, dxdh, dxdP = self.x_dep(W,h,phase)\n",
    "        \n",
    "        #print(\"shape of NG is: \", np.shape(NG))\n",
    "        \n",
    "        dLW=np.tensordot(NG,dxdW,(1,1))\n",
    "        dLW=np.diagonal(dLW,axis1=0,axis2=1)\n",
    "        dLW=np.transpose(dLW,[2,0,1])\n",
    "        \n",
    "        dLh=np.tensordot(NG,dxdh,(1,1))\n",
    "        dLh=np.diagonal(dLh,axis1=0,axis2=1)\n",
    "        dLh=np.transpose(dLh,[1,0])\n",
    "\n",
    "        dLP=np.tensordot(NG,dxdP,(1,1))\n",
    "        dLP=np.diagonal(dLP,axis1=0,axis2=1)\n",
    "        dLP=np.transpose(dLP,[1,0])\n",
    "        \n",
    "        return dLW,dLh,dLP\n",
    "    #----------------------Calculate the gradient of weights and bias throught EP-------------------------\n",
    "\n",
    "    def half_search(self, W, bias, internal_gradient, bias_gradient, study_rate):\n",
    "        \n",
    "        #print(study_rate)\n",
    "        \n",
    "        if study_rate<0.001:\n",
    "            W=W-study_rate*internal_gradient\n",
    "            bias=bias-study_rate*bias_gradient\n",
    "            return W, bias\n",
    "        else:\n",
    "            E0=np.sum(self.cost_function(self.equi_free,self.training_target))/self.N_sample\n",
    "\n",
    "            equi_temp1=self.find_free_equilibrium(W-study_rate/2*internal_gradient, bias-study_rate/2*bias_gradient, self.bias_flag, self.equi_free,self.dt)\n",
    "            E1=np.sum(self.cost_function(equi_temp1,self.training_target))/self.N_sample\n",
    "\n",
    "            equi_temp2=self.find_free_equilibrium(W-study_rate/2*internal_gradient, bias-study_rate/2*bias_gradient, self.bias_flag, equi_temp1,self.dt)\n",
    "            E2=np.sum(self.cost_function(equi_temp2,self.training_target))/self.N_sample\n",
    "\n",
    "            E_min=np.min([E0,E1,E2])\n",
    "            if E_min==E2: \n",
    "                W=W-study_rate*internal_gradient\n",
    "                bias=bias-study_rate*bias_gradient\n",
    "                return W, bias\n",
    "            elif E_min==E1:\n",
    "                W=W-study_rate/2*internal_gradient\n",
    "                bias=bias-study_rate/2*bias_gradient\n",
    "                return W, bias\n",
    "            else:\n",
    "                return self.half_search(W, bias, internal_gradient, bias_gradient, study_rate/2)\n",
    "        \n",
    "    \n",
    "    def train_with_exact_gradient(self):\n",
    "        \n",
    "        print(\"start training with exact gradient\")\n",
    "        \n",
    "        W=self.weights_0\n",
    "        bias=self.bias\n",
    "        self.equi_nudge=self.equi_free\n",
    "        target=self.training_target\n",
    "        N_data=np.shape(target)[0]\n",
    "        \n",
    "        old_cost=10*self.N*self.N\n",
    "        temp_cost=10\n",
    "        nr=0\n",
    "        self.validity_training=np.zeros(self.N_epoch)\n",
    "        \n",
    "        #print(W,bias,nr,np.abs((old_cost-temp_cost)/temp_cost))\n",
    "        \n",
    "        # Use this for one_d search\n",
    "            \n",
    "        while np.abs((old_cost-temp_cost)/temp_cost)>self.tol_W:\n",
    "            if nr==self.N_epoch: \n",
    "                break\n",
    "            #self.equi_free=self.find_free_equilibrium(W,bias,self.bias_flag,self.equi_nudge,self.dt)\n",
    "            \n",
    "            self.equi_free=self.find_nudge_equilibrium(W, bias, self.bias_flag, self.equi_nudge, self.training_target, 0, self.dt)\n",
    "            \n",
    "            gW,gh=self.exact_gradient(W,bias,self.equi_free,target)\n",
    "            internal_gradient=np.sum(gW,axis=0)/N_data\n",
    "            bias_gradient=np.sum(gh,axis=0)/N_data\n",
    "            \n",
    "            if self.bias_flag==0:\n",
    "                bias_gradient=np.zeros(np.shape(self.bias))\n",
    "                \n",
    "            W=W-self.study_rate*internal_gradient\n",
    "            bias=bias-self.study_rate*bias_gradient\n",
    "            \n",
    "            old_cost=temp_cost\n",
    "            temp_cost=self.cost_function(self.equi_free,self.training_target)\n",
    "            temp_cost=np.sqrt(np.sum(temp_cost)/self.N_sample/self.N_output)\n",
    "            self.validity_training[nr]=temp_cost\n",
    "            print(temp_cost,np.sum(np.abs(internal_gradient)),nr)\n",
    "            #print(nr)\n",
    "            \n",
    "            \n",
    "            print(\"EP gradient=\", internal_gradient,bias_gradient)\n",
    "            #print(\"Exact gradient=\", np.sum(gW,axis=0)/4,np.sum(gh,axis=0)/4)\n",
    "            nr=nr+1\n",
    "                \n",
    "        self.weights=W\n",
    "        self.bias=bias\n",
    "        \n",
    "        \n",
    "        \n",
    "    #-------------------------Test Module: test the ytrained network--------------------\n",
    "    \n",
    "    def initiate_test(self):\n",
    "        \n",
    "        '''\n",
    "        aux_ones=np.ones(self.N_test)\n",
    "        self.equi_test=np.tensordot(aux_ones, self.phase_0,0)\n",
    "        \n",
    "        '''\n",
    "        # For total irrelevant random initiate, use the code below:\n",
    "        \n",
    "        self.equi_test=2*np.pi*np.random.rand-np.pi*np.ones(np.shape(self.N_test,self.N))\n",
    "        \n",
    "        \n",
    "        self.equi_test[:,self.input_index]=self.test_data\n",
    "        \n",
    "    def validation_test(self):\n",
    "        \n",
    "        # Do the validity test. Run initiate_test() first.\n",
    "        # The validity is defined as \\sqrt((\\sum_test cost)/N)\n",
    "        \n",
    "        phase=self.find_free_equilibrium(self.weights,self.bias,self.bias_flag,self.equi_test,self.dt)\n",
    "        validity_list=self.cost_function(phase,self.target_output)\n",
    "        validity=np.sqrt(np.sum(validity_list)/self.N_test)\n",
    "        \n",
    "        self.validity_test=validity\n",
    "        self.equi_test=phase\n",
    "        \n",
    "    #------------------------Output Module: give the result for input------------------\n",
    "    \n",
    "    \n",
    "\n",
    "class SP_XY_masked_Layer_Network(SP_XY_Network):\n",
    "    def __init__(self, N, N_ev, dt, structure_list):\n",
    "        \n",
    "        # Setting parameters from the input\n",
    "        self.N=N\n",
    "        self.N_ev=N_ev\n",
    "        self.dt=dt\n",
    "        self.structure_list = structure_list\n",
    "        self.T=self.dt*self.N_ev\n",
    "        \n",
    "        # Initalize all the other parameters. Default values are all zero\n",
    "        self.N_sample=0\n",
    "        self.N_test=0\n",
    "        \n",
    "        self.weights=np.zeros([N,N])\n",
    "        self.weights_0=np.zeros([N,N])\n",
    "\n",
    "        self.bias=np.zeros([2,N])\n",
    "        self.bias_0=np.zeros([2,N])\n",
    "        \n",
    "        self.beta=0\n",
    "\n",
    "        #depth\n",
    "        self.L = len(structure_list)\n",
    "        self.mask = np.zeros([N,N])\n",
    "\n",
    "        for k in range(0, self.L-1):\n",
    "            self.mask[np.sum(structure_list[0:k]):np.sum(structure_list[0:k+1]), np.sum(structure_list[0:k+1]):np.sum(structure_list[0:k+2])] = 1\n",
    "\n",
    "        self.mask = self.mask + np.transpose(self.mask)\n",
    "        \n",
    "        self.split_points = np.zeros(len(structure_list)-1)\n",
    "        for k in range(0,len(structure_list)-1):\n",
    "            self.split_points[k] = np.sum(structure_list[0:k+1])\n",
    "            \n",
    "        self.split_points = np.asarray(self.split_points, dtype=np.int32)\n",
    "        \n",
    "    def random_state_initiation(self):\n",
    "        # Randomly set the weights and the phase_0 and bias\n",
    "        \n",
    "        # Set weights, ~N(0,1)\n",
    "        self.weights_0 = np.random.randn(self.N, self.N)\n",
    "        for k in range(0,self.N):\n",
    "            self.weights_0[k,k] = 0\n",
    "        self.weights_0 = self.mask * (self.weights_0+np.transpose(self.weights_0))/2\n",
    "        \n",
    "        self.weights = self.weights_0\n",
    "        \n",
    "        #Set bias, ~U(-0.5,0.5),U(-pi,pi)\n",
    "        bias = np.random.rand(2,self.N)\n",
    "        bias[0,:] = bias[0,:]-0.5\n",
    "        bias[1,:] = 2*np.pi*(bias[1,:]-0.5)\n",
    "        self.bias_0 = bias\n",
    "        self.bias = bias\n",
    "\n",
    "        # Set phase_0\n",
    "        self.phase_0 = np.pi*np.random.rand(self.N)-np.pi/2*np.ones(self.N)\n",
    "\n",
    "\n",
    "class SP_XY_Layer_Network(SP_XY_Network):\n",
    "    def __init__(self, N, N_ev, dt, structure_list):\n",
    "        \n",
    "        # Setting parameters from the input\n",
    "        self.N = N\n",
    "        self.N_ev = N_ev\n",
    "        self.dt = dt\n",
    "        self.structure_list = structure_list\n",
    "        self.T = self.dt * self.N_ev\n",
    "        \n",
    "        # Initalize all the other parameters. Default values are all zero\n",
    "        self.N_sample = 0\n",
    "        self.N_test = 0\n",
    "        \n",
    "        self.weights = np.zeros([N,N])\n",
    "        self.weights_0 = np.zeros([N,N])\n",
    "\n",
    "        self.bias = np.zeros([2,N])\n",
    "        self.bias_0 = np.zeros([2,N])\n",
    "        \n",
    "        self.beta = 0\n",
    "\n",
    "        #depth\n",
    "        self.L = structure_list.shape[0]\n",
    "        self.mask = np.zeros([N,N])\n",
    "        \n",
    "        self.split_points = np.zeros(self.L-1)\n",
    "        for k in range(0,self.L-1):\n",
    "            self.split_points[k] = np.sum(structure_list[0:k+1])\n",
    "            \n",
    "        self.split_points = np.asarray(self.split_points, dtype=np.int32)\n",
    "        \n",
    "        self.input_index = np.arange(0, self.split_points[0])\n",
    "        self.variable_index = np.arange(self.split_points[0], self.N)\n",
    "        self.output_index = np.arange(self.split_points[-1], self.N)\n",
    "        \n",
    "        self.WL = []\n",
    "        \n",
    "        self.structure_shape = []\n",
    "        for k in range(0,len(self.split_points)):\n",
    "            self.structure_shape.append(np.zeros(self.split_points[k]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def random_state_initiation(self):\n",
    "        # Randomly set the weights and the phase_0 and bias\n",
    "        \n",
    "        # Set weights, ~N(0,1)\n",
    "        self.WL = []\n",
    "        for k in range(0, len(self.structure_list)-1):\n",
    "            self.WL.append( np.sqrt(self.structure_list[k]+self.structure_list[k+1]) * np.random.randn(self.structure_list[k], self.structure_list[k+1]))\n",
    "        \n",
    "        \n",
    "        #Set bias, ~U(-0.5,0.5),U(-pi,pi)\n",
    "        bias = np.random.rand(2,self.N)\n",
    "        bias[0,:] = bias[0,:]-0.5\n",
    "        bias[1,:] = 2*np.pi*(bias[1,:]-0.5)\n",
    "        self.bias_0 = bias\n",
    "        self.bias = bias\n",
    "\n",
    "        # Set phase_0\n",
    "        self.phase_0 = np.pi * np.random.rand(self.N)-np.pi/2*np.ones(self.N)\n",
    "        \n",
    "        self.weights_0 = merge(self.WL, self.split_points, self.phase_0)\n",
    "        self.weights = self.weights_0\n",
    "        \n",
    "\n",
    "class SP_XY_SquareLattice_Network(SP_XY_Network):\n",
    "    def __init__(self, dimension, N_ev, dt, input_size, output_size):\n",
    "        self.architecture = 'Square Lattice'\n",
    "        \n",
    "        self.dimension = dimension\n",
    "        self.N = dimension[0]*dimension[1]\n",
    "        self.N_ev = N_ev\n",
    "        self.dt = dt\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.weights_0 = np.zeros([4] + dimension)\n",
    "\n",
    "        self.bias_0 = np.zeros([2] + dimension)\n",
    "        \n",
    "        self.input_index = np.arange(0, input_size)\n",
    "        self.variable_index = np.arange(input_size, self.N)\n",
    "        self.output_index = np.arange(self.N-output_size, self.N)\n",
    "        \n",
    "    def random_state_initiation(self):\n",
    "        weights_shape = self.weights_0.shape[1], self.weights_0.shape[2]\n",
    "        \n",
    "        W0 = np.random.randn(weights_shape[0]-1, weights_shape[1])\n",
    "        W1 = np.random.randn(weights_shape[0], weights_shape[1]-1)\n",
    "        \n",
    "        zero_row = np.zeros([1,weights_shape[1]])\n",
    "        zero_col = np.zeros([weights_shape[0],1])\n",
    "        \n",
    "        W_u = np.concatenate((W0, zero_row), axis=0)\n",
    "        W_d = np.concatenate((zero_row, W0), axis=0)\n",
    "        \n",
    "        W_l = np.concatenate((W1, zero_col), axis=1)\n",
    "        W_r = np.concatenate((zero_col, W1), axis=1)\n",
    "        \n",
    "        self.weights_0 = np.asarray([W_u, W_d, W_l, W_r])\n",
    "        self.bias_0[0,...] = np.random.randn(*self.dimension)\n",
    "        self.bias_0[1,...] = 2*np.pi*(np.random.rand(*self.dimension) - 0.5)\n",
    "        \n",
    "        \n",
    "\n",
    "#==============================================Matrix workshop=====================================================\n",
    "def merge(WL, split_points, phase):\n",
    "    N = phase.shape[0]\n",
    "    M = np.zeros([N,N])\n",
    "    \n",
    "    L = split_points.shape[0]\n",
    "    merge_points = jnp.zeros(L+2, dtype=jnp.int32)\n",
    "    merge_points = merge_points.at[1:L+1].set(split_points)\n",
    "    merge_points = merge_points.at[L+1].set(N)\n",
    "    merge_points = merge_points.at[0].set(0)\n",
    "    '''\n",
    "    merge_points = split_points.copy()\n",
    "    merge_points.append(N)\n",
    "    merge_points.insert(0, 0)\n",
    "    '''\n",
    "    \n",
    "    for k in range(0,L):\n",
    "        M[merge_points[k]:merge_points[k+1], merge_points[k+1]:merge_points[k+2]] = WL[k]\n",
    "    \n",
    "    M = M + np.transpose(M, [1,0])\n",
    "    \n",
    "    return M\n",
    "\n",
    "def split_M(W, split_points):\n",
    "    WL = []\n",
    "    N = W.shape[0]\n",
    "    \n",
    "    L = split_points.shape[0]\n",
    "    merge_points = jnp.zeros(L+2, dtype=jnp.int32)\n",
    "    merge_points = merge_points.at[1:L+1].set(split_points)\n",
    "    merge_points = merge_points.at[L+1].set(N)\n",
    "    merge_points = merge_points.at[0].set(0)\n",
    "    \n",
    "    '''\n",
    "    merge_points = split_points.copy()\n",
    "    merge_points.append(N)\n",
    "    merge_points.insert(0, 0)\n",
    "    '''\n",
    "    \n",
    "    for k in range(0,L):\n",
    "        WL.append(W[merge_points[k]:merge_points[k+1], merge_points[k+1]:merge_points[k+2]])\n",
    "    \n",
    "    return WL\n",
    "\n",
    "\n",
    "def flatten_weights(dimension, WL):\n",
    "    W_u, W_d, W_l, W_r = WL[0], WL[1], WL[2], WL[3]\n",
    "    n, m = dimension[0], dimension[1]\n",
    "    W_mat = np.zeros([n,m,n,m])\n",
    "    ind_n = np.arange(0,n)\n",
    "    ind_m = np.arange(0,m)\n",
    "    \n",
    "    for k in range(0, n-1):\n",
    "        for l in range(0, m):\n",
    "            W_mat[k,l,k+1,l] = W_u[k,l]\n",
    "            W_mat[k+1,l,k,l] = W_d[k+1,l]\n",
    "    \n",
    "    for k in range(0, n):\n",
    "        for l in range(0, m-1):\n",
    "            W_mat[k,l,k,l+1] = W_l[k,l]\n",
    "            W_mat[k,l+1,k,l] = W_r[k,l+1]\n",
    "    \n",
    "    W_mat = jnp.concatenate(W_mat)\n",
    "    W_mat = jnp.transpose(W_mat, [1,2,0])\n",
    "    W_mat = jnp.concatenate(W_mat)\n",
    "    W_mat = jnp.transpose(W_mat, [1,0])\n",
    "    \n",
    "    return W_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015da48e-2255-40d8-bb59-1cf732c3701b",
   "metadata": {
    "id": "015da48e-2255-40d8-bb59-1cf732c3701b",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for training the system with jax\n",
    "#---------------------------Energy Terms-------------------------\n",
    "from functools import partial\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "@jax.jit\n",
    "def internal_energy(W,phase):\n",
    "    # Calculate free term of internal energy for a set of phase\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "    dphase=cal_dphase(phase)\n",
    "    E_list= -0.5 * jnp.sum(W * jnp.cos(dphase), (1,2))\n",
    "        \n",
    "    return E_list\n",
    "\n",
    "@jax.jit   \n",
    "def bias_term(bias,phase):\n",
    "    # Calculate bias term of internal energy for a set of phase\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input. \n",
    "        \n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "    N_data = jnp.shape(phase)[0]\n",
    "\n",
    "    aux_ones = jnp.ones(N_data)\n",
    "    psi_mat = jnp.tensordot(aux_ones,psi,0)\n",
    "    E_list = -jnp.sum( h*jnp.cos(phase-psi_mat), axis=1)\n",
    "    return E_list\n",
    "\n",
    "@jax.jit\n",
    "def cost_function(phase,target,output_index):\n",
    "    # Calculate the cost function for each sample\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "    output_phase = jnp.reshape(phase[:,output_index],jnp.shape(target))\n",
    "    \n",
    "    doutput = output_phase-target\n",
    "    cost_mat = jnp.ones(jnp.shape(doutput))-jnp.cos(doutput)\n",
    "    cost_list = jnp.sum(cost_mat,1)/2\n",
    "        \n",
    "    return cost_list\n",
    "\n",
    "@jax.jit\n",
    "def total_energy(W, bias, phase, target, output_index, beta):\n",
    "\n",
    "    return internal_energy(W,phase) + bias_term(bias,phase) + beta * cost_function(phase,target,output_index)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def qualitative_cost(phase, target, output_index, tol):\n",
    "    # Calculate the cost function for each sample\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "    output_phase = jnp.reshape(phase[:,output_index],jnp.shape(target))\n",
    "    \n",
    "    doutput = output_phase-target\n",
    "    cost_mat = jnp.ones(jnp.shape(doutput))-jnp.cos(doutput)\n",
    "    cost_list = jnp.sum( (cost_mat>tol)/2 )\n",
    "        \n",
    "    return cost_list\n",
    "\n",
    "#-------------------Run the system--------------------\n",
    "@jax.jit\n",
    "def cal_dphase(phase):\n",
    "    # Calculate dphase[i,j]=phase[i]-phase[j]\n",
    "    \n",
    "    N = jnp.shape(phase)[1]\n",
    "    aux_ones=jnp.ones(N)\n",
    "    phase_mat=jnp.tensordot(aux_ones,phase,0)\n",
    "    phase_i=jnp.transpose(phase_mat,(1,2,0))\n",
    "    phase_j=jnp.transpose(phase_i,(0,2,1))\n",
    "    dphase=phase_i-phase_j\n",
    "        \n",
    "    return dphase\n",
    "\n",
    "@jax.jit\n",
    "def total_force(t, con_phase, W, bias, target, beta, input_index, output_index):\n",
    "    \n",
    "    N = jnp.shape(W)[0]\n",
    "    N_data = int(jnp.shape(con_phase)[0]/N)\n",
    "    phase = jnp.reshape(con_phase,(N_data,N))\n",
    "\n",
    "    dphase = cal_dphase(phase)\n",
    "    F0 = jnp.sum(W*jnp.sin(dphase),2)\n",
    "\n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    N_data=jnp.shape(phase)[0]\n",
    "    psi=jnp.tensordot(jnp.ones(N_data),psi,0)\n",
    "    \n",
    "    #print(target)\n",
    "    #print(phase[:,output_index])\n",
    "\n",
    "    output_phase = jnp.reshape(phase[:,output_index],jnp.shape(target))\n",
    "\n",
    "    F1 = -h * jnp.sin(phase-psi)\n",
    "\n",
    "    F2 = jnp.zeros(np.shape(phase))\n",
    "    temp_F2 = -jnp.sin(output_phase-target)\n",
    "    F2 = F2.at[:,output_index].set( jnp.reshape(temp_F2, jnp.shape(output_phase[:,output_index])) )\n",
    "    \n",
    "    '''\n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    temp_F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    F3 = F3.at[:,output_index].set( jnp.reshape(temp_F3, jnp.shape(output_phase[:,output_index])) )\n",
    "    '''\n",
    "    \n",
    "    #F3 = F3.at[:,output_index].set( -jnp.sin(0.5*(output_phase-target)) )\n",
    "    \n",
    "    # Apply cross entropy loss functoin\n",
    "    \n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    M1 = -jnp.sin(output_phase-target)\n",
    "    M2 = (1.00)*jnp.ones(jnp.shape(output_phase)) + jnp.cos(output_phase-target)\n",
    "    F3 = F3.at[:,output_index].set( jnp.reshape(M1/M2, jnp.shape(output_phase[:,output_index])) )\n",
    "    \n",
    "\n",
    "    F = -F0 + F1 + 0*beta*F2 + beta*F3\n",
    "    F = F.at[:,input_index].set( jnp.zeros([N_data, len(input_index)]) )\n",
    "    \n",
    "        \n",
    "    return jnp.concatenate(F)\n",
    "\n",
    "@jax.jit\n",
    "def ode_total_force(con_phase, t, W, bias, target, beta, input_index, output_index):\n",
    "    return total_force(t, con_phase, W, bias, target, beta, input_index, output_index)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def run_network(phase_0, T, W, bias, target, beta, input_index, output_index):\n",
    "    # Set the initial conditions\n",
    "    N_data, N = jnp.shape(phase_0)\n",
    "    \n",
    "    con_phase_0 = jnp.concatenate(phase_0)\n",
    "    \n",
    "    \n",
    "    # Solve the equation with diffrax\n",
    "    # Set parameter for diffrax\n",
    "    rtol = 1e-8\n",
    "    atol = 1e-8\n",
    "    t_span = [0,T]\n",
    "    #saveat = np.linspace(*t_span, 10).tolist()\n",
    "    odefunc = lambda t, con_phase, args: total_force(t, con_phase, W, bias, target, beta, input_index, output_index)\n",
    "    eqs = diffrax.ODETerm(odefunc)\n",
    "    \n",
    "    # Use 4th Runge Kutta\n",
    "    #solver = diffrax.Tsit5()\n",
    "    \n",
    "    #Use 5th Kvaerno for stiff case\n",
    "    solver = diffrax.Kvaerno4()\n",
    "    \n",
    "    stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
    "    #t = diffrax.SaveAt(ts=saveat)\n",
    "\n",
    "    # Solve the ODE\n",
    "    solution = diffrax.diffeqsolve(eqs, solver, t0=t_span[0], t1=t_span[1], dt0 = None, y0=con_phase_0,\n",
    "                               stepsize_controller=stepsize_controller, max_steps=1000000)\n",
    "    \n",
    "    L = len(solution.ts)\n",
    "    \n",
    "    phase = jnp.reshape(solution.ys[L-1,:], [N_data, N])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # Evolve with jaxode\n",
    "    \n",
    "    t_eval = jnp.array([0.,T])\n",
    "    solution = jaxode.odeint(ode_total_force, con_phase_0, t_eval, W, bias, target, beta, input_index, output_index, atol=1e-8, rtol=1e-8)\n",
    "    L = len(t_eval)\n",
    "    phase = jnp.reshape(solution[L-1,:], [N_data, N])\n",
    "    '''\n",
    "    \n",
    "    return phase\n",
    "\n",
    "\n",
    "#----------------------Calculate the gradient for the parameters-------------------------------\n",
    "@jax.jit\n",
    "def weights_gradient(equi_nudge, equi_free, beta):\n",
    "    # This is to calculate the contribution to weights gradient of free term in the internal energy\n",
    "    # The expression is: ( -cos(phi_i-phi_j)(\\beta!=0)+cos(phi_i-phi_j)(\\beta=0) )\n",
    "    nudge_dphase = cal_dphase(equi_nudge)\n",
    "    free_dphase = cal_dphase(equi_free)\n",
    "    gradient_list = (-jnp.cos(nudge_dphase)+jnp.cos(free_dphase))\n",
    "    #print(jnp.shape(gradient_list))\n",
    "    gradient = jnp.mean(gradient_list,axis=0)\n",
    "        \n",
    "    return gradient\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def bias_gradient(equi_nudge, equi_free, bias, beta): \n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "\n",
    "    h = jnp.tensordot(jnp.ones(jnp.shape(equi_free)[0]), h, 0)\n",
    "    psi = jnp.tensordot(jnp.ones(jnp.shape(equi_free)[0]), psi, 0)\n",
    "\n",
    "    g_h = jnp.cos(equi_free-psi) - jnp.cos(equi_nudge-psi)\n",
    "    g_psi = h*jnp.sin(equi_free-psi) - h*jnp.sin(equi_nudge-psi)\n",
    "\n",
    "    g_h = jnp.mean(g_h,axis=0)\n",
    "    g_psi = jnp.mean(g_psi,axis=0)\n",
    "\n",
    "    return jnp.asarray([g_h, g_psi])\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def paras_gradient(equi_nudge, equi_free, bias, beta): \n",
    "\n",
    "    return weights_gradient(equi_nudge, equi_free, beta) , bias_gradient(equi_nudge, equi_free, bias, beta)\n",
    "\n",
    "\n",
    "#---------------------------------Train the Network-------------------------------\n",
    "\n",
    "@jax.jit\n",
    "def weights_update(k, present_network):\n",
    "    # Initialize the system\n",
    "    phase_shape, W_0, bias_0, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag = present_network\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, input_index, output_index = model_paras\n",
    "    N_data = jnp.shape(training_data)[0]\n",
    "    N = jnp.shape(phase_shape)[1]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    input_tuple = input_index\n",
    "    output_tuple =  output_index\n",
    "\n",
    "    #initial phase\n",
    "    phase_0 = jnp.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "    phase_0 = jnp.asarray(phase_0)\n",
    "    phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "    \n",
    "    # Calculate the cost function\n",
    "    equi_zero = run_network(phase_0, T, W, bias, training_target, 0, input_index, output_index)\n",
    "    cost_list = cost_function(equi_zero, training_target, output_index)\n",
    "    cost = cost.at[k].set( jnp.sum(cost_list)/N_data )\n",
    "\n",
    "    # Update the weights\n",
    "    # Run the free and nudge phase for equilibrium propagation\n",
    "    # equi_free = run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "    equi_nudge = run_network(equi_zero, T, W, bias, training_target, beta, input_index, output_index)\n",
    "        \n",
    "    # Calculate the gradient for the weights\n",
    "    gW, gh = paras_gradient(equi_nudge, equi_zero, bias, beta)\n",
    "\n",
    "    # Update the weights and bias\n",
    "    W = W_0 - study_rate/beta*gW\n",
    "    bias = bias_0 - study_rate/beta*gh\n",
    "\n",
    "    # Formulate the output\n",
    "\n",
    "\n",
    "    return phase_shape, W, bias, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag\n",
    "\n",
    "@partial(jax.jit, static_argnames=['training_paras'])\n",
    "def jax_training(W_0, bias_0, training_data, training_target, training_paras, N_ev, dt, input_index, output_index, ext_init_phase_0=0, random_flag=False): \n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    #N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N = jnp.shape(W_0)[0]\n",
    "    N_data = jnp.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "\n",
    "    cost = jnp.zeros(N_epoch)\n",
    "    phase_shape = jnp.zeros([N_data, N])\n",
    "\n",
    "    # Training with fori_loop\n",
    "    running_training_paras = (beta, study_rate)\n",
    "    running_model_paras = (N_ev, dt, input_index, output_index)\n",
    "    initial_network = (phase_shape, W_0, bias_0, cost, training_data, training_target, running_training_paras, running_model_paras, ext_init_phase_0, random_flag)\n",
    "    \n",
    "    #time the training\n",
    "    final_network = jax.lax.fori_loop(0, N_epoch, weights_update, initial_network)\n",
    "    \n",
    "    W = final_network[1]\n",
    "    bias = final_network[2]\n",
    "    cost = final_network[3]\n",
    "    \n",
    "\n",
    "    return cost, W, bias\n",
    "\n",
    "\n",
    "def train_network(W_0, bias_0, training_data, training_target, training_paras, model_paras, ext_init_phase_0=0, random_flag=False):\n",
    "    \n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N_data = jnp.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    input_tuple = tuple(input_index)\n",
    "    output_tuple = tuple(output_index)\n",
    "    \n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    cost = jnp.zeros(N_epoch)\n",
    "    \n",
    "\n",
    "    # training\n",
    "    t1 = time.time()\n",
    "    for k in range(0,N_epoch):\n",
    "        phase_0 = jnp.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "        phase_0 = jnp.asarray(phase_0)\n",
    "        phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "        #print(phase_0)\n",
    "\n",
    "        #print(W.shape, phase_0.shape, bias.shape)\n",
    "\n",
    "        # Calculate the cost function\n",
    "        equi_zero = run_network(phase_0, T, W, bias, training_target, 0, input_index, output_index)\n",
    "        cost_list = cost_function(equi_zero, training_target, output_index)\n",
    "        cost = cost.at[k].set( jnp.sum(cost_list)/N_data )\n",
    "\n",
    "        # Run the free and nudge phase for equilibrium propagation\n",
    "        # equi_free = run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "        equi_nudge = run_network(phase_0, T, W, bias, training_target, beta, input_index, output_index)\n",
    "        \n",
    "        #print(equi_zero, '\\n\\n', equi_free, '\\n\\n', equi_nudge, '\\n\\n')\n",
    "        \n",
    "        #print(\"equi_zero= \\n\", equi_zero, \"\\n equi_free= \\n\", equi_free, \"\\n equi_nudge= \\n\", equi_nudge)\n",
    "        \n",
    "        # Calculate the gradient for the weights\n",
    "        gW, gh = paras_gradient(equi_nudge, equi_zero, bias, beta)\n",
    "        \n",
    "        #print(gW,'\\n\\n', gh)\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        W = W - study_rate*gW/beta\n",
    "        bias = bias - study_rate*gh/beta\n",
    "\n",
    "    t2 = time.time()\n",
    "    \n",
    "    '''\n",
    "    for k in range(0,N_epoch):\n",
    "        phase_0 = jnp.zeros([N_sample, N])\n",
    "        phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "\n",
    "        # Calculate the cost function\n",
    "        equi_zero = run_network(W, bias, phase_0, training_target, 0, T, input_tuple, output_tuple)\n",
    "        cost_list = total_energy(W, bias, equi_zero, training_target, output_index)\n",
    "        cost = cost.at[k].set( jnp.sum(cost_list)/N_sample )\n",
    "\n",
    "        # Run the free and nudge phase for equilibrium propagation\n",
    "        equi_free = run_network(W, bias, phase_0, training_target, -beta, T, input_tuple, output_tuple)\n",
    "        equi_nudge = run_network(W, bias, phase_0, training_target, beta, T, input_tuple, output_tuple)\n",
    "        \n",
    "        # Calculate the gradient for the weights\n",
    "        gW, gh = paras_gradient(equi_nudge, equi_free, bias, beta)\n",
    "\n",
    "        # Update the weights and bias\n",
    "        W = W - study_rate*gW\n",
    "        bias = bias - study_rate*gh\n",
    "    '''\n",
    "    \n",
    "    t3 = time.time()\n",
    "\n",
    "    return cost, W, bias\n",
    "\n",
    "#--------------------------Consecutive Training------------------------\n",
    "'''\n",
    "Training the net work with consecutive scheme: start from the equilibrium we get from the last epoch.\n",
    "'''\n",
    "\n",
    "@jax.jit\n",
    "def consecutive_weights_update(k, present_network):\n",
    "    # Initialize the system\n",
    "    phase_shape, W_0, bias_0, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag = present_network\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, input_index, output_index = model_paras\n",
    "    N_data= jnp.shape(training_data)[0]\n",
    "    N = jnp.shape(phase_shape)[1]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    input_tuple = input_index\n",
    "    output_tuple =  output_index\n",
    "\n",
    "    #initial phase\n",
    "    phase_0 = jnp.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "    phase_0 = jnp.asarray(phase_0)\n",
    "    phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "    \n",
    "    # Calculate the cost function\n",
    "    equi_zero = run_network(phase_0, T, W, bias, training_target, 0, input_index, output_index)\n",
    "    cost_list = cost_function(equi_zero, training_target, output_index)\n",
    "    cost = cost.at[k].set( jnp.sum(cost_list)/N_data )\n",
    "\n",
    "    # Update the weights\n",
    "    # Run the free and nudge phase for equilibrium propagation\n",
    "    # equi_free = run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "    equi_nudge = run_network(equi_zero, T, W, bias, training_target, beta, input_index, output_index)\n",
    "        \n",
    "    # Calculate the gradient for the weights\n",
    "    gW, gh = paras_gradient(equi_nudge, equi_zero, bias, beta)\n",
    "\n",
    "    # Update the weights and bias\n",
    "    W = W_0 - study_rate/beta*gW\n",
    "    bias = bias_0 - study_rate/beta*gh\n",
    "\n",
    "    # Formulate the output\n",
    "\n",
    "\n",
    "    return phase_shape, W, bias, cost, training_data, training_target, training_paras, model_paras, equi_zero, False\n",
    "\n",
    "@partial(jax.jit, static_argnames=['training_paras'])\n",
    "def consecutive_jax_training(W_0, bias_0, training_data, training_target, training_paras, N_ev, dt, input_index, output_index, ext_init_phase_0=0, random_flag=False): \n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    #N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N = jnp.shape(W_0)[0]\n",
    "    N_sample = jnp.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "\n",
    "    cost = jnp.zeros(N_epoch)\n",
    "    phase_shape = jnp.zeros([N_sample, N])\n",
    "\n",
    "    # Training with fori_loop\n",
    "    running_training_paras = (beta, study_rate)\n",
    "    running_model_paras = (N_ev, dt, input_index, output_index)\n",
    "    initial_network = (phase_shape, W_0, bias_0, cost, training_data, training_target, running_training_paras, running_model_paras, ext_init_phase_0, random_flag)\n",
    "    \n",
    "    #time the training\n",
    "    final_network = jax.lax.fori_loop(0, N_epoch, consecutive_weights_update, initial_network)\n",
    "    \n",
    "    W = final_network[1]\n",
    "    bias = final_network[2]\n",
    "    cost = final_network[3]\n",
    "    \n",
    "\n",
    "    return cost, W, bias\n",
    "\n",
    "\n",
    "#--------------------------Do multiple training in single function------------------------\n",
    "\n",
    "@jax.jit\n",
    "def loop_jax_training(k, task_paras):\n",
    "    # Here W_0 and bias_0 are lists of weights and bias that are generated outside the function\n",
    "    cost_list, cost, W_0, bias_0, training_data, training_target, training_paras, model_paras = task_paras\n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N_sample = jnp.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    N_cell = np.shape(W_0)[1]\n",
    "    phase_shape = jnp.zeros([N_sample, N_cell])\n",
    "\n",
    "    # Prepare parameters for training with fori_loop\n",
    "    running_training_paras = (beta, study_rate)\n",
    "    running_model_paras = (N_ev, dt, input_index, output_index)\n",
    "    initial_network = (phase_shape, W_0[k,:,:], bias_0[k,:,:], cost, training_data, training_target, running_training_paras, running_model_paras, 0, False)\n",
    "\n",
    "    #time the training\n",
    "    final_network = jax.lax.fori_loop(0, N_epoch, weights_update, initial_network)\n",
    "    \n",
    "    W = final_network[1]\n",
    "    bias = final_network[2]\n",
    "    new_cost = final_network[3]\n",
    "    \n",
    "    # Formulate the output\n",
    "    W_trained = W_0.at[k,:,:].set(W)\n",
    "    bias_trained = bias_0.at[k,:,:].set(bias)\n",
    "    cost_list = cost_list.at[k,:].set(new_cost)\n",
    "    \n",
    "    return cost_list, cost, W_trained, bias_trained, training_data, training_target, training_paras, model_paras\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['training_paras'])\n",
    "def multiple_jax_training(N_run, cost_list_0, cost_0, W_0, bias_0, training_data, training_target, training_paras, model_paras):\n",
    "    task_paras = cost_list_0, cost_0[0], W_0, bias_0, training_data, training_target, training_paras, model_paras\n",
    "    \n",
    "    # time the training\n",
    "    trained_networks = jax.lax.fori_loop(0, N_run, loop_jax_training, task_paras)\n",
    "    \n",
    "    W_list = trained_networks[2]\n",
    "    bias_list = trained_networks[3]\n",
    "    cost_list = trained_networks[0]\n",
    "    \n",
    "    return cost_list\n",
    "\n",
    "#@partial(jax.jit, static_argnames=['training_paras', 'training_data', 'training_target'])\n",
    "def hv_jax_training(W_0, bias_0, training_data, training_target, training_paras, N_ev, dt, input_index, output_index, ext_init_phase_0=0, random_flag=False):\n",
    "    # Train many networks together in parallelism\n",
    "    \n",
    "    multiple_run_network = jax.vmap(run_network, (0,None,0,0,None,None,None,None))\n",
    "    multiple_paras_gradient = jax.vmap(paras_gradient, (0,0,0,None))\n",
    "    multiple_cost_function = jax.vmap(cost_function, (0,None,None))\n",
    "    \n",
    "    @jax.jit\n",
    "    def hv_update(k,present_network):\n",
    "        \n",
    "        W_0, bias_0, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag = present_network\n",
    "        beta, study_rate = training_paras\n",
    "        N_ev, dt, input_index, output_index = model_paras\n",
    "        N_data = jnp.shape(training_data)[0]\n",
    "        N_run = jnp.shape(W_0)[0]\n",
    "        N = jnp.shape(W_0)[1]\n",
    "        T = N_ev * dt\n",
    "\n",
    "        #initial phase\n",
    "        phase_0 = jnp.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "        phase_0 = jnp.asarray(phase_0)\n",
    "        phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "        phase_0 = jnp.tensordot(jnp.ones(N_run), phase_0, 0)\n",
    "\n",
    "        # Calculate the cost function\n",
    "        equi_zero = multiple_run_network(phase_0, T, W_0[k], bias_0[k], training_target, 0, input_index, output_index)\n",
    "        cost_list = multiple_cost_function(equi_zero, training_target, output_index)\n",
    "        cost = cost.at[:,k].set( jnp.mean(cost_list, axis=1) )\n",
    "\n",
    "        # Update the weights\n",
    "        # Run the free and nudge phase for equilibrium propagation\n",
    "        # equi_free = run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "        equi_nudge = multiple_run_network(equi_zero, T, W_0, bias_0, training_target, beta, input_index, output_index)\n",
    "\n",
    "        # Calculate the gradient for the weights\n",
    "        gW, gh = multiple_paras_gradient(equi_nudge, equi_zero, bias_0, beta)\n",
    "\n",
    "        # Update the weights and bias\n",
    "        W = W_0 - study_rate/beta*gW\n",
    "        bias = bias_0 - study_rate/beta*gh\n",
    "\n",
    "        return W, bias, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag\n",
    "    \n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    N_run = jnp.shape(W_0)[0]\n",
    "    \n",
    "    cost_list_0 = jnp.zeros([N_run, N_epoch])\n",
    "    \n",
    "    running_training_paras = (beta, study_rate)\n",
    "    running_model_paras = (N_ev, dt, input_index, output_index)\n",
    "    \n",
    "    initial_networks = W_0, bias_0, cost_list_0, training_data, training_target, running_training_paras, running_model_paras, ext_init_phase_0, random_flag\n",
    "    \n",
    "    trained_networks = jax.lax.fori_loop(0, N_epoch, hv_update, initial_networks)\n",
    "    W, bias, cost = trained_networks[0], trained_networks[1], trained_networks[2]\n",
    "    \n",
    "    return cost, W, bias\n",
    "    \n",
    "vmul_jax_training = jax.vmap(jax_training, (0,0,None,None,(None,None,None),None,None,None,None,None,None))\n",
    "vmul_consecutive_jax_training = jax.vmap(consecutive_jax_training, (0,0,None,None,(None,None,None),None,None,None,None,None,None))\n",
    "\n",
    "\n",
    "#=================CPU training===================\n",
    "# Use scipy\n",
    "#------------------calculate equilibriums-------------------\n",
    "def sp_cal_dphase(phase):\n",
    "    # Calculate dphase[i,j]=phase[i]-phase[j]\n",
    "    \n",
    "    N = np.shape(phase)[1]\n",
    "    aux_ones=np.ones(N)\n",
    "    phase_mat=np.tensordot(aux_ones,phase,0)\n",
    "    phase_i=np.transpose(phase_mat,(1,2,0))\n",
    "    phase_j=np.transpose(phase_i,(0,2,1))\n",
    "    dphase=phase_i-phase_j\n",
    "        \n",
    "    return dphase\n",
    "\n",
    "def sp_total_force(t, con_phase, W, bias, target, beta, input_index, output_index):\n",
    "    \n",
    "    N = np.shape(W)[0]\n",
    "    Nh = int(np.shape(con_phase)[0]/N)\n",
    "    phase = np.reshape(con_phase,(Nh,N))\n",
    "\n",
    "    dphase = sp_cal_dphase(phase)\n",
    "    F0 = np.sum(W*np.sin(dphase),2)\n",
    "\n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    N_data=np.shape(phase)[0]\n",
    "    psi=np.tensordot(np.ones(N_data),psi,0)\n",
    "    \n",
    "    #print(target)\n",
    "    #print(phase[:,output_index])\n",
    "\n",
    "    output_phase = np.reshape(phase[:,output_index],np.shape(target))\n",
    "\n",
    "    F1 = -h * np.sin(phase-psi)\n",
    "\n",
    "    F2 = np.zeros(np.shape(phase))\n",
    "    temp_F2 = -np.sin(output_phase-target)\n",
    "    F2[:,output_index] = temp_F2\n",
    "    \n",
    "    '''\n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    temp_F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    F3 = F3.at[:,output_index].set( jnp.reshape(temp_F3, jnp.shape(output_phase[:,output_index])) )\n",
    "    '''\n",
    "    \n",
    "    #F3 = F3.at[:,output_index].set( -jnp.sin(0.5*(output_phase-target)) )\n",
    "    \n",
    "    # Apply cross entropy loss functoin\n",
    "    \n",
    "    F3 = np.zeros(np.shape(phase))\n",
    "    M1 = -np.sin(output_phase-target)\n",
    "    M2 = 1.0*np.ones(np.shape(output_phase)) + np.cos(output_phase-target)\n",
    "    F3[:,output_index] = M1/M2\n",
    "    \n",
    "\n",
    "    F = -F0 + F1 + 0*beta*F2 + beta*F3\n",
    "    F[:,input_index] = np.zeros([Nh, len(input_index)])\n",
    "    \n",
    "        \n",
    "    return np.concatenate(F)\n",
    "\n",
    "def sp_ode_total_force(con_phase, t, W, bias, target, beta, input_index, output_index):\n",
    "    return sp_total_force(t, con_phase, W, bias, target, beta, input_index, output_index)\n",
    "\n",
    "\n",
    "def sp_run_network(phase_0, T, W, bias, target, beta, input_index, output_index):\n",
    "    # Set the initial conditions\n",
    "    N_data, N = np.shape(phase_0)\n",
    "    \n",
    "    con_phase_0 = np.concatenate(phase_0)\n",
    "    \n",
    "    # Evolve with scipy\n",
    "    \n",
    "    t_span = np.array([0.,T])\n",
    "    \n",
    "    solution = scipy.integrate.solve_ivp(sp_total_force, t_span, con_phase_0, method='LSODA', args=[W, bias, target, beta, input_index, output_index], rtol = 1.4e-8, atol = 1.4e-8)\n",
    "    \n",
    "    L = len(solution.t)\n",
    "    con_phase = solution.y[:,L-1]\n",
    "    phase = np.reshape(con_phase, [N_data, N])\n",
    "\n",
    "    return phase\n",
    "\n",
    "\n",
    "#------------------------train the parameters-----------------------\n",
    "\n",
    "\n",
    "def sp_weights_gradient(equi_nudge, equi_free, beta):\n",
    "    # This is to calculate the contribution to weights gradient of free term in the internal energy\n",
    "    # The expression is: ( -cos(phi_i-phi_j)(\\beta!=0)+cos(phi_i-phi_j)(\\beta=0) )\n",
    "    N_data = np.shape(equi_free)[0]\n",
    "    nudge_dphase = sp_cal_dphase(equi_nudge)\n",
    "    free_dphase = sp_cal_dphase(equi_free)\n",
    "    gradient_list = (-np.cos(nudge_dphase)+np.cos(free_dphase))\n",
    "    #print(jnp.shape(gradient_list))\n",
    "    gradient = np.mean(gradient_list,axis=0)\n",
    "        \n",
    "    return gradient\n",
    "\n",
    "def sp_bias_gradient(equi_nudge, equi_free, bias, beta): \n",
    "        \n",
    "    N_sample = np.shape(equi_free)[0]\n",
    "    gradient = 0\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "\n",
    "    h = np.tensordot(np.ones(np.shape(equi_free)[0]), h, 0)\n",
    "    psi = np.tensordot(np.ones(np.shape(equi_free)[0]), psi, 0)\n",
    "\n",
    "    g_h = np.cos(equi_free-psi) - np.cos(equi_nudge-psi)\n",
    "    g_psi = h*np.sin(equi_free-psi) - h*np.sin(equi_nudge-psi)\n",
    "\n",
    "    g_h = np.mean(g_h,axis=0)\n",
    "    g_psi = np.mean(g_psi,axis=0)\n",
    "\n",
    "    return np.asarray([g_h, g_psi])\n",
    "\n",
    "\n",
    "def sp_paras_gradient(equi_nudge, equi_free, bias, beta): \n",
    "\n",
    "    return sp_weights_gradient(equi_nudge, equi_free, beta) , sp_bias_gradient(equi_nudge, equi_free, bias, beta)\n",
    "\n",
    "\n",
    "def sp_train_network(W_0, bias_0, training_data, training_target, training_paras, model_paras, ext_init_phase_0=0, random_flag=False):\n",
    "    \n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N_data = np.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    cost = np.zeros(N_epoch)\n",
    "    \n",
    "\n",
    "    # training\n",
    "    for k in range(0,N_epoch):\n",
    "        phase_0 = np.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "        phase_0[:,input_index] = training_data\n",
    "        #print(phase_0)\n",
    "\n",
    "        #print(W.shape, phase_0.shape, bias.shape)\n",
    "\n",
    "        # Calculate the cost function\n",
    "        equi_zero = sp_run_network(phase_0, T, W, bias, training_target, 0, input_index, output_index)\n",
    "        cost_list = cost_function(equi_zero, training_target, output_index)\n",
    "        cost[k] = np.sum(cost_list)/N_data\n",
    "\n",
    "        # Run the free and nudge phase for equilibrium propagation\n",
    "        equi_free = sp_run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "        equi_nudge = sp_run_network(phase_0, T, W, bias, training_target, beta, input_index, output_index)\n",
    "        \n",
    "        #print(equi_zero, '\\n\\n', equi_free, '\\n\\n', equi_nudge, '\\n\\n')\n",
    "        \n",
    "        #print(\"equi_zero= \\n\", equi_zero, \"\\n equi_free= \\n\", equi_free, \"\\n equi_nudge= \\n\", equi_nudge)\n",
    "        \n",
    "        # Calculate the gradient for the weights\n",
    "        gW, gh = sp_paras_gradient(equi_nudge, equi_zero, bias, beta)\n",
    "        \n",
    "        #print(gW,'\\n\\n', gh)\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        W = W - study_rate*gW/2/beta\n",
    "        bias = bias - study_rate*gh/2/beta\n",
    "\n",
    "    return cost, W, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5609b186",
   "metadata": {
    "id": "5609b186",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the network with optimization methods in jaxopt\n",
    "\n",
    "@jax.jit\n",
    "def opt_total_energy(variable_phase, W, bias, input_data, target, input_index, variable_index, output_index, beta):\n",
    "    \n",
    "    N = jnp.shape(input_index)[0]+jnp.shape(variable_index)[0]\n",
    "    \n",
    "    #Set the phase to calculate the energy\n",
    "    phase = jnp.zeros(N)\n",
    "    phase = phase.at[input_index].set(input_data)\n",
    "\n",
    "    phase = phase.at[variable_index].set(variable_phase)\n",
    "    \n",
    "    #Calculate the internal energy\n",
    "    aux_ones = jnp.ones(N)\n",
    "    phase_mat = jnp.tensordot(aux_ones, phase, 0)\n",
    "    dphase = phase_mat - jnp.transpose(phase_mat, [1,0])\n",
    "    E_in = -0.5 * jnp.sum(W * jnp.cos(dphase))\n",
    "    \n",
    "    #Calculate the energy from the bias terms\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "    E_bias = -jnp.sum(h * jnp.cos(phase-psi))\n",
    "    \n",
    "    #Calculate the energy from the cost function\n",
    "    output_phase = phase[output_index]\n",
    "    E_cost = -jnp.sum(jnp.log(1 + jnp.cos(output_phase - target)))\n",
    "\n",
    "    return E_in + E_bias + beta * E_cost\n",
    "\n",
    "@jax.jit\n",
    "def opt_total_force(variable_phase, W, bias, input_data, target, input_index, variable_index, output_index, beta):\n",
    "    # This calculate the gradient of the energy (-F_total)\n",
    "    phase = jnp.concatenate((input_data, variable_phase))\n",
    "    phase_mat = jnp.asarray([phase])\n",
    "    dphase = -phase_mat + jnp.transpose(phase_mat,[1,0])\n",
    "    \n",
    "    output_phase = phase[output_index]\n",
    "    doutput = output_phase-target\n",
    "    \n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    N_cell = jnp.shape(phase)[0]\n",
    "    N_temp = N_cell - jnp.shape(output_index)[0]\n",
    "    \n",
    "    F0 = jnp.sum(W*jnp.sin(dphase),axis=1)\n",
    "    \n",
    "    F1 = -h * jnp.sin(phase-psi)\n",
    "    \n",
    "    \n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    M1 = -jnp.sin(output_phase-target)\n",
    "    M2 = (1.00)*jnp.ones(jnp.shape(output_phase)) + jnp.cos(output_phase-target)\n",
    "\n",
    "    #temp_F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    F3 = F3.at[output_index].set(M1/M2)\n",
    "    \n",
    "    \n",
    "    #F3 = F3.at[:,output_index].set( -jnp.sin(0.5*(output_phase-target)) )\n",
    "    \n",
    "    # Apply cross entropy loss function\n",
    "\n",
    "    F = -F0 + F1 + beta*F3\n",
    "    \n",
    "    F = F[variable_index]\n",
    "    \n",
    "    return F\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def opt_energy_with_grad(variable_phase, args):\n",
    "    W, bias, input_data, target, input_index, variable_index, output_index, beta = args\n",
    "    E = opt_total_energy(variable_phase, W, bias, input_data, target, input_index, variable_index, output_index, beta)\n",
    "    F = opt_total_force(variable_phase, W, bias, input_data, target, input_index, variable_index, output_index, beta)\n",
    "\n",
    "    return E, -F\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def single_opt_run_network(phase_0, W, bias, target, beta, input_index, variable_index, output_index):\n",
    "    #solver = jaxopt.LBFGS(opt_energy_with_grad, value_and_grad=True, maxiter=10000, tol=1e-8)\n",
    "    solver = jaxopt.GradientDescent(opt_energy_with_grad, value_and_grad=True, maxiter=10000, tol=1e-8)\n",
    "    input_data = phase_0[input_index]\n",
    "    variable_phase_0 = phase_0[variable_index]\n",
    "\n",
    "    args = W, bias, input_data, target, input_index, variable_index, output_index, beta\n",
    "\n",
    "    res = solver.run(variable_phase_0, args)\n",
    "\n",
    "    variable_phase = res.params\n",
    "    phase = jnp.concatenate((input_data, variable_phase))\n",
    "\n",
    "    return phase\n",
    "\n",
    "v_opt_run_network = jax.vmap(single_opt_run_network, (0,None,None,0,None,None,None,None))\n",
    "\n",
    "@jax.jit\n",
    "def opt_run_network_inloop(k, args):\n",
    "\n",
    "    phase, phase_0, W, bias, target, beta, input_index, variable_index, output_index = args\n",
    "\n",
    "    #solver = jaxopt.LBFGS(opt_energy_with_grad, value_and_grad=True, maxiter=10000, tol=1e-8)\n",
    "    solver = jaxopt.GradientDescent(opt_energy_with_grad, value_and_grad=True, maxiter=10000, tol=1e-8)\n",
    "    input_data = phase_0[k,input_index]\n",
    "    variable_phase_0 = phase_0[k,variable_index]\n",
    "\n",
    "    init_args = W, bias, input_data, target[k,:], input_index, variable_index, output_index, beta\n",
    "\n",
    "    res = solver.run(variable_phase_0, init_args)\n",
    "\n",
    "    variable_phase = res.params\n",
    "    phase = phase.at[k,:].set(jnp.concatenate((input_data, variable_phase)))\n",
    "\n",
    "    return phase, phase_0, W, bias, target, beta, input_index, variable_index, output_index\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def mul_opt_run_network(phase_0, W, bias, target, beta, input_index, variable_index, output_index):\n",
    "    phase = jnp.zeros(phase_0.shape)\n",
    "    N_data, N = phase_0.shape\n",
    "    args_0 = phase, phase_0, W, bias, target, beta, input_index, variable_index, output_index\n",
    "    result = jax.lax.fori_loop(0, N_data,opt_run_network_inloop, args_0)\n",
    "\n",
    "    phase = result[0]\n",
    "\n",
    "    return phase\n",
    "\n",
    "\n",
    "#==================Eequivalence verification=================\n",
    "# Verify the equivalence between opt and ode\n",
    "\n",
    "def dE_dparas(equi, bias):\n",
    "    # This is to calculate the contribution to weights gradient of free term in the internal energy\n",
    "    # The expression is: ( -cos(phi_i-phi_j)(\\beta!=0)+cos(phi_i-phi_j)(\\beta=0) )\n",
    "    equi_mat = jnp.asarray([equi])\n",
    "    free_dphase = equi_mat - jnp.transpose(equi_mat)\n",
    "    g_W = -jnp.cos(free_dphase)\n",
    "\n",
    "    #Calculate derivative over bias\n",
    "\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "\n",
    "    #h = jnp.tensordot(jnp.ones(jnp.shape(equi)[0]), h, 0)\n",
    "    #psi = jnp.tensordot(jnp.ones(jnp.shape(equi)[0]), psi, 0)\n",
    "\n",
    "    #print(equi.shape, psi.shape, h.shape)\n",
    "    g_h = -jnp.cos(equi-psi)\n",
    "    g_psi = -h*jnp.sin(equi-psi)\n",
    "        \n",
    "    return g_W, g_h, g_psi\n",
    "\n",
    "mul_dE_dparas = jax.vmap(dE_dparas, (0,None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42bc2edd",
   "metadata": {
    "id": "42bc2edd",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def qualitative_cost(phase, target, output_index, tol):\n",
    "    # Calculate the cost function for each sample\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "    output_phase = jnp.reshape(phase[:,output_index],jnp.shape(target))\n",
    "    \n",
    "    doutput = output_phase-target\n",
    "    cost_mat = jnp.ones(jnp.shape(doutput))-jnp.cos(doutput)\n",
    "    cost_list = jnp.sum( (cost_mat>tol)/2 )\n",
    "        \n",
    "    return cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6209a601",
   "metadata": {
    "id": "6209a601",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running and training XY network with layer structure\n",
    "#=======================================Running without layer structure=============================================\n",
    "@jax.jit\n",
    "def single_force(t, phase, W, bias, target, input_index, output_index, beta):\n",
    "    # This calculate the gradient of the energy (-F_total)\n",
    "    phase_mat = jnp.asarray([phase])\n",
    "    dphase = -phase_mat + jnp.transpose(phase_mat,[1,0])\n",
    "    \n",
    "    output_phase = phase[output_index]\n",
    "    \n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    N_cell = jnp.shape(phase)[0]\n",
    "    N_temp = N_cell - jnp.shape(output_index)[0]\n",
    "    \n",
    "    F0 = jnp.sum(W*jnp.sin(dphase),axis=1)\n",
    "    \n",
    "    F1 = -h * jnp.sin(phase-psi)\n",
    "    \n",
    "    \n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    M1 = -jnp.sin(output_phase-target)\n",
    "    M2 = (1.00)*jnp.ones(jnp.shape(output_phase)) + jnp.cos(output_phase-target)\n",
    "    F3 = F3.at[output_index].set(M1/M2)\n",
    "    \n",
    "    #temp_F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    #F3 = F3.at[output_index].set( -jnp.sin(0.5*(output_phase-target)) )\n",
    "    \n",
    "    # Apply cross entropy loss functoin\n",
    "\n",
    "    F = -F0 + F1 + beta*F3\n",
    "    \n",
    "    F = F.at[input_index].set(0)\n",
    "    \n",
    "    return F\n",
    "\n",
    "@jax.jit\n",
    "def single_run_network(phase_0, T, W, bias, target, beta, input_index, output_index):\n",
    "    # Solve the equation with diffrax\n",
    "    # Set parameter for diffrax\n",
    "    rtol = 1e-8\n",
    "    atol = 1e-8\n",
    "    t_span = [0,T]\n",
    "    #saveat = np.linspace(*t_span, 10).tolist()\n",
    "    odefunc = lambda t, phase, args: single_force(t, phase, W, bias, target, input_index, output_index, beta)\n",
    "    eqs = diffrax.ODETerm(odefunc)\n",
    "    \n",
    "    # Use 4th Runge Kutta\n",
    "    solver = diffrax.Tsit5()\n",
    "    \n",
    "    #Use 5th Kvaerno for stiff case\n",
    "    #solver = diffrax.Kvaerno4()\n",
    "    \n",
    "    stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
    "    #t = diffrax.SaveAt(ts=saveat)\n",
    "\n",
    "    # Solve the ODE\n",
    "    solution = diffrax.diffeqsolve(eqs, solver, t0=t_span[0], t1=t_span[1], dt0 = None, y0=phase_0,\n",
    "                               stepsize_controller=stepsize_controller, max_steps=10000000)\n",
    "    \n",
    "    L = len(solution.ts)\n",
    "    \n",
    "    phase = solution.ys[L-1,:]\n",
    "\n",
    "    return phase\n",
    "\n",
    "@jax.jit\n",
    "def mul_run_network(phase_0, T, W, bias, target, beta, input_index, output_index):\n",
    "    odefunc = lambda phase_0, target: single_run_network(phase_0, T, W, bias, target, beta, input_index, output_index)\n",
    "    phase = jax.tree_map(odefunc, list(phase_0), list(target))\n",
    "    return jnp.asarray(phase)\n",
    "\n",
    "v_run_network = jax.vmap(single_run_network, (0,None,None,None,0,None,None,None))\n",
    "\n",
    "\n",
    "#=================================Calculate the energy and force with layered structure=================================\n",
    "@jax.jit\n",
    "def backward_force(phase, W, phase_next):\n",
    "    mphase = phase[None,:]\n",
    "    mphase_next = phase_next[None,:]\n",
    "    return jnp.sum(W * jnp.sin(jnp.transpose(mphase, [1,0]) - mphase_next), axis=1)\n",
    "\n",
    "@jax.jit\n",
    "def forward_force(phase, W, phase_last):\n",
    "    mphase = phase[None,:]\n",
    "    mphase_last = phase_last[None,:]\n",
    "    return jnp.sum(W * jnp.sin(mphase - jnp.transpose(mphase_last, [1,0])), axis=0)\n",
    "\n",
    "@jax.jit\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "@jax.jit\n",
    "def layered_free_energy(phase1, W, phase2):\n",
    "    #This calculate the energy of free energy\n",
    "    '''\n",
    "        W: W^{n,n+1}, coupling between the n-th layer and n+1-th layer. n = 0,1,2...L, L+1 the number of layers in the network. \n",
    "        phase: phi^n\n",
    "        next_phase : phi^{n+1}\n",
    "    '''\n",
    "    \n",
    "    E = -jnp.sum( W * jnp.cos(jnp.transpose(phase1[None,:]) - phase2[None,:]) )\n",
    "    return E\n",
    "\n",
    "@jax.jit\n",
    "def layered_energy(phase, WL, bias, target, structure_shape, beta):\n",
    "    \n",
    "    # Calculate the free energy layer by layer\n",
    "    # Split and translate the phase according to layer structure\n",
    "    split_points = jax.tree_map(jnp.size, structure_shape)\n",
    "    phase_list = jnp.split(phase, split_points)\n",
    "    \n",
    "    phase1_list = phase_list.copy()\n",
    "    phase2_list = phase_list.copy()\n",
    "    \n",
    "    del phase1_list[-1]\n",
    "    del phase2_list[0]\n",
    "    \n",
    "    E0 = jnp.sum( jnp.asarray(jax.tree_map(layered_free_energy, phase1_list, WL, phase2_list)) )\n",
    "    \n",
    "    #Calculate the energy from the bias terms\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "    E_bias = -jnp.sum(h * jnp.cos(phase-psi))\n",
    "    \n",
    "    #Calculate the energy from the cost function\n",
    "    output_phase = phase_list[-1]\n",
    "    E_cost = -jnp.sum(jnp.log(1 + jnp.cos(output_phase - target)))\n",
    "    \n",
    "    #Sum all the terms and get the total energy\n",
    "    E = E0 + E_bias + beta * E_cost\n",
    "    \n",
    "    return E\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def layered_force(t, phase, WL, bias, target, structure_shape, beta):\n",
    "    split_points = jax.tree_map(jnp.size, structure_shape)\n",
    "    phase_list = jnp.split(phase, split_points)\n",
    "    L = len(split_points)\n",
    "    phase_next_list = phase_list.copy()\n",
    "    phase_next_list.append(phase_list[len(phase_list)-1])\n",
    "    del phase_next_list[0]\n",
    "    \n",
    "    phase_last_list = phase_list.copy()\n",
    "    phase_last_list.insert(0, phase_list[0])\n",
    "    del phase_last_list[L+1]\n",
    "    \n",
    "    WL_back = WL.copy()\n",
    "    WL_back.append(jnp.zeros([phase_list[L].shape[0], phase_list[L].shape[0]]))\n",
    "    WL_forward = WL.copy()\n",
    "    WL_forward.insert(0, jnp.zeros([phase_list[0].shape[0], phase_list[0].shape[0]]))\n",
    "    \n",
    "    # Get the backward and forward force\n",
    "    BF = jax.tree_map(backward_force, phase_list, WL_back, phase_next_list)\n",
    "    FF = jax.tree_map(forward_force, phase_list, WL_forward, phase_last_list)\n",
    "    \n",
    "    FL0 = jax.tree_map(add, BF, FF)\n",
    "    FL0[0] = FL0[0]*0\n",
    "    \n",
    "    # Calculate bias force\n",
    "    \n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    F1 = -h * jnp.sin(phase-psi)\n",
    "    F1 = F1.at[0:split_points[0]].set(0)\n",
    "    \n",
    "    # Calculate the force from cost function\n",
    "    \n",
    "    output_phase = phase_list[len(phase_list)-1]\n",
    "    M1 = -jnp.sin(output_phase - target)\n",
    "    M2 = (1.00)*jnp.ones(jnp.shape(output_phase)) + jnp.cos(output_phase-target)\n",
    "    #F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    F3 = M1/M2\n",
    "    \n",
    "    FL0[len(FL0)-1] = FL0[len(FL0)-1] - beta*F3\n",
    "    F = -jnp.concatenate(FL0) + F1\n",
    "    \n",
    "    #Merge the forces together and concatenate\n",
    "    \n",
    "    \n",
    "    \n",
    "    #return phase_list, phase_last_list, phase_last_list, WL_forward, WL_back, BF, FF\n",
    "    return F\n",
    "    \n",
    "\n",
    "#=======================================Run network with solving ODE======================================\n",
    "@jax.jit\n",
    "def single_layer_run_network(phase_0, T, WL, bias, target, structure_shape, beta):\n",
    "    #run network with layer structure for single inputs\n",
    "    # Solve the equation with diffrax\n",
    "    # Set parameter for diffrax\n",
    "    rtol = 1e-8\n",
    "    atol = 1e-8\n",
    "    t_span = [0,T]\n",
    "    #saveat = np.linspace(*t_span, 10).tolist()\n",
    "    odefunc = lambda t, phase, args: layered_force(t, phase, WL, bias, target, structure_shape, beta)\n",
    "    eqs = diffrax.ODETerm(odefunc)\n",
    "    \n",
    "    # Use 4th Runge Kutta\n",
    "    solver = diffrax.Tsit5()\n",
    "    \n",
    "    #Use 5th Kvaerno for stiff case\n",
    "    #solver = diffrax.Kvaerno4()\n",
    "    \n",
    "    stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
    "    #t = diffrax.SaveAt(ts=saveat)\n",
    "\n",
    "    # Solve the ODE\n",
    "    solution = diffrax.diffeqsolve(eqs, solver, t0=t_span[0], t1=t_span[1], dt0 = None, y0=phase_0,\n",
    "                               stepsize_controller=stepsize_controller, max_steps=10000000)\n",
    "    \n",
    "    L = len(solution.ts)\n",
    "    \n",
    "    phase = solution.ys[L-1,:]\n",
    "\n",
    "    return phase\n",
    "\n",
    "@jax.jit\n",
    "def ode_run_layered_network(phase_0, T, WL, bias, target, structure_shape, beta):\n",
    "    odefunc = lambda phase_0, target: single_layer_run_network(phase_0, T, WL, bias, target, structure_shape, beta)\n",
    "    phase = jax.tree_map(odefunc, list(phase_0), list(target))\n",
    "    return jnp.asarray(phase)\n",
    "\n",
    "@jax.jit\n",
    "def v_run_layered_network(phase_0, T, WL, bias, target, structure_shape, beta):\n",
    "    odefunc = lambda phase_0, target: single_layer_run_network(phase_0, T, WL, bias, target, structure_shape, beta)\n",
    "    phase = jax.vmap(odefunc, (0,0))(phase_0, target)\n",
    "    return phase\n",
    "\n",
    "\n",
    "#==========================================Running with optimization methods==========================================\n",
    "\n",
    "@jax.jit\n",
    "def opt_layered_energy(variable_phase, input_data, WL, bias, target, structure_shape, beta):\n",
    "    phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    return layered_energy(phase, WL, bias, target, structure_shape, beta)\n",
    "\n",
    "@jax.jit\n",
    "def opt_layered_grad(variable_phase, input_data, WL, bias, target, structure_shape, beta):\n",
    "    variable_index = jnp.arange(structure_shape[0].shape[0], bias.shape[1])\n",
    "    phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    grad_E = -layered_force(0, phase, WL, bias, target, structure_shape, beta)[variable_index]\n",
    "    \n",
    "    return grad_E\n",
    "\n",
    "@jax.jit\n",
    "def opt_layered_energy_with_grad(variable_phase, input_data, WL, bias, target, structure_shape, beta):\n",
    "    E = opt_layered_energy(variable_phase, input_data, WL, bias, target, structure_shape, beta)\n",
    "    grad_E = opt_layered_grad(variable_phase, input_data, WL, bias, target, structure_shape, beta)\n",
    "    \n",
    "    return E, grad_E\n",
    "\n",
    "@jax.jit\n",
    "def opt_single_layer_run_network(phase_0, WL, bias, target, structure_shape, beta):\n",
    "    # Run network for single set of input data\n",
    "    \n",
    "    input_index = jnp.arange(0, structure_shape[0].shape[0])\n",
    "    variable_index = jnp.arange(structure_shape[0].shape[0], bias.shape[1])\n",
    "    \n",
    "    input_data = phase_0[input_index]\n",
    "    variable_phase_0 = phase_0[variable_index]\n",
    "    \n",
    "    optfunc = lambda variable_phase: opt_layered_energy_with_grad(variable_phase, input_data, WL, bias, target, structure_shape, beta)\n",
    "    \n",
    "    solver = jaxopt.LBFGS(optfunc, value_and_grad=True, maxiter=10000, tol=1e-6)\n",
    "    #solver = jaxopt.GradientDescent(optfunc, value_and_grad=True, maxiter=10000, tol=1e-6)\n",
    "\n",
    "    #args = input_data, WL, bias, target, structure_shape, beta\n",
    "\n",
    "    res = solver.run(variable_phase_0)\n",
    "\n",
    "    variable_phase = res.params\n",
    "    phase = jnp.concatenate((input_data, variable_phase))\n",
    "\n",
    "    return phase\n",
    "\n",
    "@jax.jit\n",
    "def opt_run_layered_network(phase_0, WL, bias, target, structure_shape, beta):\n",
    "    runfunc = lambda phase_0, target: opt_single_layer_run_network(phase_0, WL, bias, target, structure_shape, beta)\n",
    "    phase = jax.tree_map(runfunc, list(phase_0), list(target))\n",
    "    return jnp.asarray(phase)\n",
    "\n",
    "@jax.jit\n",
    "def v_opt_run_layered_network(phase_0, WL, bias, target, structure_shape, beta):\n",
    "    runfunc = lambda phase_0, target: opt_single_layer_run_network(phase_0, WL, bias, target, structure_shape, beta)\n",
    "    phase = jax.vmap(runfunc, (0,0))(phase_0, target)\n",
    "    return phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "825ae807",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#====================================Run the network with lattice structure=================================\n",
    "#--------------------------------------Evolve the system with solving ODE-----------------------------------\n",
    "# moving phase\n",
    "@jax.jit\n",
    "def move_phase(phase):\n",
    "    phase_shape = phase.shape\n",
    "    sub_row = jnp.zeros([1, phase_shape[1]])\n",
    "    sub_col = jnp.zeros([phase_shape[0], 1])\n",
    "    \n",
    "    #phase_u[i,j] = phase[i+1, j]\n",
    "    phase_u = jnp.delete(phase, 0, axis=0)\n",
    "    #phase_u = jnp.concatenate((phase_u, sub_row), axis=0)\n",
    "    phase_u = jnp.concatenate((phase_u, phase[-1,:][None,:]), axis=0)\n",
    "    \n",
    "    #phase_d[i,j] = phase[i-1, j]\n",
    "    phase_d = jnp.delete(phase, phase_shape[0]-1, axis=0)\n",
    "    #phase_d = jnp.concatenate((sub_row, phase_d), axis=0)\n",
    "    phase_d = jnp.concatenate((phase[0,:][None,:], phase_d), axis=0)\n",
    "    \n",
    "    #phase_l[i,j] = phase[i, j+1]\n",
    "    phase_l = jnp.delete(phase, 0, axis=1)\n",
    "    #phase_l = jnp.concatenate((phase_l, sub_col), axis=1)\n",
    "    phase_l = jnp.concatenate((phase_l, phase[:,-1][:,None]), axis=1)\n",
    "    \n",
    "    #phase_r[i,j] = phase[i, j-1]\n",
    "    phase_r = jnp.delete(phase, phase_shape[1]-1, axis=1)\n",
    "    #phase_r = jnp.concatenate((sub_col, phase_r), axis=1)\n",
    "    phase_r = jnp.concatenate((phase[:,0][:,None], phase_r), axis=1)\n",
    "    \n",
    "    return jnp.asarray([phase_u, phase_d, phase_l, phase_r])\n",
    "\n",
    "@jax.jit\n",
    "def SquareLattice_total_energy(flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    dimension = lattice_form.shape\n",
    "    phase = jnp.reshape(flatten_phase, dimension)\n",
    "    moved_phase = move_phase(phase)\n",
    "    E0 = -0.5 * jnp.sum(W * jnp.cos(phase - moved_phase))\n",
    "    \n",
    "    h = bias[0,...]\n",
    "    psi = bias[1,...]\n",
    "    \n",
    "    E1 = -jnp.sum(h*jnp.cos(phase - psi))\n",
    "    \n",
    "    phase_out = phase[-1, output_index]\n",
    "    E2 = -jnp.sum(jnp.log(1+jnp.cos(phase_out - target)))\n",
    "    \n",
    "    return E0 + E1 + beta*E2\n",
    "\n",
    "@jax.jit\n",
    "def SquareLattice_total_force(t, flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    dimension = lattice_form.shape\n",
    "    phase = jnp.reshape(flatten_phase, dimension)\n",
    "    moved_phase = move_phase(phase)\n",
    "    F0 = -jnp.sum(W * jnp.sin(phase - moved_phase), axis=0)\n",
    "    \n",
    "    h = bias[0,...]\n",
    "    psi = bias[1,...]\n",
    "    F1 = -h*jnp.sin(phase - psi)\n",
    "    \n",
    "    row_output_index = output_index - (dimension[0]-1)*dimension[1]\n",
    "    \n",
    "    phase_out = phase[-1, row_output_index]\n",
    "    F2 = -jnp.sin(phase_out - target)/(1+jnp.cos(phase_out - target))\n",
    "    \n",
    "    F = F1 + F0\n",
    "    \n",
    "    F_out = F[-1, row_output_index]\n",
    "    F_out = F_out + F2*beta\n",
    "    \n",
    "    F = F.at[0,input_index].set(0)\n",
    "    F = F.at[-1,row_output_index].set(F_out)\n",
    "    \n",
    "    return jnp.concatenate(F)\n",
    "\n",
    "@jax.jit\n",
    "def single_lattice_run_network(phase_0, T, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    #run network with layer structure for single inputs\n",
    "    # Solve the equation with diffrax\n",
    "\n",
    "    # Transform a lattice configuration into a sequence\n",
    "    flatten_phase_0 = jnp.concatenate(phase_0)\n",
    "    \n",
    "    # Set parameter for diffrax\n",
    "    rtol = 1e-8\n",
    "    atol = 1e-8\n",
    "    t_span = [0,T]\n",
    "    #saveat = np.linspace(*t_span, 10).tolist()\n",
    "    odefunc = lambda t, flatten_phase_0, args: SquareLattice_total_force(t, flatten_phase_0, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    eqs = diffrax.ODETerm(odefunc)\n",
    "    \n",
    "    # Use 4th Runge Kutta\n",
    "    solver = diffrax.Tsit5()\n",
    "    \n",
    "    #Use 5th Kvaerno for stiff case\n",
    "    #solver = diffrax.Kvaerno4()\n",
    "    \n",
    "    stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
    "    #t = diffrax.SaveAt(ts=saveat)\n",
    "    \n",
    "    # Solve the ODE\n",
    "    solution = diffrax.diffeqsolve(eqs, solver, t0=t_span[0], t1=t_span[1], dt0 = None, y0=flatten_phase_0,\n",
    "                               stepsize_controller=stepsize_controller, max_steps=10000000)\n",
    "    \n",
    "    L = len(solution.ts)\n",
    "    \n",
    "    phase = solution.ys[L-1,:].reshape(phase_0.shape)\n",
    "\n",
    "    return phase\n",
    "\n",
    "v_run_lattice_network = jax.vmap(single_lattice_run_network, (0,None,None,None,0,None,None,None,None))\n",
    "\n",
    "#--------------------------------------Evolve the system via optimization methods-----------------------------------------\n",
    "@jax.jit\n",
    "def opt_SquareLattice_total_energy(variable_phase, input_data, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    flatten_phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    E = SquareLattice_total_energy(flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    return E\n",
    "\n",
    "@jax.jit\n",
    "def opt_SquareLattice_total_force(variable_phase, input_data, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    flatten_phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    F = SquareLattice_total_force(0, flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    return F\n",
    "\n",
    "@jax.jit\n",
    "def opt_SquareLattice_energy_with_grad(variable_phase, input_data, W, bias, target, lattice_form, input_index, variable_index, output_index, beta):\n",
    "    flatten_phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    E = SquareLattice_total_energy(flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    F = SquareLattice_total_force(0, flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    return E, -F[variable_index]\n",
    "\n",
    "@jax.jit\n",
    "def opt_single_SquareLattice_run_network(phase_0, W, bias, target, lattice_form, input_index, variable_index, output_index, beta):\n",
    "    # Run network for single set of input data\n",
    "    \n",
    "    flatten_phase_0 = jnp.concatenate(phase_0)\n",
    "    \n",
    "    input_data = flatten_phase_0[input_index]\n",
    "    variable_phase_0 = flatten_phase_0[variable_index]\n",
    "    \n",
    "    optfunc = lambda variable_phase: opt_SquareLattice_energy_with_grad(variable_phase, input_data, W, bias, target, lattice_form, input_index, variable_index, output_index, beta)\n",
    "    \n",
    "    solver = jaxopt.LBFGS(optfunc, value_and_grad=True, maxiter=10000, tol=1e-6)\n",
    "    #solver = jaxopt.GradientDescent(optfunc, value_and_grad=True, maxiter=10000, tol=1e-6)\n",
    "\n",
    "    #args = input_data, WL, bias, target, structure_shape, beta\n",
    "\n",
    "    res = solver.run(variable_phase_0)\n",
    "\n",
    "    variable_phase = res.params\n",
    "    flatten_phase = jnp.concatenate((input_data, variable_phase))\n",
    "\n",
    "    return jnp.reshape(flatten_phase, phase_0.shape)\n",
    "\n",
    "v_opt_run_SquareLattice_network = jax.vmap(opt_single_SquareLattice_run_network, (0,None,None,0,None,None,None,None,None))\n",
    "\n",
    "#-----------------------------------Calculate the gradient over parameters with EP-----------------------------------------\n",
    "\n",
    "@jax.jit\n",
    "def single_SquareLattice_paras_gradient(equi_nudge, equi_zero, bias, beta):\n",
    "    #Here equi_nudge and equi_zero are all configuration for a single phase\n",
    "    # Calculate gradient over weights\n",
    "    moved_equi_zero = move_phase(equi_zero)\n",
    "    dp_zero = equi_zero - moved_equi_zero\n",
    "    \n",
    "    moved_equi_nudge = move_phase(equi_nudge)\n",
    "    dp_nudge = equi_nudge - moved_equi_nudge\n",
    "    \n",
    "    gW = -jnp.cos(dp_nudge) + jnp.cos(dp_zero)\n",
    "    \n",
    "    # Calculate the gradient over bias\n",
    "    h = bias[0,...]\n",
    "    psi = bias[1,...]\n",
    "    \n",
    "    g_h = jnp.cos(equi_zero-psi) - jnp.cos(equi_nudge-psi)\n",
    "    g_psi = h*jnp.sin(equi_zero-psi) - h*jnp.sin(equi_nudge-psi)\n",
    "    \n",
    "    gh = jnp.asarray([g_h, g_psi])\n",
    "    \n",
    "    return gW/beta, gh/beta\n",
    "\n",
    "\n",
    "def SquareLattice_EP_param_gradient(W_0, bias_0, phase_0, training_target, training_paras, model_paras):\n",
    "    \n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, lattice_form, input_index, variable_index, output_index = model_paras\n",
    "    \n",
    "    N = bias_0.shape[1]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    batch_size = phase_0.shape[0]\n",
    "    batch_training_target = jnp.tensordot(jnp.ones(batch_size), training_target, 0)\n",
    "    \n",
    "    #equi_zero = v_opt_run_SquareLattice_network(jnp.concatenate(phase_0), W_0, bias_0, jnp.concatenate(batch_training_target), lattice_form, input_index, variable_index, output_index, 0)\n",
    "    #equi_nudge = v_opt_run_SquareLattice_network(equi_zero, W_0, bias_0, jnp.concatenate(batch_training_target), lattice_form, input_index, variable_index, output_index, beta)\n",
    "\n",
    "    equi_zero = v_run_lattice_network(jnp.concatenate(phase_0), T, W_0, bias_0, jnp.concatenate(batch_training_target), lattice_form, input_index, output_index, 0)\n",
    "    equi_nudge = v_run_lattice_network(equi_zero, T, W_0, bias_0, jnp.concatenate(batch_training_target), lattice_form, input_index, output_index, beta)\n",
    "\n",
    "    gWL, ghL = jax.vmap(single_SquareLattice_paras_gradient, in_axes=(0,0,None,None), out_axes=(0,0))(equi_nudge, equi_zero, bias_0, beta)\n",
    "    \n",
    "    gW = jnp.mean(gWL, axis=0)\n",
    "    gh = jnp.mean(ghL, axis=0)\n",
    "    \n",
    "    flatten_equi_zero = jax.vmap(jnp.concatenate)(equi_zero)\n",
    "    \n",
    "    cost_list = cost_function(flatten_equi_zero, jnp.concatenate(batch_training_target), output_index)\n",
    "    cost = jnp.mean(cost_list)\n",
    "    \n",
    "    q_cost_list = qualitative_cost(flatten_equi_zero, jnp.concatenate(batch_training_target), output_index, 0.1)\n",
    "    q_cost = jnp.mean(q_cost_list)\n",
    "    \n",
    "    return gW, gh, cost, q_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbcf2537",
   "metadata": {
    "id": "fbcf2537",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#===================Step by step update===============\n",
    "#--------------------------------initialize the state------------------------------\n",
    "def init_phase(N, input_index, input_data, batch_size):\n",
    "    N_data = input_data.shape[0]\n",
    "    phase_0 = np.random.rand(np.max([batch_size,1]), N_data, N) * (batch_size > 0)\n",
    "    phase_0[:,:,input_index] = input_data\n",
    "    \n",
    "    return jnp.asarray(phase_0)\n",
    "\n",
    "@jax.jit\n",
    "def jax_init_phase(phase_form, input_index, input_data, batch_form, seed, from_zero=False):\n",
    "    N_data, N = phase_form.shape\n",
    "    batch_size = batch_form.shape[0]\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    BS = jnp.max(jnp.asarray([1, batch_size]))\n",
    "    phase_0 = jax.random.uniform(key, shape=(batch_size, N_data, N), dtype=jnp.float64, minval=-jnp.pi, maxval=jnp.pi) * (1 - (from_zero==True))\n",
    "    #phase_0 = np.random.rand(np.max([batch_size,1]), N_data, N) * (batch_size > 0)\n",
    "    phase_0 = phase_0.at[:,:,input_index].set(input_data)\n",
    "    \n",
    "    return jnp.asarray(phase_0)\n",
    "\n",
    "@jax.jit\n",
    "def lattice_jax_init_phase(phase_form, input_index, input_data, batch_form, seed, from_zero=False):\n",
    "    N_data, *dim = phase_form.shape\n",
    "    batch_size = batch_form.shape[0]\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    phase_0 = jax.random.uniform(key, shape=(batch_size, N_data, *dim), dtype=jnp.float64, minval=-jnp.pi, maxval=jnp.pi) * (1 - (from_zero==True))\n",
    "    #phase_0 = np.random.rand(np.max([batch_size,1]), N_data, N) * (batch_size > 0)\n",
    "    phase_0 = phase_0.at[...,0,input_index].set(input_data)\n",
    "    \n",
    "    return jnp.asarray(phase_0)\n",
    "\n",
    "#------------------------------Calculate gradient with EP---------------------------\n",
    "@jax.jit\n",
    "def EP_param_gradient(W_0, bias_0, phase_0, training_target, training_paras, model_paras):\n",
    "    # Initialize the system\n",
    "    #batch_size = batch_form.shape[0]\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, input_index, variable_index, output_index = model_paras\n",
    "    N_data = jnp.shape(training_target)[0]\n",
    "    N = jnp.shape(W_0)[1]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    #initial phase\n",
    "    batch_size = phase_0.shape[0]\n",
    "    batch_training_target = jnp.tensordot(jnp.ones(batch_size), training_target, 0)\n",
    "    \n",
    "    '''\n",
    "    equi_zero = v_opt_run_network(jnp.concatenate(phase_0), W_0, bias_0, jnp.concatenate(batch_training_target), 0, input_index, variable_index, output_index)\n",
    "    if jnp.isnan(jnp.sum(equi_zero)):\n",
    "        equi_zero = v_run_network(jnp.concatenate(phase_0), T, W_0, bias_0, jnp.concatenate(batch_training_target), 0, input_index, output_index)\n",
    "        \n",
    "    equi_nudge = v_opt_run_network(equi_zero, W_0, bias_0, jnp.concatenate(batch_training_target), beta, input_index, variable_index, output_index)\n",
    "    if jnp.isnan(jnp.sum(equi_nudge)):\n",
    "        equi_nudge = v_run_network(equi_zero, T, W_0, bias_0, jnp.concatenate(batch_training_target), beta, input_index, output_index)\n",
    "    '''\n",
    "    equi_zero = v_run_network(jnp.concatenate(phase_0), T, W_0, bias_0, jnp.concatenate(batch_training_target), 0, input_index, output_index)\n",
    "    equi_nudge = v_run_network(equi_zero, T, W_0, bias_0, jnp.concatenate(batch_training_target), beta, input_index, output_index)\n",
    "    \n",
    "    cost_list = cost_function(equi_zero, jnp.concatenate(batch_training_target), output_index)\n",
    "    cost = jnp.mean(cost_list)\n",
    "    \n",
    "    q_cost_list = qualitative_cost(equi_zero, jnp.concatenate(batch_training_target), output_index, 0.1)\n",
    "    q_cost = jnp.mean(q_cost_list)\n",
    "    \n",
    "    gW, gh = paras_gradient(equi_nudge, equi_zero, bias_0, beta)\n",
    "    \n",
    "\n",
    "    # Formulate the output\n",
    "\n",
    "\n",
    "    return gW/beta, gh/beta, cost, q_cost\n",
    "\n",
    "#---------------------------------Calculate parameter gradient with EP with layer structure-------------------------------\n",
    "@jax.jit\n",
    "def weights_deriv_in_layer(phase1, phase2):\n",
    "    gW_loc = -jnp.cos(phase2[:,None,:] - jnp.transpose(phase1[:,None,:], axes=(0,2,1)))\n",
    "    \n",
    "    return jnp.mean(gW_loc, axis=0)\n",
    "\n",
    "@jax.jit\n",
    "def weights_gradient_in_layer(phase, structure_shape):\n",
    "    split_points = jax.tree_map(jnp.size, structure_shape)\n",
    "    phase_list = jnp.split(phase, split_points, axis=1)\n",
    "    \n",
    "    phase1_list = phase_list.copy()\n",
    "    phase2_list = phase_list.copy()\n",
    "    \n",
    "    del phase1_list[-1]\n",
    "    del phase2_list[0]\n",
    "    \n",
    "    return jax.tree_map(weights_deriv_in_layer, phase1_list, phase2_list)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def layer_paras_gradient(equi_nudge, equi_zero, bias, structure_shape, beta):\n",
    "    \n",
    "    #Calculate gradient over bias\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "\n",
    "    h = jnp.tensordot(jnp.ones(jnp.shape(equi_zero)[0]), h, 0)\n",
    "    psi = jnp.tensordot(jnp.ones(jnp.shape(equi_zero)[0]), psi, 0)\n",
    "\n",
    "    g_h = jnp.cos(equi_zero-psi) - jnp.cos(equi_nudge-psi)\n",
    "    g_psi = h*jnp.sin(equi_zero-psi) - h*jnp.sin(equi_nudge-psi)\n",
    "\n",
    "    g_h = jnp.mean(g_h,axis=0)\n",
    "    g_psi = jnp.mean(g_psi,axis=0)\n",
    "\n",
    "    gh = jnp.asarray([g_h, g_psi])\n",
    "    \n",
    "    #Calculate gradient over coupling\n",
    "    dEdW_zero = weights_gradient_in_layer(equi_zero, structure_shape)\n",
    "    dEdW_nudge = weights_gradient_in_layer(equi_nudge, structure_shape)\n",
    "    \n",
    "    gW = jax.tree_map(jnp.subtract, dEdW_nudge, dEdW_zero)\n",
    "    \n",
    "    return jax.tree_map(jnp.divide, gW, list(beta*jnp.ones(len(gW)))), gh/beta\n",
    "        \n",
    "\n",
    "#@jax.jit\n",
    "def layered_EP_param_gradient(WL_0, bias_0, phase_0, training_target, training_paras, model_paras):\n",
    "    # Initialize the system\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, structure_list, split_points, structure_shape = model_paras\n",
    "    \n",
    "    N = bias_0.shape[1]\n",
    "    T = N_ev * dt\n",
    "\n",
    "    input_index = jnp.arange(0, structure_shape[0].shape[0])\n",
    "    output_index = jnp.arange(structure_shape[-1].shape[0], N)\n",
    "    \n",
    "    batch_size = phase_0.shape[0]\n",
    "    batch_training_target = jnp.tensordot(jnp.ones(batch_size), training_target, 0)\n",
    "    \n",
    "    '''\n",
    "    equi_zero = v_opt_run_layered_network(jnp.concatenate(phase_0), WL_0, bias_0, jnp.concatenate(batch_training_target), structure_shape, 0)\n",
    "    if jnp.isnan(jnp.sum(equi_zero)):\n",
    "        equi_zero = v_run_layered_network(jnp.concatenate(phase_0), T, WL_0, bias_0, jnp.concatenate(batch_training_target), structure_shape, 0)\n",
    "    \n",
    "    equi_nudge = v_opt_run_layered_network(equi_zero, WL_0, bias_0, jnp.concatenate(batch_training_target), structure_shape, beta)\n",
    "    if jnp.isnan(jnp.sum(equi_nudge)):\n",
    "        equi_nudge = v_run_layered_network(equi_zero, T, WL_0, bias_0, jnp.concatenate(batch_training_target), structure_shape, beta)\n",
    "\n",
    "    # Calculate the loss\n",
    "    cost_list = cost_function(equi_zero, batch_training_target, output_index)\n",
    "    cost = jnp.mean(cost_list)\n",
    "\n",
    "    q_cost_list = qualitative_cost(equi_zero, batch_training_target, output_index, tol=0.1)\n",
    "    q_cost = jnp.mean(q_cost_list)\n",
    "\n",
    "    '''\n",
    "    zero_run = lambda phase_0: v_run_layered_network(phase_0, T, WL_0, bias_0, training_target, structure_shape, 0)\n",
    "    equi_zero = jax.vmap(zero_run,(0))(phase_0)\n",
    "    \n",
    "    nudge_run = lambda phase_0: v_run_layered_network(phase_0, T, WL_0, bias_0, training_target, structure_shape, beta)\n",
    "    equi_nudge = jax.vmap(nudge_run, (0))(equi_zero)\n",
    "    \n",
    "    cost_list = jax.vmap(cost_function, (0, None, None))(equi_zero, training_target, output_index)\n",
    "    cost = jnp.mean(cost_list)\n",
    "\n",
    "    q_cost_list = jax.vmap(qualitative_cost, (0, None, None, None))(equi_zero, training_target, output_index, 0.1)\n",
    "    q_cost = jnp.mean(q_cost_list)\n",
    "    \n",
    "    equi_zero = jnp.concatenate(equi_zero, axis=0)\n",
    "    equi_nudge = jnp.concatenate(equi_nudge, axis=0)\n",
    "    \n",
    "\n",
    "    gW, gh = layer_paras_gradient(equi_nudge, equi_zero, bias_0, structure_shape, beta)\n",
    "    \n",
    "    return gW, gh, cost, q_cost\n",
    "\n",
    "\n",
    "#---------------------------------Calculate parameter gradient with EP with lattice structure-------------------------------\n",
    "\n",
    "    \n",
    "#================================Optimization Updates========================\n",
    "@jax.jit\n",
    "def gradient_descent_update(paras, g_paras, study_rate_0, itr_time, study_rate_f=0.001, decay=0):\n",
    "    eta = jnp.max(jnp.asarray([study_rate_0 - decay*itr_time, study_rate_f]))\n",
    "    return paras - eta * g_paras\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Adam_update(paras, g_paras, study_rate_0, itr_time, s, r, r1=0.9, r2=0.999, delta=1e-8, study_rate_f=0.001, decay=0):\n",
    "    '''\n",
    "        This implement update stem with adam. s is the momentum, r realizes adaptive stepsize\n",
    "    '''\n",
    "    # Get the study rate, epsilon in Adam\n",
    "    eta = jnp.max(jnp.asarray([study_rate_0 - decay*itr_time, study_rate_f]))\n",
    "    \n",
    "    new_t = itr_time + 1\n",
    "    new_s = r1 * s + (1 - r1) * g_paras\n",
    "    new_r = r2 * r + (1 - r2) * (g_paras * g_paras)\n",
    "    \n",
    "    s_hat = new_s/(1 - r1**new_t)\n",
    "    r_hat = new_r/(1 - r2**new_t)\n",
    "    \n",
    "    new_paras = paras - eta * s_hat / (delta + jnp.sqrt(r_hat))\n",
    "    \n",
    "    return new_paras, new_s, new_r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3fa1e",
   "metadata": {},
   "source": [
    "$Test the network with XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b6f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "\n",
    "training_data = jnp.pi/2 * jnp.asarray([[-1,-1], [-1,1], [1,-1], [1,1]])\n",
    "training_target = jnp.pi/2 * jnp.asarray([[-1], [1], [1], [-1]])\n",
    "\n",
    "N_data = training_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3408082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_US_model(model, N_epoch, batch_size, study_rate, training_data, training_target):\n",
    "    #test all-to-all connected network\n",
    "    from_zero = False\n",
    "    if batch_size == 0:\n",
    "        batch_size = 1\n",
    "        from_zero = True\n",
    "\n",
    "    batch_form = jnp.zeros(batch_size)\n",
    "    phase_form = jnp.zeros([N_data, model.N])\n",
    "\n",
    "    training_paras = beta, study_rate\n",
    "    model_paras = N_ev, dt, model.input_index, model.variable_index, model.output_index\n",
    "\n",
    "    phase_0 = init_phase(model.N, model.input_index, training_data, batch_size)\n",
    "    W_0 = model.weights_0\n",
    "    bias_0 = model.bias_0\n",
    "\n",
    "    WL = np.zeros([N_epoch+1, model.N, model.N])\n",
    "    biasL = np.zeros([N_epoch+1, 2, model.N])\n",
    "    costL = np.zeros(N_epoch)\n",
    "\n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    WL[0,...] = W_0\n",
    "    biasL[0,...] = bias_0\n",
    "\n",
    "    #prepare for Adam\n",
    "    s_W = 0*W\n",
    "    r_W = 0*W\n",
    "    s_h = 0*bias\n",
    "    r_h = 0*bias\n",
    "\n",
    "    for k in range(0, N_epoch):\n",
    "        phase_0 = jax_init_phase(phase_form, model.input_index, training_data, batch_form, k, from_zero)\n",
    "        #phase_0 = init_phase(UA_model.N, UA_model.input_index, training_data, batch_size)\n",
    "        gW, gh, cost, q_cost = EP_param_gradient(W, bias, phase_0, training_target, training_paras, model_paras)\n",
    "        \n",
    "        W = gradient_descent_update(W, gW, study_rate, k)\n",
    "        bias = gradient_descent_update(bias, gh, study_rate, k)\n",
    "        \n",
    "        #W, s_W, r_W = Adam_update(W, gW, study_rate, k, s_W, r_W)\n",
    "        #bias, s_h, r_h = Adam_update(bias, gh, study_rate, k, s_h, r_h)\n",
    "        \n",
    "        costL[k] = cost\n",
    "    \n",
    "    return WL, biasL, costL\n",
    "\n",
    "\n",
    "def train_layered_model(model, N_epoch, batch_size, study_rate, training_data, training_target):\n",
    "    #test layered network with XOR\n",
    "\n",
    "    from_zero = False\n",
    "    if batch_size == 0:\n",
    "        batch_size = 1\n",
    "        from_zero = True\n",
    "\n",
    "    batch_form = jnp.zeros(batch_size)\n",
    "\n",
    "    training_paras = beta, study_rate\n",
    "    model_paras = N_ev, dt, model.structure_list, model.split_points, model.structure_shape\n",
    "\n",
    "    N_data = training_data.shape[0]\n",
    "    phase_form = jnp.zeros([N_data, model.N])\n",
    "    WL_0 = model.WL\n",
    "    bias_0 = model.bias_0\n",
    "\n",
    "    WLL_layer = []\n",
    "    biasL_layer = np.zeros([N_epoch+1, 2, model.N])\n",
    "    costL_layer = np.zeros(N_epoch)\n",
    "\n",
    "    WL = WL_0\n",
    "    bias = bias_0\n",
    "\n",
    "    WLL_layer = [WL_0]\n",
    "    biasL_layer[0,...] = bias_0\n",
    "\n",
    "    #prepare for Adam\n",
    "    WL_shape = jax.tree_map(jnp.shape, WL_0)\n",
    "\n",
    "    s_W = []\n",
    "    for shape in WL_shape:\n",
    "        s_W.append(jnp.zeros(shape))\n",
    "\n",
    "    r_W = s_W.copy()\n",
    "\n",
    "    s_h = jnp.zeros(bias_0.shape)\n",
    "    r_h = s_h\n",
    "    \n",
    "    depth = len(WL)\n",
    "    study_rate_list = [study_rate]\n",
    "    gWL = np.zeros([N_epoch, depth])\n",
    "    ngWL = np.zeros([N_epoch, depth])\n",
    "    \n",
    "    for k in range(1, depth-1):\n",
    "        study_rate_list.append(study_rate_list[-1]*1)\n",
    "        \n",
    "    study_rate_list.append(study_rate_list[-1] * 1)\n",
    "\n",
    "    for k in range(0, N_epoch):\n",
    "        phase_0 = jax_init_phase(phase_form, model.input_index, training_data, batch_form, k, from_zero)\n",
    "        gW, gh, cost, q_cost = layered_EP_param_gradient(WL, bias, phase_0, training_target, training_paras, model_paras)\n",
    "        \n",
    "        abs_gW = jax.tree_util.tree_map(jnp.linalg.norm, gW)\n",
    "        ngW = jax.tree_util.tree_map(jnp.divide, abs_gW, jax.tree_util.tree_map(jnp.linalg.norm, WL))\n",
    "        \n",
    "        ugW = jax.tree_util.tree_map(jnp.divide, gW, jax.tree_util.tree_map(jnp.linalg.norm, WL))\n",
    "        #ugW = jax.tree_util.tree_map(jnp.multiply, ugW, jax.tree_util.tree_map(jnp.sqrt, jax.tree_util.tree_map(jnp.size, WL) ))\n",
    "        \n",
    "        \n",
    "        update_func = lambda paras, g_paras, study_rate_list: gradient_descent_update(paras, g_paras, study_rate_list, k)\n",
    "        WL = jax.tree_util.tree_map(update_func, WL, gW, study_rate_list)\n",
    "        \n",
    "        '''\n",
    "        bias_list = jnp.split(bias, layer_model.split_points, axis=1)\n",
    "        gh_list = jnp.split(gh, layer_model.split_points, axis=1)\n",
    "        bias0 = bias_list[0]\n",
    "        del bias_list[0]\n",
    "        del gh_list[0]\n",
    "        ugh_list = jax.tree_util.tree_map(jnp.divide, gh_list, jax.tree_util.tree_map(jnp.linalg.norm, bias_list))\n",
    "        \n",
    "        bias_list = jax.tree_util.tree_map(update_func, bias_list, gh_list, study_rate_list)\n",
    "        bias_list.insert(0,bias0)\n",
    "        bias = jnp.concatenate(bias_list, axis=1)\n",
    "        '''\n",
    "        bias = gradient_descent_update(bias, gh, study_rate, k)\n",
    "\n",
    "        '''\n",
    "        ugW = jax.tree_util.tree_map(jnp.divide, gW, jax.tree_util.tree_map(jnp.linalg.norm, WL))\n",
    "        bias_list = jnp.split(bias, layer_model.split_points, axis=1)\n",
    "        gh_list = jnp.split(gh, layer_model.split_points, axis=1)\n",
    "        bias0 = bias_list[0]\n",
    "        del bias_list[0]\n",
    "        del gh_list[0]\n",
    "        \n",
    "        update_func = lambda paras, g_paras, s, r: Adam_update(paras, g_paras, study_rate, k, s, r)\n",
    "        bias, s_h, r_h = update_func(bias, gh, s_h, r_h)\n",
    "        res = jax.tree_map(update_func, WL, gW, s_W, r_W)\n",
    "        WL, s_W, r_W = tuple(zip(*res))\n",
    "        WL, s_W, r_W = list(WL), list(s_W), list(r_W)\n",
    "        '''\n",
    "        \n",
    "        WLL_layer.append(WL)\n",
    "        biasL_layer[k+1,...] = bias\n",
    "        costL_layer[k] = cost\n",
    "        gWL[k,:] = jnp.asarray(abs_gW)\n",
    "        ngWL[k,:] = jnp.asarray(ngW)\n",
    "         \n",
    "    return WLL_layer, biasL_layer, costL_layer, gWL, ngWL\n",
    "\n",
    "def train_squarelattice_model(model, N_epoch, batch_size, study_rate, training_data, training_target):\n",
    "    #test Square lattice network with XOR\n",
    "    from_zero = False\n",
    "    if batch_size == 0:\n",
    "        batch_size = 1\n",
    "        from_zero = True\n",
    "\n",
    "    batch_form = jnp.zeros(batch_size)\n",
    "\n",
    "    lattice_form = jnp.zeros(model.dimension)\n",
    "\n",
    "    training_paras = beta, study_rate\n",
    "    model_paras = N_ev, dt, lattice_form, model.input_index, model.variable_index, model.output_index\n",
    "\n",
    "    N_data = training_data.shape[0]\n",
    "    phase_form = jnp.zeros([N_data, *model.dimension])\n",
    "\n",
    "    W_0 = model.weights_0\n",
    "    bias_0 = model.bias_0\n",
    "\n",
    "    WL_lattice = []\n",
    "    biasL_lattice = []\n",
    "    costL_lattice = np.zeros(N_epoch)\n",
    "\n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    WL_lattice = [W_0]\n",
    "    biasL_lattice.append(bias_0)\n",
    "\n",
    "    #prepare for Adam\n",
    "    #prepare for Adam\n",
    "    s_W = 0*W\n",
    "    r_W = 0*W\n",
    "    s_h = 0*bias\n",
    "    r_h = 0*bias\n",
    "\n",
    "    for k in range(0, N_epoch):\n",
    "        phase_0 = lattice_jax_init_phase(phase_form, model.input_index, training_data, batch_form, k**2, from_zero)\n",
    "        gW, gh, cost, q_cost = SquareLattice_EP_param_gradient(W, bias, phase_0, training_target, training_paras, model_paras)\n",
    "        \n",
    "        W = gradient_descent_update(W, gW, study_rate/100, k)\n",
    "        bias = gradient_descent_update(bias, gh, study_rate/100, k)\n",
    "        \n",
    "        #W, s_W, r_W = Adam_update(W, gW, study_rate, k, s_W, r_W)\n",
    "        #bias, s_h, r_h = Adam_update(bias, gh, study_rate, k, s_h, r_h)\n",
    "        \n",
    "        \n",
    "        WL_lattice.append(W)\n",
    "        biasL_lattice.append(bias)\n",
    "        costL_lattice[k] = cost\n",
    "\n",
    "    return WL_lattice, biasL_lattice, costL_lattice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca3bc2",
   "metadata": {},
   "source": [
    "#Start doing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c86c27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the network\n",
    "N_ev = 1000\n",
    "dt = 0.1\n",
    "\n",
    "study_rate = 0.1\n",
    "beta = 0.1\n",
    "\n",
    "#Define the network with square lattice architecture\n",
    "dimension = [3,3]\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "lattice_model = SP_XY_SquareLattice_Network(dimension, N_ev, dt, input_size, output_size)\n",
    "lattice_model.random_state_initiation()\n",
    "\n",
    "#Define the network with all-to-all connection\n",
    "N = 9\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "\n",
    "#Define the network with layer stryctyre\n",
    "structure_list = jnp.asarray([2,5,5,1])\n",
    "N = jnp.sum(structure_list)\n",
    "layer_model = SP_XY_Layer_Network(N, N_ev, dt, structure_list)\n",
    "layer_model.random_state_initiation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4dd688b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  7, 12], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_model.split_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4a71c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experimental parameters\n",
    "N_task = 100\n",
    "N_epoch = 1000\n",
    "study_rate = 0.1\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28310763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 283.81809759140015\n"
     ]
    }
   ],
   "source": [
    "N_task = 1\n",
    "all_cost_layer = np.zeros([N_task, N_epoch])\n",
    "depth = len(layer_model.WL)\n",
    "all_gWL = np.zeros([N_task, N_epoch, depth])\n",
    "all_ngWL = np.zeros([N_task, N_epoch, depth])\n",
    "\n",
    "for k in range(0, N_task):\n",
    "    t0 = time.time()\n",
    "    #layer_model.random_state_initiation()\n",
    "    WLL_layer, biasL_layer, costL_layer, gWL, ngWL = train_layered_model(layer_model, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "    t1 = time.time()\n",
    "    print(k,t1-t0)\n",
    "    all_cost_layer[k,:] = costL_layer\n",
    "    all_gWL[k,...] = gWL\n",
    "    all_ngWL[k,...] = ngWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c03c0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x150748070f40>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,N_epoch), jnp.mean(all_cost_layer, axis=0), linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer structure\n",
    "\n",
    "x = np.concatenate(np.tensordot(np.ones(N_task), np.arange(0, N_epoch),0))\n",
    "y = np.concatenate(all_cost_layer)\n",
    "\n",
    "plt.hist2d(x, np.log10(y), bins=(80,80), vmax=50)\n",
    "plt.plot(np.arange(0,N_epoch), jnp.log10(jnp.mean(all_cost_layer, axis=0)), color='white', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'black']\n",
    "for k in range(0, all_gWL.shape[2]):\n",
    "    plt.plot(np.arange(0,N_epoch), jnp.mean(all_gWL, axis=0)[:,k], color=color_list[k], label='{0}-{1}'.format(k,k+1), alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c96411",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'black']\n",
    "for k in range(0, all_gWL.shape[2]):\n",
    "    plt.plot(np.arange(0,N_epoch), jnp.mean(all_ngWL, axis=0)[:,k], color=color_list[k], label='{0}-{1}'.format(k,k+1), alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4a69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "EqProp2",
   "language": "python",
   "name": "eqprop2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
