{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142493e9",
   "metadata": {
    "id": "142493e9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.integrate\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import diffrax\n",
    "from jax.scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d59cf27-de74-4dbd-846d-ad7dcfd915bd",
   "metadata": {
    "id": "0d59cf27-de74-4dbd-846d-ad7dcfd915bd"
   },
   "outputs": [],
   "source": [
    "import jaxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9b0be8-a08b-4d30-8a1a-2b7d18c8d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.loadtxt('N=5', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b0839d-c9fe-4010-9d28-6e8cad38108a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9P6weRnbrVZ8",
   "metadata": {
    "id": "9P6weRnbrVZ8"
   },
   "source": [
    "define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb2960e-c42a-4f33-b134-0449f9b1bb1e",
   "metadata": {
    "id": "5cb2960e-c42a-4f33-b134-0449f9b1bb1e",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class SP_XY_Network:\n",
    "    \n",
    "    '''\n",
    "    This is aimed to implement a neural network consisting of XY oscillators with bias term.\n",
    "    We only define the network. The datat need to be got from outside.\n",
    "\n",
    "\n",
    "    Terms: \n",
    "        data: original data\n",
    "        phase: a configuration of every cells\n",
    "    \n",
    "    ===========================================\n",
    "    \n",
    "    Variables: \n",
    "    \n",
    "        Parameters: \n",
    "        \n",
    "            (set in __init__())\n",
    "            N: number of neurons (including input and output cells)\n",
    "            N_ev: maximum number of updating when looking for the equilibrium (free and with cost function)\n",
    "            N_epoch: maximum number of iteration when doing gradient descent for the weights\n",
    "            dt: time step for evolution to find equilibrium\n",
    "            tol_eq: tolerance for searcheng for the equilibrium\n",
    "            tol_W: tolerance for optimizing weights\n",
    "            N_input: number of input cells\n",
    "            N_output: number of output cells\n",
    "            \n",
    "            (set in get_training/test_data)\n",
    "            N_sample: number of samples for training\n",
    "            N_test: number of data sets fot validation test\n",
    "            \n",
    "            (set to be 0 in __init__ and updated when getting data)\n",
    "            wieghts_0: initiate value for couplings\n",
    "            phase_0: initiate configuration of all cells\n",
    "            \n",
    "        --------------------------\n",
    "            \n",
    "        External Data:\n",
    "        \n",
    "            (set in __init__())\n",
    "            input_index: indices of input cells\n",
    "            output_index: indices of output cells\n",
    "            \n",
    "            (set to be 0 in __init__(), get from get_xxx_data)\n",
    "            training_data: data to input cells for training\n",
    "            test_data: data to input cells for validation test\n",
    "            \n",
    "            training_target: data to output cells for training\n",
    "            target_output: desired output of output cells, used for validation tests\n",
    "            \n",
    "        ---------------------------\n",
    "        \n",
    "        Internal Variables:\n",
    "            \n",
    "            weights: weights, couplings between cells. N x N\n",
    "            bias: set to be [[h0,h1,h2...h_N],[psi_0,psi_1,psi_2...psi_N]]. \n",
    "            h: strength of local field\n",
    "            psi: direction of the local field \n",
    "            \n",
    "            equi_free: configuration at free equilibrium (all cells). Used for training\n",
    "            equi_nudge: configuration at total equilibrium (all cells). Used for training\n",
    "            test_result_phase: phase figuration for output in validation test (all cells)\n",
    "            test_result_data: data to output cells for validation test (output cells)\n",
    "            \n",
    "            input_data: ...\n",
    "            output_data: ...\n",
    "            \n",
    "            validity_training: ......\n",
    "            validity_test: ......\n",
    "            \n",
    "            -----------------------\n",
    "        \n",
    "        Functions: \n",
    "            \n",
    "            Setting the Network: \n",
    "                \n",
    "                __init__: set the number of cells, determine input and output cells, set all data to be zero\n",
    "                get_training_data(input_data, output_data): input the training data, prepare for the training stage\n",
    "                get_test_data(input_data, target_output): \n",
    "                random_state_initiation(): initiate the phase_0 and weights_0 randomly for the cells other than input and output cells\n",
    "                \n",
    "            -----------------------\n",
    "            \n",
    "            Calculation of Internal Properties:\n",
    "            \n",
    "                internal_energy(self,...): calculate the internal energy: E=1/2 \\sum_{ij} W_{ij} cos(\\phi_i-\\phi_j)\n",
    "                bias_term(self,...): calculate the bias term\n",
    "                internal_energy(self,...): internal_energy+bias_term\n",
    "                \n",
    "                sinlge_cost(self,phase,target_phase): calculate the cost function for single phases\n",
    "                total_energy(self,...): internal_energy+single_cost\n",
    "                cost_function(self, output, target_output): calculate cost function for outputs and target outputs\n",
    "                \n",
    "                internal_force: gradient of free term of internal energy\n",
    "                bias_force: gradient of bias term of internal energy\n",
    "                cost_force: ......\n",
    "                internal_force: internal_force+bias_force\n",
    "                total_force: internal_force+cost_force\n",
    "                \n",
    "                ...force45: used for 4th Runge Kutte method\n",
    "                \n",
    "                weights_gradient(self,...): calculate the gradient for training\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #------------- Setting the system ------------------\n",
    "    \n",
    "    def __init__(self, N, N_ev, dt, input_index, output_index):\n",
    "        \n",
    "        # Setting parameters from the input\n",
    "        self.N=N\n",
    "        self.N_ev=N_ev\n",
    "        self.dt=dt\n",
    "        self.input_index=input_index\n",
    "        self.output_index=output_index\n",
    "        \n",
    "        self.variable_index = np.delete(np.arange(0,N), input_index)\n",
    "        \n",
    "        self.N_input=len(input_index)\n",
    "        self.N_output=len(output_index)\n",
    "        self.T=self.dt*self.N_ev\n",
    "        \n",
    "        # Initalize all the other parameters. Default values are all zero\n",
    "        self.N_sample=0\n",
    "        self.N_test=0\n",
    "        \n",
    "        self.weights=np.zeros([N,N])\n",
    "        self.weights_0=np.zeros([N,N])\n",
    "\n",
    "        self.bias=np.zeros([2,N])\n",
    "        self.bias_0=np.zeros([2,N])\n",
    "        \n",
    "        self.beta=0.001\n",
    "        \n",
    "    #---------------------------Initiation Module: initiate the network---------------------------------\n",
    "        \n",
    "    def get_initial_state(self, weights_0,phase_0,bias_0):\n",
    "        # Set weights_0 and phase_0 manually\n",
    "        \n",
    "        self.weights_0=weights_0\n",
    "        self.weights=weights_0\n",
    "        self.phase_0=phase_0\n",
    "        self.bias=bias_0\n",
    "        self.bias_0=bias_0\n",
    "\n",
    "        \n",
    "    def random_state_initiation(self):\n",
    "        # Randomly set the weights and the phase_0 and bias\n",
    "        \n",
    "        # Set weights, ~N(0,1)\n",
    "        self.weights_0=np.random.randn(self.N, self.N)\n",
    "        for k in range(0,self.N):\n",
    "            self.weights_0[k,k]=0\n",
    "        self.weights_0=(self.weights_0+np.transpose(self.weights_0))/2\n",
    "        \n",
    "        self.weights=self.weights_0\n",
    "        \n",
    "        #Set bias, ~U(-0.5,0.5),U(-pi,pi)\n",
    "        bias=np.random.rand(2,self.N)\n",
    "        bias[0,:]=bias[0,:]-0.5\n",
    "        bias[1,:]=2*np.pi*(bias[1,:]-0.5)\n",
    "        self.bias_0=bias\n",
    "        self.bias=bias\n",
    "\n",
    "        # Set phase_0\n",
    "        self.phase_0=np.pi*np.random.rand(self.N)-np.pi/2*np.ones(self.N)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_beta(self,beta):\n",
    "        \n",
    "        self.beta=beta\n",
    "        \n",
    "    #--------------------------Energy Module: Calculate the energy and cost functions--------------------------\n",
    "    ''' \n",
    "        Functions here are design to simultaneousle deal with a set of phase. \n",
    "        The value will be returned and used for other calculations such as assess the reliability of the system.  \n",
    "    '''\n",
    "    def dphase(self,phase):\n",
    "        # Calculate dphase[i,j]=phase[i]-phase[j]\n",
    "        \n",
    "        aux_ones=np.ones(self.N)\n",
    "        phase_mat=np.tensordot(aux_ones,phase,0)\n",
    "        phase_i=np.transpose(phase_mat,(1,2,0))\n",
    "        phase_j=np.transpose(phase_i,(0,2,1))\n",
    "        dphase=phase_i-phase_j\n",
    "        \n",
    "        return dphase\n",
    "\n",
    "    def internal_energy(self,W,phase):\n",
    "        # Calculate free term of internal energy for a set of phase\n",
    "        # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "        dphase=self.dphase(phase)\n",
    "        E_list=0.5*np.sum(W*np.cos(dphase),(1,2))\n",
    "        \n",
    "        return E_list\n",
    "    \n",
    "    def bias_term(self,bias,phase):\n",
    "        # Calculate bias term of internal energy for a set of phase\n",
    "        # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input. \n",
    "        \n",
    "        h=bias[0,:]\n",
    "        psi=bias[1,:]\n",
    "        N_data=np.shape(phase)[0]\n",
    "\n",
    "        aux_ones=np.ones(N_data)\n",
    "        psi_mat=np.tensordot(aux_ones,psi,0)\n",
    "        E_list=np.sum(h*np.cos(phase-psi_mat),axis=1)\n",
    "        return E_list\n",
    "    \n",
    "    def cost_function(self,phase,target):\n",
    "        # Calculate the cost function for each sample\n",
    "        # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "        doutput=phase[:,self.output_index]-target\n",
    "        cost_mat=np.ones(np.shape(doutput))-np.cos(1*doutput)\n",
    "        cost_list=np.sum(cost_mat,1)\n",
    "        \n",
    "        return cost_list\n",
    "\n",
    "    def total_energy(self, W, bias, phase, target):\n",
    "\n",
    "        return self.internal_energy(W,phase)+self.bias_term(bias,phase)+self.cost_function(phase,target)\n",
    "        \n",
    "    #----------------------Force Module: Calculate the force induced by the energy for evolution----------------------------\n",
    "    \n",
    "    def internal_force(self,W,phase):\n",
    "        # Calculate the force induced by the free term of energy\n",
    "        # Here is a sign problem\n",
    "        \n",
    "        dphase=self.dphase(phase)\n",
    "        F_list=np.sum(W*np.sin(dphase),2)\n",
    "        return F_list\n",
    "    \n",
    "    def bias_force(self,bias,phase):\n",
    "        # Calculate the force induced by bias term \n",
    "        \n",
    "        h=bias[0,:]\n",
    "        psi=bias[1,:]\n",
    "        N_data=np.shape(phase)[0]\n",
    "        psi=np.tensordot(np.ones(N_data),psi,0)\n",
    "        F_list=-h*np.sin(phase-psi)\n",
    "        return F_list\n",
    "    \n",
    "    def cost_force(self, phase, target):\n",
    "        \n",
    "        #print(phase,target)\n",
    "        F=np.zeros(np.shape(phase))\n",
    "        F[:,self.output_index]=-np.sin(1*(phase[:,self.output_index]-target))\n",
    "        return F\n",
    "\n",
    "    def reg_force(self, phase, target):\n",
    "        \n",
    "        #print(phase,target)\n",
    "        F=np.zeros(np.shape(phase))\n",
    "        F[:,self.output_index]=-np.sin(0.5*(phase[:,self.output_index]-target))\n",
    "        return F\n",
    "    \n",
    "    def total_force(self, t, con_phase, W, bias, target, beta):\n",
    "        '''\n",
    "        Give the total force under different requirement: \n",
    "        bias_flag==0: no bias.\n",
    "        bias_flag==1: with bias.\n",
    "        '''\n",
    "        \n",
    "        Nh=int(len(con_phase)/self.N)\n",
    "        phase=np.reshape(con_phase,(Nh,self.N))\n",
    "\n",
    "        F0=self.internal_force(W,phase)\n",
    "        F1=self.bias_force(bias,phase)\n",
    "\n",
    "        F2 = beta*self.cost_force(phase,target)\n",
    "\n",
    "        F3 = beta*beta*self.reg_force(phase,target)\n",
    "        F = -F0+F1+F2+F3\n",
    "        F[:,self.input_index]=0\n",
    "        \n",
    "        return np.concatenate(F)\n",
    "        \n",
    "    \n",
    "    #-----------------------Evolution Module: Do evolutions and find the equilibrium ---------------------\n",
    "        \n",
    "\n",
    "    def run_network(self, W, bias, phase_0, target, beta, T):\n",
    "        \n",
    "        # Use scipy.integrate.solve_ivp\n",
    "        N_data = np.shape(phase_0)[0]\n",
    "        con_phase=np.concatenate(phase_0)\n",
    "        t_span=[0,T]\n",
    "        temp_model=sp.integrate.solve_ivp(self.total_force,t_span,con_phase,method='RK45',args=[W,bias,target,beta])\n",
    "        \n",
    "        L=len(temp_model.t)\n",
    "\n",
    "        phase = np.zeros([L, N_data, self.N])\n",
    "\n",
    "        for k in range(0,L): \n",
    "            phase[k,:,:] = np.reshape(temp_model.y[:,k], [N_data, self.N])\n",
    "        \n",
    "        #return temp_model.t, phase\n",
    "\n",
    "        return phase[L-1]\n",
    "\n",
    "\n",
    "    #------------------------Inverse Evolution-----------------------------\n",
    "\n",
    "    def inverse_force(self, t, con_phase, W, bias, target, beta):\n",
    "        \n",
    "        return -self.total_force(t, con_phase, W, bias, target, beta)\n",
    "\n",
    "    def inverse_evolution(self, W, bias, phase_0, phase_drift, target, T):\n",
    "\n",
    "        # Here phase_0 should be a stable equilibrium\n",
    "        # phase drift is some random perturbatio of the equilibrium so that the evolution could start. \n",
    "\n",
    "        N_data = np.shape(phase_0)[0]\n",
    "        phase_0 = phase_0 + phase_drift\n",
    "        con_phase=np.concatenate(phase_0)\n",
    "        t_span=[0,T]\n",
    "        temp_model=sp.integrate.solve_ivp(self.inverse_force,t_span,con_phase,method='RK45',args=[W,bias,target,0])\n",
    "        \n",
    "        L=len(temp_model.t)\n",
    "\n",
    "        phase = np.zeros([L, N_data, self.N])\n",
    "\n",
    "        for k in range(0,L): \n",
    "            phase[k,:,:] = np.reshape(temp_model.y[:,k], [N_data, self.N])\n",
    "        \n",
    "        return temp_model.t, phase\n",
    "\n",
    "\n",
    "    #=========================Calculate Exact Gradient==========================\n",
    "\n",
    "\n",
    "    def prod_phase(self,phase):\n",
    "        \n",
    "        N_data=np.shape(phase)[0]\n",
    "        \n",
    "        # Calculate the kroneck product prod_phase_ij = phase_i * phase_j\n",
    "        prod_phase=np.tensordot(phase,phase,0)\n",
    "        prod_phase=np.diagonal(prod_phase,axis1=0,axis2=2)\n",
    "        prod_phase=np.transpose(prod_phase,[2,0,1])\n",
    "        \n",
    "        return prod_phase\n",
    "    \n",
    "    def merge_weights(self,M_IS,M_SI,M_SS):\n",
    "        dim=len(np.shape(M_IS))\n",
    "        shape=np.shape(M_IS)[0:dim-2]\n",
    "\n",
    "        N_I=np.shape(M_IS)[dim-2]\n",
    "        N_S=np.shape(M_IS)[dim-1]\n",
    "        \n",
    "        M_II=np.zeros(np.concatenate((shape,[N_I,N_I]),axis=0))\n",
    "        M_up=np.concatenate((M_II,M_IS),axis=dim-1)\n",
    "        M_down=np.concatenate((M_SI,M_SS),axis=dim-1)\n",
    "        M=np.concatenate((M_up,M_down),axis=dim-2)\n",
    "        \n",
    "        return M\n",
    "        \n",
    "    \n",
    "    def destruct_weights(self,W):\n",
    "        \n",
    "        input_index=self.input_index\n",
    "        free_index=list(set(range(0,self.N))-set(input_index))\n",
    "        M_II=(W[...,input_index,:])[...,input_index]\n",
    "        M_IS=(W[...,input_index,:])[...,free_index]\n",
    "        M_SI=(W[...,free_index,:])[...,input_index]\n",
    "        M_SS=(W[...,free_index,:])[...,free_index]\n",
    "        \n",
    "        return M_II,M_IS,M_SI,M_SS\n",
    "        \n",
    "    def data_prod(self, A, B):\n",
    "        # This is to calculate M_nij=A_ni * B_nj. Here i and j can be either single or multiple index. \n",
    "        # We first do tensor product P_nimj=A_ni * B_mj. Then take the diagonal over index m and n. Then do the transpose. \n",
    "\n",
    "        dim_A=len(np.shape(A))\n",
    "        dim_B=len(np.shape(B))\n",
    "        M=np.tensordot(A,B,0)\n",
    "        T_prod=np.diagonal(M,axis1=0,axis2=dim_A)\n",
    "        prod=np.transpose(T_prod,[dim_A+dim_B-2]+list(range(0,dim_A+dim_B-2)))\n",
    "\n",
    "        return prod\n",
    "\n",
    "\n",
    "    def E_2nd_derivatives(self,W,bias,phase):\n",
    "        \n",
    "        # This calculate the dependence tensor to calculate the exact gradient. \n",
    "        \n",
    "        N_data=np.shape(phase)[0]\n",
    "        h=np.tensordot(np.ones(N_data),bias[0,:],0)\n",
    "        psi=np.tensordot(np.ones(N_data),bias[1,:],0)\n",
    "\n",
    "        prod_phase=self.prod_phase(phase)\n",
    "        diff_phase=self.dphase(phase)\n",
    "        Id=np.eye(self.N)\n",
    "        \n",
    "        # Calculate Hessian martix H and its inverse A=pinv(H). \n",
    "        # H_nij= (d^2E/(dx_i dx_j))_n = \\sum_k W_ik cos(x_ni - x_nk) + h_ni cos(x_ni-psi_ni) \\delta_ij - W_ij*cos(x_ni-x_nj)\n",
    "        M=W*np.cos(diff_phase)\n",
    "        B=h*np.cos(phase-psi)\n",
    "        G=np.sum(M,axis=2)+B\n",
    "        \n",
    "        H_diagonal=np.tensordot(G,Id,0)\n",
    "        H_diagonal=np.diagonal(H_diagonal,axis1=1,axis2=2)\n",
    "        H_diagonal=np.transpose(H_diagonal,[0,2,1])\n",
    "        \n",
    "        Hess=H_diagonal-M\n",
    "        #print(\"shape of A is: \", np.shape(A))\n",
    "\n",
    "        # Calculate half of matrix dEdW_nikl = d^2 E/(dx_i dW_kl)_n = (RW_nikl + RW_nilk)/2\n",
    "        RW=np.tensordot(np.sin(diff_phase),Id,0)\n",
    "        RW=np.diagonal(RW,axis1=2,axis2=4)\n",
    "        RW=np.transpose(RW,[0,2,1,3])/2\n",
    "        # Reconsturct the blocks to guarantee the boundary condition\n",
    "        '''\n",
    "        dExW_II, dExW_IS, dExW_SI, dExW_SS = self.destruct_weights(RW)\n",
    "        print(dExW_II, dExW_IS, dExW_SI, dExW_SS)\n",
    "        dExW_II = 0*dExW_II\n",
    "        '''\n",
    "        dExW = (RW + np.transpose(RW, [0,1,3,2]))/2\n",
    "        #dExW = self.merge_weights(dExW_IS, dExW_SI, dExW_SS)\n",
    "        #print(\"shape of RW is: \", np.shape(RW))\n",
    "        #print(\"RW= \\n\", RW)\n",
    "        #print(\"dExW= \\n\", dExW)\n",
    "        \n",
    "\n",
    "        # Calculate dExh_ik = d^2E/(dh_k dx_i) = sin(phi_k-psi_k) * delta_ik\n",
    "        Rh=-np.tensordot(np.sin(phase-psi),Id,0)\n",
    "        dExh=np.diagonal(Rh,axis1=1,axis2=3)\n",
    "        #dExh[:,:,self.input_index]=0\n",
    "        #dExh[:,self.input_index,:]=0\n",
    "        #print(\"shape of Rh is: \", np.shape(Rh))\n",
    "        \n",
    "        \n",
    "        # Calculate dExp_ik = d^2E/(dpsi_k dx_i) = -h_k * cos(phi_k-psi_k) * delta_ik\n",
    "        RP = -h*np.cos(phase-psi)\n",
    "        RP = np.tensordot(RP, Id, 0)\n",
    "        RP = np.diagonal(RP, axis1=1,axis2=2)\n",
    "        dExP = np.transpose(RP, [0,2,1])\n",
    "        #dExP[:,:,self.input_index]=0\n",
    "        #dExP[:,self.input_index,:]=0\n",
    "\n",
    "        return Hess, dExW, dExh, dExP\n",
    "\n",
    "\n",
    "    def x_dep(self,W,bias,phase):\n",
    "        # Calculate x dependence over internal parameters dx/dW, dx/dh, dx/dpsi\n",
    "        # dx_i/d W_k = sum_j inv(Hess)_ij dExW_k\n",
    "\n",
    "        Hess, dExW, dExh, dExP = self.E_2nd_derivatives(W,bias,phase) \n",
    "        # Calculate inverse Hessian: A=H^-1\n",
    "\n",
    "        A=np.linalg.pinv(Hess)\n",
    "        print(\"A=\",A)\n",
    "        A_II, A_IS, A_SI, A_SS = self.destruct_weights(A)\n",
    "        A=self.merge_weights(A_IS*0, A_SI*0, A_SS)\n",
    "        print(\"A=\",A)\n",
    "        dxdW=self.data_prod(A,dExW)\n",
    "        dxdW=np.diagonal(dxdW,axis1=2, axis2=3)\n",
    "        dxdW=np.sum(dxdW,axis=4)\n",
    "\n",
    "\n",
    "        dxdh=self.data_prod(A,dExh)\n",
    "        dxdh=np.diagonal(dxdh,axis1=2, axis2=3)\n",
    "        dxdh=np.sum(dxdh,axis=3)\n",
    "\n",
    "        dxdP=self.data_prod(A,dExP)\n",
    "        dxdP=np.diagonal(dxdP,axis1=2, axis2=3)\n",
    "        dxdP=np.sum(dxdP,axis=3)\n",
    "        \n",
    "        return dxdW, dxdh, dxdP\n",
    "    \n",
    "    def exact_gradient(self,W,h,phase,target):\n",
    "        \n",
    "        # This calculate the exact gradient of loss function over weights and bias with linear self-consistent function. \n",
    "        \n",
    "        # Calculate the naive gradient\n",
    "        NG=np.zeros(np.shape(phase))\n",
    "        NG[:,self.output_index] = -np.sin(phase[:,self.output_index]-target)\n",
    "        \n",
    "        dxdW, dxdh, dxdP = self.x_dep(W,h,phase)\n",
    "        \n",
    "        #print(\"shape of NG is: \", np.shape(NG))\n",
    "        \n",
    "        dLW=np.tensordot(NG,dxdW,(1,1))\n",
    "        dLW=np.diagonal(dLW,axis1=0,axis2=1)\n",
    "        dLW=np.transpose(dLW,[2,0,1])\n",
    "        \n",
    "        dLh=np.tensordot(NG,dxdh,(1,1))\n",
    "        dLh=np.diagonal(dLh,axis1=0,axis2=1)\n",
    "        dLh=np.transpose(dLh,[1,0])\n",
    "\n",
    "        dLP=np.tensordot(NG,dxdP,(1,1))\n",
    "        dLP=np.diagonal(dLP,axis1=0,axis2=1)\n",
    "        dLP=np.transpose(dLP,[1,0])\n",
    "        \n",
    "        return dLW,dLh,dLP\n",
    "    #----------------------Calculate the gradient of weights and bias throught EP-------------------------\n",
    "\n",
    "    def half_search(self, W, bias, internal_gradient, bias_gradient, study_rate):\n",
    "        \n",
    "        #print(study_rate)\n",
    "        \n",
    "        if study_rate<0.001:\n",
    "            W=W-study_rate*internal_gradient\n",
    "            bias=bias-study_rate*bias_gradient\n",
    "            return W, bias\n",
    "        else:\n",
    "            E0=np.sum(self.cost_function(self.equi_free,self.training_target))/self.N_sample\n",
    "\n",
    "            equi_temp1=self.find_free_equilibrium(W-study_rate/2*internal_gradient, bias-study_rate/2*bias_gradient, self.bias_flag, self.equi_free,self.dt)\n",
    "            E1=np.sum(self.cost_function(equi_temp1,self.training_target))/self.N_sample\n",
    "\n",
    "            equi_temp2=self.find_free_equilibrium(W-study_rate/2*internal_gradient, bias-study_rate/2*bias_gradient, self.bias_flag, equi_temp1,self.dt)\n",
    "            E2=np.sum(self.cost_function(equi_temp2,self.training_target))/self.N_sample\n",
    "\n",
    "            E_min=np.min([E0,E1,E2])\n",
    "            if E_min==E2: \n",
    "                W=W-study_rate*internal_gradient\n",
    "                bias=bias-study_rate*bias_gradient\n",
    "                return W, bias\n",
    "            elif E_min==E1:\n",
    "                W=W-study_rate/2*internal_gradient\n",
    "                bias=bias-study_rate/2*bias_gradient\n",
    "                return W, bias\n",
    "            else:\n",
    "                return self.half_search(W, bias, internal_gradient, bias_gradient, study_rate/2)\n",
    "        \n",
    "    \n",
    "    def train_with_exact_gradient(self):\n",
    "        \n",
    "        print(\"start training with exact gradient\")\n",
    "        \n",
    "        W=self.weights_0\n",
    "        bias=self.bias\n",
    "        self.equi_nudge=self.equi_free\n",
    "        target=self.training_target\n",
    "        N_data=np.shape(target)[0]\n",
    "        \n",
    "        old_cost=10*self.N*self.N\n",
    "        temp_cost=10\n",
    "        nr=0\n",
    "        self.validity_training=np.zeros(self.N_epoch)\n",
    "        \n",
    "        #print(W,bias,nr,np.abs((old_cost-temp_cost)/temp_cost))\n",
    "        \n",
    "        # Use this for one_d search\n",
    "            \n",
    "        while np.abs((old_cost-temp_cost)/temp_cost)>self.tol_W:\n",
    "            if nr==self.N_epoch: \n",
    "                break\n",
    "            #self.equi_free=self.find_free_equilibrium(W,bias,self.bias_flag,self.equi_nudge,self.dt)\n",
    "            \n",
    "            self.equi_free=self.find_nudge_equilibrium(W, bias, self.bias_flag, self.equi_nudge, self.training_target, 0, self.dt)\n",
    "            \n",
    "            gW,gh=self.exact_gradient(W,bias,self.equi_free,target)\n",
    "            internal_gradient=np.sum(gW,axis=0)/N_data\n",
    "            bias_gradient=np.sum(gh,axis=0)/N_data\n",
    "            \n",
    "            if self.bias_flag==0:\n",
    "                bias_gradient=np.zeros(np.shape(self.bias))\n",
    "                \n",
    "            W=W-self.study_rate*internal_gradient\n",
    "            bias=bias-self.study_rate*bias_gradient\n",
    "            \n",
    "            old_cost=temp_cost\n",
    "            temp_cost=self.cost_function(self.equi_free,self.training_target)\n",
    "            temp_cost=np.sqrt(np.sum(temp_cost)/self.N_sample/self.N_output)\n",
    "            self.validity_training[nr]=temp_cost\n",
    "            print(temp_cost,np.sum(np.abs(internal_gradient)),nr)\n",
    "            #print(nr)\n",
    "            \n",
    "            \n",
    "            print(\"EP gradient=\", internal_gradient,bias_gradient)\n",
    "            #print(\"Exact gradient=\", np.sum(gW,axis=0)/4,np.sum(gh,axis=0)/4)\n",
    "            nr=nr+1\n",
    "                \n",
    "        self.weights=W\n",
    "        self.bias=bias\n",
    "        \n",
    "        \n",
    "        \n",
    "    #-------------------------Test Module: test the ytrained network--------------------\n",
    "    \n",
    "    def initiate_test(self):\n",
    "        \n",
    "        '''\n",
    "        aux_ones=np.ones(self.N_test)\n",
    "        self.equi_test=np.tensordot(aux_ones, self.phase_0,0)\n",
    "        \n",
    "        '''\n",
    "        # For total irrelevant random initiate, use the code below:\n",
    "        \n",
    "        self.equi_test=2*np.pi*np.random.rand-np.pi*np.ones(np.shape(self.N_test,self.N))\n",
    "        \n",
    "        \n",
    "        self.equi_test[:,self.input_index]=self.test_data\n",
    "        \n",
    "    def validation_test(self):\n",
    "        \n",
    "        # Do the validity test. Run initiate_test() first.\n",
    "        # The validity is defined as \\sqrt((\\sum_test cost)/N)\n",
    "        \n",
    "        phase=self.find_free_equilibrium(self.weights,self.bias,self.bias_flag,self.equi_test,self.dt)\n",
    "        validity_list=self.cost_function(phase,self.target_output)\n",
    "        validity=np.sqrt(np.sum(validity_list)/self.N_test)\n",
    "        \n",
    "        self.validity_test=validity\n",
    "        self.equi_test=phase\n",
    "        \n",
    "    #------------------------Output Module: give the result for input------------------\n",
    "    \n",
    "    \n",
    "\n",
    "class SP_XY_masked_Layer_Network(SP_XY_Network):\n",
    "    def __init__(self, N, N_ev, dt, structure_list):\n",
    "        \n",
    "        # Setting parameters from the input\n",
    "        self.N=N\n",
    "        self.N_ev=N_ev\n",
    "        self.dt=dt\n",
    "        self.structure_list = structure_list\n",
    "        self.T=self.dt*self.N_ev\n",
    "        \n",
    "        # Initalize all the other parameters. Default values are all zero\n",
    "        self.N_sample=0\n",
    "        self.N_test=0\n",
    "        \n",
    "        self.weights=np.zeros([N,N])\n",
    "        self.weights_0=np.zeros([N,N])\n",
    "\n",
    "        self.bias=np.zeros([2,N])\n",
    "        self.bias_0=np.zeros([2,N])\n",
    "        \n",
    "        self.beta=0\n",
    "\n",
    "        #depth\n",
    "        self.L = len(structure_list)\n",
    "        self.mask = np.zeros([N,N])\n",
    "\n",
    "        for k in range(0, self.L-1):\n",
    "            self.mask[np.sum(structure_list[0:k]):np.sum(structure_list[0:k+1]), np.sum(structure_list[0:k+1]):np.sum(structure_list[0:k+2])] = 1\n",
    "\n",
    "        self.mask = self.mask + np.transpose(self.mask)\n",
    "        \n",
    "        self.split_points = np.zeros(len(structure_list)-1)\n",
    "        for k in range(0,len(structure_list)-1):\n",
    "            self.split_points[k] = np.sum(structure_list[0:k+1])\n",
    "            \n",
    "        self.split_points = np.asarray(self.split_points, dtype=np.int32)\n",
    "        \n",
    "    def random_state_initiation(self):\n",
    "        # Randomly set the weights and the phase_0 and bias\n",
    "        \n",
    "        # Set weights, ~N(0,1)\n",
    "        self.weights_0 = np.random.randn(self.N, self.N)\n",
    "        for k in range(0,self.N):\n",
    "            self.weights_0[k,k] = 0\n",
    "        self.weights_0 = self.mask * (self.weights_0+np.transpose(self.weights_0))/2\n",
    "        \n",
    "        self.weights = self.weights_0\n",
    "        \n",
    "        #Set bias, ~U(-0.5,0.5),U(-pi,pi)\n",
    "        bias = np.random.rand(2,self.N)\n",
    "        bias[0,:] = bias[0,:]-0.5\n",
    "        bias[1,:] = 2*np.pi*(bias[1,:]-0.5)\n",
    "        self.bias_0 = bias\n",
    "        self.bias = bias\n",
    "\n",
    "        # Set phase_0\n",
    "        self.phase_0 = np.pi*np.random.rand(self.N)-np.pi/2*np.ones(self.N)\n",
    "\n",
    "\n",
    "class SP_XY_Layer_Network(SP_XY_Network):\n",
    "    def __init__(self, N, N_ev, dt, structure_list):\n",
    "        \n",
    "        # Setting parameters from the input\n",
    "        self.N = N\n",
    "        self.N_ev = N_ev\n",
    "        self.dt = dt\n",
    "        self.structure_list = structure_list\n",
    "        self.T = self.dt * self.N_ev\n",
    "        \n",
    "        # Initalize all the other parameters. Default values are all zero\n",
    "        self.N_sample = 0\n",
    "        self.N_test = 0\n",
    "        \n",
    "        self.weights = np.zeros([N,N])\n",
    "        self.weights_0 = np.zeros([N,N])\n",
    "\n",
    "        self.bias = np.zeros([2,N])\n",
    "        self.bias_0 = np.zeros([2,N])\n",
    "        \n",
    "        self.beta = 0\n",
    "\n",
    "        #depth\n",
    "        self.L = structure_list.shape[0]\n",
    "        self.mask = np.zeros([N,N])\n",
    "        \n",
    "        self.split_points = np.zeros(self.L-1)\n",
    "        for k in range(0,self.L-1):\n",
    "            self.split_points[k] = np.sum(structure_list[0:k+1])\n",
    "            \n",
    "        self.split_points = np.asarray(self.split_points, dtype=np.int32)\n",
    "        \n",
    "        self.input_index = np.arange(0, self.split_points[0])\n",
    "        self.variable_index = np.arange(self.split_points[0], self.N)\n",
    "        self.output_index = np.arange(self.split_points[-1], self.N)\n",
    "        \n",
    "        self.WL = []\n",
    "        \n",
    "        self.structure_shape = []\n",
    "        for k in range(0,len(self.split_points)):\n",
    "            self.structure_shape.append(np.zeros(self.split_points[k]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def random_state_initiation(self):\n",
    "        # Randomly set the weights and the phase_0 and bias\n",
    "        \n",
    "        # Set weights, ~N(0,1)\n",
    "        self.WL = []\n",
    "        init_strength = 1\n",
    "        for k in range(0, len(self.structure_list)-1):\n",
    "            is_mid = 1 - (k==0) - (k==len(self.structure_list)-2)\n",
    "            init_W = init_strength * is_mid * np.eye(self.structure_list[k], self.structure_list[k+1])\n",
    "            #self.WL.append( init_W + 0.1*(1-is_mid)/np.sqrt(self.structure_list[k]+self.structure_list[k+1]) * np.random.randn(self.structure_list[k], self.structure_list[k+1]))\n",
    "            self.WL.append( 0.1/np.sqrt(self.structure_list[k]+self.structure_list[k+1]) * np.random.randn(self.structure_list[k], self.structure_list[k+1]))\n",
    "            \n",
    "        \n",
    "        #Set bias, ~U(-0.5,0.5),U(-pi,pi)\n",
    "        bias = np.asanyarray([0*np.random.randn(self.N), np.random.rand(self.N)])\n",
    "        #bias[0,:] = bias[0,:]-0.5\n",
    "        bias[1,:] = np.pi*(bias[1,:]-0.5)\n",
    "        self.bias_0 = bias\n",
    "        self.bias = bias\n",
    "\n",
    "        # Set phase_0\n",
    "        self.phase_0 = np.pi * np.random.rand(self.N)-np.pi/2*np.ones(self.N)\n",
    "        \n",
    "        self.weights_0 = merge(self.WL, self.split_points, self.phase_0)\n",
    "        self.weights = self.weights_0\n",
    "        \n",
    "\n",
    "class SP_XY_SquareLattice_Network(SP_XY_Network):\n",
    "    def __init__(self, dimension, N_ev, dt, input_size, output_size):\n",
    "        self.architecture = 'Square Lattice'\n",
    "        \n",
    "        self.dimension = dimension\n",
    "        self.N = dimension[0]*dimension[1]\n",
    "        self.N_ev = N_ev\n",
    "        self.dt = dt\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.weights_0 = np.zeros([4] + dimension)\n",
    "\n",
    "        self.bias_0 = np.zeros([2] + dimension)\n",
    "        \n",
    "        self.input_index = np.arange(0, input_size)\n",
    "        self.variable_index = np.arange(input_size, self.N)\n",
    "        self.output_index = np.arange(self.N-output_size, self.N)\n",
    "        \n",
    "    def random_state_initiation(self):\n",
    "        weights_shape = self.weights_0.shape[1], self.weights_0.shape[2]\n",
    "        \n",
    "        W0 = np.random.randn(weights_shape[0]-1, weights_shape[1])\n",
    "        W1 = np.random.randn(weights_shape[0], weights_shape[1]-1)\n",
    "        \n",
    "        zero_row = np.zeros([1,weights_shape[1]])\n",
    "        zero_col = np.zeros([weights_shape[0],1])\n",
    "        \n",
    "        W_u = np.concatenate((W0, zero_row), axis=0)\n",
    "        W_d = np.concatenate((zero_row, W0), axis=0)\n",
    "        \n",
    "        W_l = np.concatenate((W1, zero_col), axis=1)\n",
    "        W_r = np.concatenate((zero_col, W1), axis=1)\n",
    "        \n",
    "        self.weights_0 = np.asarray([W_u, W_d, W_l, W_r])\n",
    "        self.bias_0[0,...] = np.random.randn(*self.dimension)\n",
    "        self.bias_0[1,...] = 2*np.pi*(np.random.rand(*self.dimension) - 0.5)\n",
    "        \n",
    "        \n",
    "\n",
    "#==============================================Matrix workshop=====================================================\n",
    "def merge(WL, split_points, phase):\n",
    "    N = phase.shape[0]\n",
    "    M = np.zeros([N,N])\n",
    "    \n",
    "    L = split_points.shape[0]\n",
    "    merge_points = jnp.zeros(L+2, dtype=jnp.int32)\n",
    "    merge_points = merge_points.at[1:L+1].set(split_points)\n",
    "    merge_points = merge_points.at[L+1].set(N)\n",
    "    merge_points = merge_points.at[0].set(0)\n",
    "    '''\n",
    "    merge_points = split_points.copy()\n",
    "    merge_points.append(N)\n",
    "    merge_points.insert(0, 0)\n",
    "    '''\n",
    "    \n",
    "    for k in range(0,L):\n",
    "        M[merge_points[k]:merge_points[k+1], merge_points[k+1]:merge_points[k+2]] = WL[k]\n",
    "    \n",
    "    M = M + np.transpose(M, [1,0])\n",
    "    \n",
    "    return M\n",
    "\n",
    "def split_M(W, split_points):\n",
    "    WL = []\n",
    "    N = W.shape[0]\n",
    "    \n",
    "    L = split_points.shape[0]\n",
    "    merge_points = jnp.zeros(L+2, dtype=jnp.int32)\n",
    "    merge_points = merge_points.at[1:L+1].set(split_points)\n",
    "    merge_points = merge_points.at[L+1].set(N)\n",
    "    merge_points = merge_points.at[0].set(0)\n",
    "    \n",
    "    '''\n",
    "    merge_points = split_points.copy()\n",
    "    merge_points.append(N)\n",
    "    merge_points.insert(0, 0)\n",
    "    '''\n",
    "    \n",
    "    for k in range(0,L):\n",
    "        WL.append(W[merge_points[k]:merge_points[k+1], merge_points[k+1]:merge_points[k+2]])\n",
    "    \n",
    "    return WL\n",
    "\n",
    "\n",
    "def flatten_weights(dimension, WL):\n",
    "    W_u, W_d, W_l, W_r = WL[0], WL[1], WL[2], WL[3]\n",
    "    n, m = dimension[0], dimension[1]\n",
    "    W_mat = np.zeros([n,m,n,m])\n",
    "    ind_n = np.arange(0,n)\n",
    "    ind_m = np.arange(0,m)\n",
    "    \n",
    "    for k in range(0, n-1):\n",
    "        for l in range(0, m):\n",
    "            W_mat[k,l,k+1,l] = W_u[k,l]\n",
    "            W_mat[k+1,l,k,l] = W_d[k+1,l]\n",
    "    \n",
    "    for k in range(0, n):\n",
    "        for l in range(0, m-1):\n",
    "            W_mat[k,l,k,l+1] = W_l[k,l]\n",
    "            W_mat[k,l+1,k,l] = W_r[k,l+1]\n",
    "    \n",
    "    W_mat = jnp.concatenate(W_mat)\n",
    "    W_mat = jnp.transpose(W_mat, [1,2,0])\n",
    "    W_mat = jnp.concatenate(W_mat)\n",
    "    W_mat = jnp.transpose(W_mat, [1,0])\n",
    "    \n",
    "    return W_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015da48e-2255-40d8-bb59-1cf732c3701b",
   "metadata": {
    "id": "015da48e-2255-40d8-bb59-1cf732c3701b",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions for training the system with jax\n",
    "#---------------------------Energy Terms-------------------------\n",
    "from functools import partial\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "@jax.jit\n",
    "def internal_energy(W,phase):\n",
    "    # Calculate free term of internal energy for a set of phase\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "    dphase=cal_dphase(phase)\n",
    "    E_list= -0.5 * jnp.sum(W * jnp.cos(dphase), (1,2))\n",
    "        \n",
    "    return E_list\n",
    "\n",
    "@jax.jit   \n",
    "def bias_term(bias,phase):\n",
    "    # Calculate bias term of internal energy for a set of phase\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input. \n",
    "        \n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "    N_data = jnp.shape(phase)[0]\n",
    "\n",
    "    aux_ones = jnp.ones(N_data)\n",
    "    psi_mat = jnp.tensordot(aux_ones,psi,0)\n",
    "    E_list = -jnp.sum( h*jnp.cos(phase-psi_mat), axis=1)\n",
    "    return E_list\n",
    "\n",
    "@jax.jit\n",
    "def cost_function(phase,target,output_index):\n",
    "    # Calculate the cost function for each sample\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "    output_phase = jnp.reshape(phase[:,output_index],jnp.shape(target))\n",
    "    \n",
    "    doutput = output_phase-target\n",
    "    cost_mat = jnp.ones(jnp.shape(doutput))-jnp.cos(doutput)\n",
    "    cost_list = jnp.sum(cost_mat,1)/2\n",
    "        \n",
    "    return cost_list\n",
    "\n",
    "@jax.jit\n",
    "def total_energy(W, bias, phase, target, output_index, beta):\n",
    "\n",
    "    return internal_energy(W,phase) + bias_term(bias,phase) + beta * cost_function(phase,target,output_index)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def qualitative_cost(phase, target, output_index, tol):\n",
    "    # Calculate the cost function for each sample\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "    output_phase = jnp.reshape(phase[:,output_index],jnp.shape(target))\n",
    "    \n",
    "    doutput = output_phase-target\n",
    "    cost_mat = jnp.ones(jnp.shape(doutput))-jnp.cos(doutput)\n",
    "    cost_list = jnp.sum( (cost_mat>tol)/2 )\n",
    "        \n",
    "    return cost_list\n",
    "\n",
    "#-------------------Run the system--------------------\n",
    "@jax.jit\n",
    "def cal_dphase(phase):\n",
    "    # Calculate dphase[i,j]=phase[i]-phase[j]\n",
    "    \n",
    "    N = jnp.shape(phase)[1]\n",
    "    aux_ones=jnp.ones(N)\n",
    "    phase_mat=jnp.tensordot(aux_ones,phase,0)\n",
    "    phase_i=jnp.transpose(phase_mat,(1,2,0))\n",
    "    phase_j=jnp.transpose(phase_i,(0,2,1))\n",
    "    dphase=phase_i-phase_j\n",
    "        \n",
    "    return dphase\n",
    "\n",
    "@jax.jit\n",
    "def total_force(t, con_phase, W, bias, target, beta, input_index, output_index):\n",
    "    \n",
    "    N = jnp.shape(W)[0]\n",
    "    N_data = int(jnp.shape(con_phase)[0]/N)\n",
    "    phase = jnp.reshape(con_phase,(N_data,N))\n",
    "\n",
    "    dphase = cal_dphase(phase)\n",
    "    F0 = jnp.sum(W*jnp.sin(dphase),2)\n",
    "\n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    N_data=jnp.shape(phase)[0]\n",
    "    psi=jnp.tensordot(jnp.ones(N_data),psi,0)\n",
    "    \n",
    "    #print(target)\n",
    "    #print(phase[:,output_index])\n",
    "\n",
    "    output_phase = jnp.reshape(phase[:,output_index],jnp.shape(target))\n",
    "\n",
    "    F1 = -h * jnp.sin(phase-psi)\n",
    "\n",
    "    F2 = jnp.zeros(np.shape(phase))\n",
    "    temp_F2 = -jnp.sin(output_phase-target)\n",
    "    F2 = F2.at[:,output_index].set( jnp.reshape(temp_F2, jnp.shape(output_phase[:,output_index])) )\n",
    "    \n",
    "    '''\n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    temp_F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    F3 = F3.at[:,output_index].set( jnp.reshape(temp_F3, jnp.shape(output_phase[:,output_index])) )\n",
    "    '''\n",
    "    \n",
    "    #F3 = F3.at[:,output_index].set( -jnp.sin(0.5*(output_phase-target)) )\n",
    "    \n",
    "    # Apply cross entropy loss functoin\n",
    "    \n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    M1 = -jnp.sin(output_phase-target)\n",
    "    M2 = (1.00)*jnp.ones(jnp.shape(output_phase)) + jnp.cos(output_phase-target)\n",
    "    F3 = F3.at[:,output_index].set( jnp.reshape(M1/M2, jnp.shape(output_phase[:,output_index])) )\n",
    "    \n",
    "\n",
    "    F = -F0 + F1 + 0*beta*F2 + beta*F3\n",
    "    F = F.at[:,input_index].set( jnp.zeros([N_data, len(input_index)]) )\n",
    "    \n",
    "        \n",
    "    return jnp.concatenate(F)\n",
    "\n",
    "@jax.jit\n",
    "def ode_total_force(con_phase, t, W, bias, target, beta, input_index, output_index):\n",
    "    return total_force(t, con_phase, W, bias, target, beta, input_index, output_index)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def run_network(phase_0, T, W, bias, target, beta, input_index, output_index):\n",
    "    # Set the initial conditions\n",
    "    N_data, N = jnp.shape(phase_0)\n",
    "    \n",
    "    con_phase_0 = jnp.concatenate(phase_0)\n",
    "    \n",
    "    \n",
    "    # Solve the equation with diffrax\n",
    "    # Set parameter for diffrax\n",
    "    rtol = 1e-8\n",
    "    atol = 1e-8\n",
    "    t_span = [0,T]\n",
    "    #saveat = np.linspace(*t_span, 10).tolist()\n",
    "    odefunc = lambda t, con_phase, args: total_force(t, con_phase, W, bias, target, beta, input_index, output_index)\n",
    "    eqs = diffrax.ODETerm(odefunc)\n",
    "    \n",
    "    # Use 4th Runge Kutta\n",
    "    #solver = diffrax.Tsit5()\n",
    "    \n",
    "    #Use 5th Kvaerno for stiff case\n",
    "    solver = diffrax.Kvaerno4()\n",
    "    \n",
    "    stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
    "    #t = diffrax.SaveAt(ts=saveat)\n",
    "\n",
    "    # Solve the ODE\n",
    "    solution = diffrax.diffeqsolve(eqs, solver, t0=t_span[0], t1=t_span[1], dt0 = None, y0=con_phase_0,\n",
    "                               stepsize_controller=stepsize_controller, max_steps=1000000)\n",
    "    \n",
    "    L = len(solution.ts)\n",
    "    \n",
    "    phase = jnp.reshape(solution.ys[L-1,:], [N_data, N])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # Evolve with jaxode\n",
    "    \n",
    "    t_eval = jnp.array([0.,T])\n",
    "    solution = jaxode.odeint(ode_total_force, con_phase_0, t_eval, W, bias, target, beta, input_index, output_index, atol=1e-8, rtol=1e-8)\n",
    "    L = len(t_eval)\n",
    "    phase = jnp.reshape(solution[L-1,:], [N_data, N])\n",
    "    '''\n",
    "    \n",
    "    return phase\n",
    "\n",
    "\n",
    "#----------------------Calculate the gradient for the parameters-------------------------------\n",
    "@jax.jit\n",
    "def weights_gradient(equi_nudge, equi_free, beta):\n",
    "    # This is to calculate the contribution to weights gradient of free term in the internal energy\n",
    "    # The expression is: ( -cos(phi_i-phi_j)(\\beta!=0)+cos(phi_i-phi_j)(\\beta=0) )\n",
    "    nudge_dphase = cal_dphase(equi_nudge)\n",
    "    free_dphase = cal_dphase(equi_free)\n",
    "    gradient_list = (-jnp.cos(nudge_dphase)+jnp.cos(free_dphase))\n",
    "    #print(jnp.shape(gradient_list))\n",
    "    gradient = jnp.mean(gradient_list,axis=0)\n",
    "        \n",
    "    return gradient\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def bias_gradient(equi_nudge, equi_free, bias, beta): \n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "\n",
    "    h = jnp.tensordot(jnp.ones(jnp.shape(equi_free)[0]), h, 0)\n",
    "    psi = jnp.tensordot(jnp.ones(jnp.shape(equi_free)[0]), psi, 0)\n",
    "\n",
    "    g_h = jnp.cos(equi_free-psi) - jnp.cos(equi_nudge-psi)\n",
    "    g_psi = h*jnp.sin(equi_free-psi) - h*jnp.sin(equi_nudge-psi)\n",
    "\n",
    "    g_h = jnp.mean(g_h,axis=0)\n",
    "    g_psi = jnp.mean(g_psi,axis=0)\n",
    "\n",
    "    return jnp.asarray([g_h, g_psi])\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def paras_gradient(equi_nudge, equi_free, bias, beta): \n",
    "\n",
    "    return weights_gradient(equi_nudge, equi_free, beta) , bias_gradient(equi_nudge, equi_free, bias, beta)\n",
    "\n",
    "\n",
    "#---------------------------------Train the Network-------------------------------\n",
    "\n",
    "@jax.jit\n",
    "def weights_update(k, present_network):\n",
    "    # Initialize the system\n",
    "    phase_shape, W_0, bias_0, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag = present_network\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, input_index, output_index = model_paras\n",
    "    N_data = jnp.shape(training_data)[0]\n",
    "    N = jnp.shape(phase_shape)[1]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    input_tuple = input_index\n",
    "    output_tuple =  output_index\n",
    "\n",
    "    #initial phase\n",
    "    phase_0 = jnp.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "    phase_0 = jnp.asarray(phase_0)\n",
    "    phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "    \n",
    "    # Calculate the cost function\n",
    "    equi_zero = run_network(phase_0, T, W, bias, training_target, 0, input_index, output_index)\n",
    "    cost_list = cost_function(equi_zero, training_target, output_index)\n",
    "    cost = cost.at[k].set( jnp.sum(cost_list)/N_data )\n",
    "\n",
    "    # Update the weights\n",
    "    # Run the free and nudge phase for equilibrium propagation\n",
    "    # equi_free = run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "    equi_nudge = run_network(equi_zero, T, W, bias, training_target, beta, input_index, output_index)\n",
    "        \n",
    "    # Calculate the gradient for the weights\n",
    "    gW, gh = paras_gradient(equi_nudge, equi_zero, bias, beta)\n",
    "\n",
    "    # Update the weights and bias\n",
    "    W = W_0 - study_rate/beta*gW\n",
    "    bias = bias_0 - study_rate/beta*gh\n",
    "\n",
    "    # Formulate the output\n",
    "\n",
    "\n",
    "    return phase_shape, W, bias, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag\n",
    "\n",
    "@partial(jax.jit, static_argnames=['training_paras'])\n",
    "def jax_training(W_0, bias_0, training_data, training_target, training_paras, N_ev, dt, input_index, output_index, ext_init_phase_0=0, random_flag=False): \n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    #N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N = jnp.shape(W_0)[0]\n",
    "    N_data = jnp.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "\n",
    "    cost = jnp.zeros(N_epoch)\n",
    "    phase_shape = jnp.zeros([N_data, N])\n",
    "\n",
    "    # Training with fori_loop\n",
    "    running_training_paras = (beta, study_rate)\n",
    "    running_model_paras = (N_ev, dt, input_index, output_index)\n",
    "    initial_network = (phase_shape, W_0, bias_0, cost, training_data, training_target, running_training_paras, running_model_paras, ext_init_phase_0, random_flag)\n",
    "    \n",
    "    #time the training\n",
    "    final_network = jax.lax.fori_loop(0, N_epoch, weights_update, initial_network)\n",
    "    \n",
    "    W = final_network[1]\n",
    "    bias = final_network[2]\n",
    "    cost = final_network[3]\n",
    "    \n",
    "\n",
    "    return cost, W, bias\n",
    "\n",
    "\n",
    "def train_network(W_0, bias_0, training_data, training_target, training_paras, model_paras, ext_init_phase_0=0, random_flag=False):\n",
    "    \n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N_data = jnp.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    input_tuple = tuple(input_index)\n",
    "    output_tuple = tuple(output_index)\n",
    "    \n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    cost = jnp.zeros(N_epoch)\n",
    "    \n",
    "\n",
    "    # training\n",
    "    t1 = time.time()\n",
    "    for k in range(0,N_epoch):\n",
    "        phase_0 = jnp.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "        phase_0 = jnp.asarray(phase_0)\n",
    "        phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "        #print(phase_0)\n",
    "\n",
    "        #print(W.shape, phase_0.shape, bias.shape)\n",
    "\n",
    "        # Calculate the cost function\n",
    "        equi_zero = run_network(phase_0, T, W, bias, training_target, 0, input_index, output_index)\n",
    "        cost_list = cost_function(equi_zero, training_target, output_index)\n",
    "        cost = cost.at[k].set( jnp.sum(cost_list)/N_data )\n",
    "\n",
    "        # Run the free and nudge phase for equilibrium propagation\n",
    "        # equi_free = run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "        equi_nudge = run_network(phase_0, T, W, bias, training_target, beta, input_index, output_index)\n",
    "        \n",
    "        #print(equi_zero, '\\n\\n', equi_free, '\\n\\n', equi_nudge, '\\n\\n')\n",
    "        \n",
    "        #print(\"equi_zero= \\n\", equi_zero, \"\\n equi_free= \\n\", equi_free, \"\\n equi_nudge= \\n\", equi_nudge)\n",
    "        \n",
    "        # Calculate the gradient for the weights\n",
    "        gW, gh = paras_gradient(equi_nudge, equi_zero, bias, beta)\n",
    "        \n",
    "        #print(gW,'\\n\\n', gh)\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        W = W - study_rate*gW/beta\n",
    "        bias = bias - study_rate*gh/beta\n",
    "\n",
    "    t2 = time.time()\n",
    "    \n",
    "    '''\n",
    "    for k in range(0,N_epoch):\n",
    "        phase_0 = jnp.zeros([N_sample, N])\n",
    "        phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "\n",
    "        # Calculate the cost function\n",
    "        equi_zero = run_network(W, bias, phase_0, training_target, 0, T, input_tuple, output_tuple)\n",
    "        cost_list = total_energy(W, bias, equi_zero, training_target, output_index)\n",
    "        cost = cost.at[k].set( jnp.sum(cost_list)/N_sample )\n",
    "\n",
    "        # Run the free and nudge phase for equilibrium propagation\n",
    "        equi_free = run_network(W, bias, phase_0, training_target, -beta, T, input_tuple, output_tuple)\n",
    "        equi_nudge = run_network(W, bias, phase_0, training_target, beta, T, input_tuple, output_tuple)\n",
    "        \n",
    "        # Calculate the gradient for the weights\n",
    "        gW, gh = paras_gradient(equi_nudge, equi_free, bias, beta)\n",
    "\n",
    "        # Update the weights and bias\n",
    "        W = W - study_rate*gW\n",
    "        bias = bias - study_rate*gh\n",
    "    '''\n",
    "    \n",
    "    t3 = time.time()\n",
    "\n",
    "    return cost, W, bias\n",
    "\n",
    "#--------------------------Consecutive Training------------------------\n",
    "'''\n",
    "Training the net work with consecutive scheme: start from the equilibrium we get from the last epoch.\n",
    "'''\n",
    "\n",
    "@jax.jit\n",
    "def consecutive_weights_update(k, present_network):\n",
    "    # Initialize the system\n",
    "    phase_shape, W_0, bias_0, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag = present_network\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, input_index, output_index = model_paras\n",
    "    N_data= jnp.shape(training_data)[0]\n",
    "    N = jnp.shape(phase_shape)[1]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    input_tuple = input_index\n",
    "    output_tuple =  output_index\n",
    "\n",
    "    #initial phase\n",
    "    phase_0 = jnp.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "    phase_0 = jnp.asarray(phase_0)\n",
    "    phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "    \n",
    "    # Calculate the cost function\n",
    "    equi_zero = run_network(phase_0, T, W, bias, training_target, 0, input_index, output_index)\n",
    "    cost_list = cost_function(equi_zero, training_target, output_index)\n",
    "    cost = cost.at[k].set( jnp.sum(cost_list)/N_data )\n",
    "\n",
    "    # Update the weights\n",
    "    # Run the free and nudge phase for equilibrium propagation\n",
    "    # equi_free = run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "    equi_nudge = run_network(equi_zero, T, W, bias, training_target, beta, input_index, output_index)\n",
    "        \n",
    "    # Calculate the gradient for the weights\n",
    "    gW, gh = paras_gradient(equi_nudge, equi_zero, bias, beta)\n",
    "\n",
    "    # Update the weights and bias\n",
    "    W = W_0 - study_rate/beta*gW\n",
    "    bias = bias_0 - study_rate/beta*gh\n",
    "\n",
    "    # Formulate the output\n",
    "\n",
    "\n",
    "    return phase_shape, W, bias, cost, training_data, training_target, training_paras, model_paras, equi_zero, False\n",
    "\n",
    "@partial(jax.jit, static_argnames=['training_paras'])\n",
    "def consecutive_jax_training(W_0, bias_0, training_data, training_target, training_paras, N_ev, dt, input_index, output_index, ext_init_phase_0=0, random_flag=False): \n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    #N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N = jnp.shape(W_0)[0]\n",
    "    N_sample = jnp.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "\n",
    "    cost = jnp.zeros(N_epoch)\n",
    "    phase_shape = jnp.zeros([N_sample, N])\n",
    "\n",
    "    # Training with fori_loop\n",
    "    running_training_paras = (beta, study_rate)\n",
    "    running_model_paras = (N_ev, dt, input_index, output_index)\n",
    "    initial_network = (phase_shape, W_0, bias_0, cost, training_data, training_target, running_training_paras, running_model_paras, ext_init_phase_0, random_flag)\n",
    "    \n",
    "    #time the training\n",
    "    final_network = jax.lax.fori_loop(0, N_epoch, consecutive_weights_update, initial_network)\n",
    "    \n",
    "    W = final_network[1]\n",
    "    bias = final_network[2]\n",
    "    cost = final_network[3]\n",
    "    \n",
    "\n",
    "    return cost, W, bias\n",
    "\n",
    "\n",
    "#--------------------------Do multiple training in single function------------------------\n",
    "\n",
    "@jax.jit\n",
    "def loop_jax_training(k, task_paras):\n",
    "    # Here W_0 and bias_0 are lists of weights and bias that are generated outside the function\n",
    "    cost_list, cost, W_0, bias_0, training_data, training_target, training_paras, model_paras = task_paras\n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N_sample = jnp.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    N_cell = np.shape(W_0)[1]\n",
    "    phase_shape = jnp.zeros([N_sample, N_cell])\n",
    "\n",
    "    # Prepare parameters for training with fori_loop\n",
    "    running_training_paras = (beta, study_rate)\n",
    "    running_model_paras = (N_ev, dt, input_index, output_index)\n",
    "    initial_network = (phase_shape, W_0[k,:,:], bias_0[k,:,:], cost, training_data, training_target, running_training_paras, running_model_paras, 0, False)\n",
    "\n",
    "    #time the training\n",
    "    final_network = jax.lax.fori_loop(0, N_epoch, weights_update, initial_network)\n",
    "    \n",
    "    W = final_network[1]\n",
    "    bias = final_network[2]\n",
    "    new_cost = final_network[3]\n",
    "    \n",
    "    # Formulate the output\n",
    "    W_trained = W_0.at[k,:,:].set(W)\n",
    "    bias_trained = bias_0.at[k,:,:].set(bias)\n",
    "    cost_list = cost_list.at[k,:].set(new_cost)\n",
    "    \n",
    "    return cost_list, cost, W_trained, bias_trained, training_data, training_target, training_paras, model_paras\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=['training_paras'])\n",
    "def multiple_jax_training(N_run, cost_list_0, cost_0, W_0, bias_0, training_data, training_target, training_paras, model_paras):\n",
    "    task_paras = cost_list_0, cost_0[0], W_0, bias_0, training_data, training_target, training_paras, model_paras\n",
    "    \n",
    "    # time the training\n",
    "    trained_networks = jax.lax.fori_loop(0, N_run, loop_jax_training, task_paras)\n",
    "    \n",
    "    W_list = trained_networks[2]\n",
    "    bias_list = trained_networks[3]\n",
    "    cost_list = trained_networks[0]\n",
    "    \n",
    "    return cost_list\n",
    "\n",
    "#@partial(jax.jit, static_argnames=['training_paras', 'training_data', 'training_target'])\n",
    "def hv_jax_training(W_0, bias_0, training_data, training_target, training_paras, N_ev, dt, input_index, output_index, ext_init_phase_0=0, random_flag=False):\n",
    "    # Train many networks together in parallelism\n",
    "    \n",
    "    multiple_run_network = jax.vmap(run_network, (0,None,0,0,None,None,None,None))\n",
    "    multiple_paras_gradient = jax.vmap(paras_gradient, (0,0,0,None))\n",
    "    multiple_cost_function = jax.vmap(cost_function, (0,None,None))\n",
    "    \n",
    "    @jax.jit\n",
    "    def hv_update(k,present_network):\n",
    "        \n",
    "        W_0, bias_0, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag = present_network\n",
    "        beta, study_rate = training_paras\n",
    "        N_ev, dt, input_index, output_index = model_paras\n",
    "        N_data = jnp.shape(training_data)[0]\n",
    "        N_run = jnp.shape(W_0)[0]\n",
    "        N = jnp.shape(W_0)[1]\n",
    "        T = N_ev * dt\n",
    "\n",
    "        #initial phase\n",
    "        phase_0 = jnp.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "        phase_0 = jnp.asarray(phase_0)\n",
    "        phase_0 = phase_0.at[:,input_index].set(training_data)\n",
    "        phase_0 = jnp.tensordot(jnp.ones(N_run), phase_0, 0)\n",
    "\n",
    "        # Calculate the cost function\n",
    "        equi_zero = multiple_run_network(phase_0, T, W_0[k], bias_0[k], training_target, 0, input_index, output_index)\n",
    "        cost_list = multiple_cost_function(equi_zero, training_target, output_index)\n",
    "        cost = cost.at[:,k].set( jnp.mean(cost_list, axis=1) )\n",
    "\n",
    "        # Update the weights\n",
    "        # Run the free and nudge phase for equilibrium propagation\n",
    "        # equi_free = run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "        equi_nudge = multiple_run_network(equi_zero, T, W_0, bias_0, training_target, beta, input_index, output_index)\n",
    "\n",
    "        # Calculate the gradient for the weights\n",
    "        gW, gh = multiple_paras_gradient(equi_nudge, equi_zero, bias_0, beta)\n",
    "\n",
    "        # Update the weights and bias\n",
    "        W = W_0 - study_rate/beta*gW\n",
    "        bias = bias_0 - study_rate/beta*gh\n",
    "\n",
    "        return W, bias, cost, training_data, training_target, training_paras, model_paras, ext_init_phase_0, random_flag\n",
    "    \n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    N_run = jnp.shape(W_0)[0]\n",
    "    \n",
    "    cost_list_0 = jnp.zeros([N_run, N_epoch])\n",
    "    \n",
    "    running_training_paras = (beta, study_rate)\n",
    "    running_model_paras = (N_ev, dt, input_index, output_index)\n",
    "    \n",
    "    initial_networks = W_0, bias_0, cost_list_0, training_data, training_target, running_training_paras, running_model_paras, ext_init_phase_0, random_flag\n",
    "    \n",
    "    trained_networks = jax.lax.fori_loop(0, N_epoch, hv_update, initial_networks)\n",
    "    W, bias, cost = trained_networks[0], trained_networks[1], trained_networks[2]\n",
    "    \n",
    "    return cost, W, bias\n",
    "    \n",
    "vmul_jax_training = jax.vmap(jax_training, (0,0,None,None,(None,None,None),None,None,None,None,None,None))\n",
    "vmul_consecutive_jax_training = jax.vmap(consecutive_jax_training, (0,0,None,None,(None,None,None),None,None,None,None,None,None))\n",
    "\n",
    "\n",
    "#=================CPU training===================\n",
    "# Use scipy\n",
    "#------------------calculate equilibriums-------------------\n",
    "def sp_cal_dphase(phase):\n",
    "    # Calculate dphase[i,j]=phase[i]-phase[j]\n",
    "    \n",
    "    N = np.shape(phase)[1]\n",
    "    aux_ones=np.ones(N)\n",
    "    phase_mat=np.tensordot(aux_ones,phase,0)\n",
    "    phase_i=np.transpose(phase_mat,(1,2,0))\n",
    "    phase_j=np.transpose(phase_i,(0,2,1))\n",
    "    dphase=phase_i-phase_j\n",
    "        \n",
    "    return dphase\n",
    "\n",
    "def sp_total_force(t, con_phase, W, bias, target, beta, input_index, output_index):\n",
    "    \n",
    "    N = np.shape(W)[0]\n",
    "    Nh = int(np.shape(con_phase)[0]/N)\n",
    "    phase = np.reshape(con_phase,(Nh,N))\n",
    "\n",
    "    dphase = sp_cal_dphase(phase)\n",
    "    F0 = np.sum(W*np.sin(dphase),2)\n",
    "\n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    N_data=np.shape(phase)[0]\n",
    "    psi=np.tensordot(np.ones(N_data),psi,0)\n",
    "    \n",
    "    #print(target)\n",
    "    #print(phase[:,output_index])\n",
    "\n",
    "    output_phase = np.reshape(phase[:,output_index],np.shape(target))\n",
    "\n",
    "    F1 = -h * np.sin(phase-psi)\n",
    "\n",
    "    F2 = np.zeros(np.shape(phase))\n",
    "    temp_F2 = -np.sin(output_phase-target)\n",
    "    F2[:,output_index] = temp_F2\n",
    "    \n",
    "    '''\n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    temp_F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    F3 = F3.at[:,output_index].set( jnp.reshape(temp_F3, jnp.shape(output_phase[:,output_index])) )\n",
    "    '''\n",
    "    \n",
    "    #F3 = F3.at[:,output_index].set( -jnp.sin(0.5*(output_phase-target)) )\n",
    "    \n",
    "    # Apply cross entropy loss functoin\n",
    "    \n",
    "    F3 = np.zeros(np.shape(phase))\n",
    "    M1 = -np.sin(output_phase-target)\n",
    "    M2 = 1.0*np.ones(np.shape(output_phase)) + np.cos(output_phase-target)\n",
    "    F3[:,output_index] = M1/M2\n",
    "    \n",
    "\n",
    "    F = -F0 + F1 + 0*beta*F2 + beta*F3\n",
    "    F[:,input_index] = np.zeros([Nh, len(input_index)])\n",
    "    \n",
    "        \n",
    "    return np.concatenate(F)\n",
    "\n",
    "def sp_ode_total_force(con_phase, t, W, bias, target, beta, input_index, output_index):\n",
    "    return sp_total_force(t, con_phase, W, bias, target, beta, input_index, output_index)\n",
    "\n",
    "\n",
    "def sp_run_network(phase_0, T, W, bias, target, beta, input_index, output_index):\n",
    "    # Set the initial conditions\n",
    "    N_data, N = np.shape(phase_0)\n",
    "    \n",
    "    con_phase_0 = np.concatenate(phase_0)\n",
    "    \n",
    "    # Evolve with scipy\n",
    "    \n",
    "    t_span = np.array([0.,T])\n",
    "    \n",
    "    solution = scipy.integrate.solve_ivp(sp_total_force, t_span, con_phase_0, method='LSODA', args=[W, bias, target, beta, input_index, output_index], rtol = 1.4e-8, atol = 1.4e-8)\n",
    "    \n",
    "    L = len(solution.t)\n",
    "    con_phase = solution.y[:,L-1]\n",
    "    phase = np.reshape(con_phase, [N_data, N])\n",
    "\n",
    "    return phase\n",
    "\n",
    "\n",
    "#------------------------train the parameters-----------------------\n",
    "\n",
    "\n",
    "def sp_weights_gradient(equi_nudge, equi_free, beta):\n",
    "    # This is to calculate the contribution to weights gradient of free term in the internal energy\n",
    "    # The expression is: ( -cos(phi_i-phi_j)(\\beta!=0)+cos(phi_i-phi_j)(\\beta=0) )\n",
    "    N_data = np.shape(equi_free)[0]\n",
    "    nudge_dphase = sp_cal_dphase(equi_nudge)\n",
    "    free_dphase = sp_cal_dphase(equi_free)\n",
    "    gradient_list = (-np.cos(nudge_dphase)+np.cos(free_dphase))\n",
    "    #print(jnp.shape(gradient_list))\n",
    "    gradient = np.mean(gradient_list,axis=0)\n",
    "        \n",
    "    return gradient\n",
    "\n",
    "def sp_bias_gradient(equi_nudge, equi_free, bias, beta): \n",
    "        \n",
    "    N_sample = np.shape(equi_free)[0]\n",
    "    gradient = 0\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "\n",
    "    h = np.tensordot(np.ones(np.shape(equi_free)[0]), h, 0)\n",
    "    psi = np.tensordot(np.ones(np.shape(equi_free)[0]), psi, 0)\n",
    "\n",
    "    g_h = np.cos(equi_free-psi) - np.cos(equi_nudge-psi)\n",
    "    g_psi = h*np.sin(equi_free-psi) - h*np.sin(equi_nudge-psi)\n",
    "\n",
    "    g_h = np.mean(g_h,axis=0)\n",
    "    g_psi = np.mean(g_psi,axis=0)\n",
    "\n",
    "    return np.asarray([g_h, g_psi])\n",
    "\n",
    "\n",
    "def sp_paras_gradient(equi_nudge, equi_free, bias, beta): \n",
    "\n",
    "    return sp_weights_gradient(equi_nudge, equi_free, beta) , sp_bias_gradient(equi_nudge, equi_free, bias, beta)\n",
    "\n",
    "\n",
    "def sp_train_network(W_0, bias_0, training_data, training_target, training_paras, model_paras, ext_init_phase_0=0, random_flag=False):\n",
    "    \n",
    "    # Initialize the system\n",
    "    N_epoch, beta, study_rate = training_paras\n",
    "    N, N_ev, dt, input_index, output_index = model_paras\n",
    "    N_data = np.shape(training_data)[0]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    cost = np.zeros(N_epoch)\n",
    "    \n",
    "\n",
    "    # training\n",
    "    for k in range(0,N_epoch):\n",
    "        phase_0 = np.zeros([N_data, N]) + ext_init_phase_0 + random_flag*2*np.pi*(np.random.rand(N_data, N)-0.5)\n",
    "        phase_0[:,input_index] = training_data\n",
    "        #print(phase_0)\n",
    "\n",
    "        #print(W.shape, phase_0.shape, bias.shape)\n",
    "\n",
    "        # Calculate the cost function\n",
    "        equi_zero = sp_run_network(phase_0, T, W, bias, training_target, 0, input_index, output_index)\n",
    "        cost_list = cost_function(equi_zero, training_target, output_index)\n",
    "        cost[k] = np.sum(cost_list)/N_data\n",
    "\n",
    "        # Run the free and nudge phase for equilibrium propagation\n",
    "        equi_free = sp_run_network(phase_0, T, W, bias, training_target, -beta, input_index, output_index)\n",
    "        equi_nudge = sp_run_network(phase_0, T, W, bias, training_target, beta, input_index, output_index)\n",
    "        \n",
    "        #print(equi_zero, '\\n\\n', equi_free, '\\n\\n', equi_nudge, '\\n\\n')\n",
    "        \n",
    "        #print(\"equi_zero= \\n\", equi_zero, \"\\n equi_free= \\n\", equi_free, \"\\n equi_nudge= \\n\", equi_nudge)\n",
    "        \n",
    "        # Calculate the gradient for the weights\n",
    "        gW, gh = sp_paras_gradient(equi_nudge, equi_zero, bias, beta)\n",
    "        \n",
    "        #print(gW,'\\n\\n', gh)\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        W = W - study_rate*gW/2/beta\n",
    "        bias = bias - study_rate*gh/2/beta\n",
    "\n",
    "    return cost, W, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5609b186",
   "metadata": {
    "id": "5609b186",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the network with optimization methods in jaxopt\n",
    "\n",
    "@jax.jit\n",
    "def opt_total_energy(variable_phase, W, bias, input_data, target, input_index, variable_index, output_index, beta):\n",
    "    \n",
    "    N = jnp.shape(input_index)[0]+jnp.shape(variable_index)[0]\n",
    "    \n",
    "    #Set the phase to calculate the energy\n",
    "    phase = jnp.zeros(N)\n",
    "    phase = phase.at[input_index].set(input_data)\n",
    "\n",
    "    phase = phase.at[variable_index].set(variable_phase)\n",
    "    \n",
    "    #Calculate the internal energy\n",
    "    aux_ones = jnp.ones(N)\n",
    "    phase_mat = jnp.tensordot(aux_ones, phase, 0)\n",
    "    dphase = phase_mat - jnp.transpose(phase_mat, [1,0])\n",
    "    E_in = -0.5 * jnp.sum(W * jnp.cos(dphase))\n",
    "    \n",
    "    #Calculate the energy from the bias terms\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "    E_bias = -jnp.sum(h * jnp.cos(phase-psi))\n",
    "    \n",
    "    #Calculate the energy from the cost function\n",
    "    output_phase = phase[output_index]\n",
    "    E_cost = -jnp.sum(jnp.log(1 + jnp.cos(output_phase - target)))\n",
    "\n",
    "    return E_in + E_bias + beta * E_cost\n",
    "\n",
    "@jax.jit\n",
    "def opt_total_force(variable_phase, W, bias, input_data, target, input_index, variable_index, output_index, beta):\n",
    "    # This calculate the gradient of the energy (-F_total)\n",
    "    phase = jnp.concatenate((input_data, variable_phase))\n",
    "    phase_mat = jnp.asarray([phase])\n",
    "    dphase = -phase_mat + jnp.transpose(phase_mat,[1,0])\n",
    "    \n",
    "    output_phase = phase[output_index]\n",
    "    doutput = output_phase-target\n",
    "    \n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    N_cell = jnp.shape(phase)[0]\n",
    "    N_temp = N_cell - jnp.shape(output_index)[0]\n",
    "    \n",
    "    F0 = jnp.sum(W*jnp.sin(dphase),axis=1)\n",
    "    \n",
    "    F1 = -h * jnp.sin(phase-psi)\n",
    "    \n",
    "    \n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    M1 = -jnp.sin(output_phase-target)\n",
    "    M2 = (1.00)*jnp.ones(jnp.shape(output_phase)) + jnp.cos(output_phase-target)\n",
    "\n",
    "    #temp_F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    F3 = F3.at[output_index].set(M1/M2)\n",
    "    \n",
    "    \n",
    "    #F3 = F3.at[:,output_index].set( -jnp.sin(0.5*(output_phase-target)) )\n",
    "    \n",
    "    # Apply cross entropy loss function\n",
    "\n",
    "    F = -F0 + F1 + beta*F3\n",
    "    \n",
    "    F = F[variable_index]\n",
    "    \n",
    "    return F\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def opt_energy_with_grad(variable_phase, args):\n",
    "    W, bias, input_data, target, input_index, variable_index, output_index, beta = args\n",
    "    E = opt_total_energy(variable_phase, W, bias, input_data, target, input_index, variable_index, output_index, beta)\n",
    "    F = opt_total_force(variable_phase, W, bias, input_data, target, input_index, variable_index, output_index, beta)\n",
    "\n",
    "    return E, -F\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def single_opt_run_network(phase_0, W, bias, target, beta, input_index, variable_index, output_index):\n",
    "    #solver = jaxopt.LBFGS(opt_energy_with_grad, value_and_grad=True, maxiter=10000, tol=1e-8)\n",
    "    solver = jaxopt.GradientDescent(opt_energy_with_grad, value_and_grad=True, maxiter=10000, tol=1e-8)\n",
    "    input_data = phase_0[input_index]\n",
    "    variable_phase_0 = phase_0[variable_index]\n",
    "\n",
    "    args = W, bias, input_data, target, input_index, variable_index, output_index, beta\n",
    "\n",
    "    res = solver.run(variable_phase_0, args)\n",
    "\n",
    "    variable_phase = res.params\n",
    "    phase = jnp.concatenate((input_data, variable_phase))\n",
    "\n",
    "    return phase\n",
    "\n",
    "v_opt_run_network = jax.vmap(single_opt_run_network, (0,None,None,0,None,None,None,None))\n",
    "\n",
    "@jax.jit\n",
    "def opt_run_network_inloop(k, args):\n",
    "\n",
    "    phase, phase_0, W, bias, target, beta, input_index, variable_index, output_index = args\n",
    "\n",
    "    #solver = jaxopt.LBFGS(opt_energy_with_grad, value_and_grad=True, maxiter=10000, tol=1e-8)\n",
    "    solver = jaxopt.GradientDescent(opt_energy_with_grad, value_and_grad=True, maxiter=10000, tol=1e-8)\n",
    "    input_data = phase_0[k,input_index]\n",
    "    variable_phase_0 = phase_0[k,variable_index]\n",
    "\n",
    "    init_args = W, bias, input_data, target[k,:], input_index, variable_index, output_index, beta\n",
    "\n",
    "    res = solver.run(variable_phase_0, init_args)\n",
    "\n",
    "    variable_phase = res.params\n",
    "    phase = phase.at[k,:].set(jnp.concatenate((input_data, variable_phase)))\n",
    "\n",
    "    return phase, phase_0, W, bias, target, beta, input_index, variable_index, output_index\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def mul_opt_run_network(phase_0, W, bias, target, beta, input_index, variable_index, output_index):\n",
    "    phase = jnp.zeros(phase_0.shape)\n",
    "    N_data, N = phase_0.shape\n",
    "    args_0 = phase, phase_0, W, bias, target, beta, input_index, variable_index, output_index\n",
    "    result = jax.lax.fori_loop(0, N_data,opt_run_network_inloop, args_0)\n",
    "\n",
    "    phase = result[0]\n",
    "\n",
    "    return phase\n",
    "\n",
    "\n",
    "#==================Eequivalence verification=================\n",
    "# Verify the equivalence between opt and ode\n",
    "\n",
    "def dE_dparas(equi, bias):\n",
    "    # This is to calculate the contribution to weights gradient of free term in the internal energy\n",
    "    # The expression is: ( -cos(phi_i-phi_j)(\\beta!=0)+cos(phi_i-phi_j)(\\beta=0) )\n",
    "    equi_mat = jnp.asarray([equi])\n",
    "    free_dphase = equi_mat - jnp.transpose(equi_mat)\n",
    "    g_W = -jnp.cos(free_dphase)\n",
    "\n",
    "    #Calculate derivative over bias\n",
    "\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "\n",
    "    #h = jnp.tensordot(jnp.ones(jnp.shape(equi)[0]), h, 0)\n",
    "    #psi = jnp.tensordot(jnp.ones(jnp.shape(equi)[0]), psi, 0)\n",
    "\n",
    "    #print(equi.shape, psi.shape, h.shape)\n",
    "    g_h = -jnp.cos(equi-psi)\n",
    "    g_psi = -h*jnp.sin(equi-psi)\n",
    "        \n",
    "    return g_W, g_h, g_psi\n",
    "\n",
    "mul_dE_dparas = jax.vmap(dE_dparas, (0,None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42bc2edd",
   "metadata": {
    "id": "42bc2edd",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def qualitative_cost(phase, target, output_index, tol):\n",
    "    # Calculate the cost function for each sample\n",
    "    # !!Note: phase should be a 2-d tensor (N_sample (or others) x N_cell). For single phase, use [phase] as the input.\n",
    "        \n",
    "    output_phase = jnp.reshape(phase[:,output_index],jnp.shape(target))\n",
    "    \n",
    "    doutput = output_phase-target\n",
    "    cost_mat = jnp.ones(jnp.shape(doutput))-jnp.cos(doutput)\n",
    "    cost_list = jnp.sum( (cost_mat>tol)/2 )\n",
    "        \n",
    "    return cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6209a601",
   "metadata": {
    "id": "6209a601",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running and training XY network with layer structure\n",
    "#=======================================Running without layer structure=============================================\n",
    "@jax.jit\n",
    "def single_force(t, phase, W, bias, target, input_index, output_index, beta):\n",
    "    # This calculate the gradient of the energy (-F_total)\n",
    "    phase_mat = jnp.asarray([phase])\n",
    "    dphase = -phase_mat + jnp.transpose(phase_mat,[1,0])\n",
    "    \n",
    "    output_phase = phase[output_index]\n",
    "    \n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    N_cell = jnp.shape(phase)[0]\n",
    "    N_temp = N_cell - jnp.shape(output_index)[0]\n",
    "    \n",
    "    F0 = jnp.sum(W*jnp.sin(dphase),axis=1)\n",
    "    \n",
    "    F1 = -h * jnp.sin(phase-psi)\n",
    "    \n",
    "    \n",
    "    F3 = jnp.zeros(np.shape(phase))\n",
    "    M1 = -jnp.sin(output_phase-target)\n",
    "    M2 = (1.00)*jnp.ones(jnp.shape(output_phase)) + jnp.cos(output_phase-target)\n",
    "    F3 = F3.at[output_index].set(M1/M2)\n",
    "    \n",
    "    #temp_F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    #F3 = F3.at[output_index].set( -jnp.sin(0.5*(output_phase-target)) )\n",
    "    \n",
    "    # Apply cross entropy loss functoin\n",
    "\n",
    "    F = -F0 + F1 + beta*F3\n",
    "    \n",
    "    F = F.at[input_index].set(0)\n",
    "    \n",
    "    return F\n",
    "\n",
    "@jax.jit\n",
    "def single_run_network(phase_0, T, W, bias, target, beta, input_index, output_index):\n",
    "    # Solve the equation with diffrax\n",
    "    # Set parameter for diffrax\n",
    "    rtol = 1e-8\n",
    "    atol = 1e-8\n",
    "    t_span = [0,T]\n",
    "    #saveat = np.linspace(*t_span, 10).tolist()\n",
    "    odefunc = lambda t, phase, args: single_force(t, phase, W, bias, target, input_index, output_index, beta)\n",
    "    eqs = diffrax.ODETerm(odefunc)\n",
    "    \n",
    "    # Use 4th Runge Kutta\n",
    "    solver = diffrax.Tsit5()\n",
    "    \n",
    "    #Use 5th Kvaerno for stiff case\n",
    "    #solver = diffrax.Kvaerno4()\n",
    "    \n",
    "    stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
    "    #t = diffrax.SaveAt(ts=saveat)\n",
    "\n",
    "    # Solve the ODE\n",
    "    solution = diffrax.diffeqsolve(eqs, solver, t0=t_span[0], t1=t_span[1], dt0 = None, y0=phase_0,\n",
    "                               stepsize_controller=stepsize_controller, max_steps=10000000)\n",
    "    \n",
    "    L = len(solution.ts)\n",
    "    \n",
    "    phase = solution.ys[L-1,:]\n",
    "\n",
    "    return phase\n",
    "\n",
    "@jax.jit\n",
    "def mul_run_network(phase_0, T, W, bias, target, beta, input_index, output_index):\n",
    "    odefunc = lambda phase_0, target: single_run_network(phase_0, T, W, bias, target, beta, input_index, output_index)\n",
    "    phase = jax.tree_map(odefunc, list(phase_0), list(target))\n",
    "    return jnp.asarray(phase)\n",
    "\n",
    "v_run_network = jax.vmap(single_run_network, (0,None,None,None,0,None,None,None))\n",
    "\n",
    "\n",
    "#=================================Calculate the energy and force with layered structure=================================\n",
    "@jax.jit\n",
    "def backward_force(phase, W, phase_next):\n",
    "    mphase = phase[None,:]\n",
    "    mphase_next = phase_next[None,:]\n",
    "    return jnp.sum(W * jnp.sin(jnp.transpose(mphase, [1,0]) - mphase_next), axis=1)\n",
    "\n",
    "@jax.jit\n",
    "def forward_force(phase, W, phase_last):\n",
    "    mphase = phase[None,:]\n",
    "    mphase_last = phase_last[None,:]\n",
    "    return jnp.sum(W * jnp.sin(mphase - jnp.transpose(mphase_last, [1,0])), axis=0)\n",
    "\n",
    "@jax.jit\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "@jax.jit\n",
    "def layered_free_energy(phase1, W, phase2):\n",
    "    #This calculate the energy of free energy\n",
    "    '''\n",
    "        W: W^{n,n+1}, coupling between the n-th layer and n+1-th layer. n = 0,1,2...L, L+1 the number of layers in the network. \n",
    "        phase: phi^n\n",
    "        next_phase : phi^{n+1}\n",
    "    '''\n",
    "    \n",
    "    E = -jnp.sum( W * jnp.cos(jnp.transpose(phase1[None,:]) - phase2[None,:]) )\n",
    "    return E\n",
    "\n",
    "@jax.jit\n",
    "def layered_energy(phase, WL, bias, target, structure_shape, beta):\n",
    "    \n",
    "    # Calculate the free energy layer by layer\n",
    "    # Split and translate the phase according to layer structure\n",
    "    split_points = jax.tree_map(jnp.size, structure_shape)\n",
    "    phase_list = jnp.split(phase, split_points)\n",
    "    \n",
    "    phase1_list = phase_list.copy()\n",
    "    phase2_list = phase_list.copy()\n",
    "    \n",
    "    del phase1_list[-1]\n",
    "    del phase2_list[0]\n",
    "    \n",
    "    E0 = jnp.sum( jnp.asarray(jax.tree_map(layered_free_energy, phase1_list, WL, phase2_list)) )\n",
    "    \n",
    "    #Calculate the energy from the bias terms\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "    E_bias = -jnp.sum(h * jnp.cos(phase-psi))\n",
    "    \n",
    "    #Calculate the energy from the cost function\n",
    "    output_phase = phase_list[-1]\n",
    "    E_cost = -jnp.sum(jnp.log(1 + jnp.cos(output_phase - target)))\n",
    "    \n",
    "    #Sum all the terms and get the total energy\n",
    "    E = E0 + E_bias + beta * E_cost\n",
    "    \n",
    "    return E\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def layered_force(t, phase, WL, bias, target, structure_shape, beta):\n",
    "    split_points = jax.tree_map(jnp.size, structure_shape)\n",
    "    phase_list = jnp.split(phase, split_points)\n",
    "    L = len(split_points)\n",
    "    phase_next_list = phase_list.copy()\n",
    "    phase_next_list.append(phase_list[len(phase_list)-1])\n",
    "    del phase_next_list[0]\n",
    "    \n",
    "    phase_last_list = phase_list.copy()\n",
    "    phase_last_list.insert(0, phase_list[0])\n",
    "    del phase_last_list[L+1]\n",
    "    \n",
    "    WL_back = WL.copy()\n",
    "    WL_back.append(jnp.zeros([phase_list[L].shape[0], phase_list[L].shape[0]]))\n",
    "    WL_forward = WL.copy()\n",
    "    WL_forward.insert(0, jnp.zeros([phase_list[0].shape[0], phase_list[0].shape[0]]))\n",
    "    \n",
    "    # Get the backward and forward force\n",
    "    BF = jax.tree_map(backward_force, phase_list, WL_back, phase_next_list)\n",
    "    FF = jax.tree_map(forward_force, phase_list, WL_forward, phase_last_list)\n",
    "    \n",
    "    FL0 = jax.tree_map(add, BF, FF)\n",
    "    FL0[0] = FL0[0]*0\n",
    "    \n",
    "    # Calculate bias force\n",
    "    \n",
    "    h=bias[0,:]\n",
    "    psi=bias[1,:]\n",
    "    F1 = -h * jnp.sin(phase-psi)\n",
    "    F1 = F1.at[0:split_points[0]].set(0)\n",
    "    \n",
    "    # Calculate the force from cost function\n",
    "    \n",
    "    output_phase = phase_list[len(phase_list)-1]\n",
    "    M1 = -jnp.sin(output_phase - target)\n",
    "    M2 = (1.00)*jnp.ones(jnp.shape(output_phase)) + jnp.cos(output_phase-target)\n",
    "    #F3 = -jnp.sin(0.5*(output_phase-target))\n",
    "    F3 = M1/M2\n",
    "    \n",
    "    FL0[len(FL0)-1] = FL0[len(FL0)-1] - beta*F3\n",
    "    F = -jnp.concatenate(FL0) + F1\n",
    "    \n",
    "    #Merge the forces together and concatenate\n",
    "    \n",
    "    \n",
    "    \n",
    "    #return phase_list, phase_last_list, phase_last_list, WL_forward, WL_back, BF, FF\n",
    "    return F\n",
    "    \n",
    "\n",
    "#=======================================Run network with solving ODE======================================\n",
    "@jax.jit\n",
    "def single_layer_run_network(phase_0, T, WL, bias, target, structure_shape, beta):\n",
    "    #run network with layer structure for single inputs\n",
    "    # Solve the equation with diffrax\n",
    "    # Set parameter for diffrax\n",
    "    rtol = 1e-8\n",
    "    atol = 1e-8\n",
    "    t_span = [0,T]\n",
    "    #saveat = np.linspace(*t_span, 10).tolist()\n",
    "    odefunc = lambda t, phase, args: layered_force(t, phase, WL, bias, target, structure_shape, beta)\n",
    "    eqs = diffrax.ODETerm(odefunc)\n",
    "    \n",
    "    # Use 4th Runge Kutta\n",
    "    solver = diffrax.Tsit5()\n",
    "    \n",
    "    #Use 5th Kvaerno for stiff case\n",
    "    #solver = diffrax.Kvaerno4()\n",
    "    \n",
    "    stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
    "    #t = diffrax.SaveAt(ts=saveat)\n",
    "\n",
    "    # Solve the ODE\n",
    "    solution = diffrax.diffeqsolve(eqs, solver, t0=t_span[0], t1=t_span[1], dt0 = None, y0=phase_0,\n",
    "                               stepsize_controller=stepsize_controller, max_steps=10000000)\n",
    "    \n",
    "    L = len(solution.ts)\n",
    "    \n",
    "    phase = solution.ys[L-1,:]\n",
    "\n",
    "    return phase\n",
    "\n",
    "@jax.jit\n",
    "def ode_run_layered_network(phase_0, T, WL, bias, target, structure_shape, beta):\n",
    "    odefunc = lambda phase_0, target: single_layer_run_network(phase_0, T, WL, bias, target, structure_shape, beta)\n",
    "    phase = jax.tree_map(odefunc, list(phase_0), list(target))\n",
    "    return jnp.asarray(phase)\n",
    "\n",
    "@jax.jit\n",
    "def v_run_layered_network(phase_0, T, WL, bias, target, structure_shape, beta):\n",
    "    odefunc = lambda phase_0, target: single_layer_run_network(phase_0, T, WL, bias, target, structure_shape, beta)\n",
    "    phase = jax.vmap(odefunc, (0,0))(phase_0, target)\n",
    "    return phase\n",
    "\n",
    "\n",
    "#==========================================Running with optimization methods==========================================\n",
    "\n",
    "@jax.jit\n",
    "def opt_layered_energy(variable_phase, input_data, WL, bias, target, structure_shape, beta):\n",
    "    phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    return layered_energy(phase, WL, bias, target, structure_shape, beta)\n",
    "\n",
    "@jax.jit\n",
    "def opt_layered_grad(variable_phase, input_data, WL, bias, target, structure_shape, beta):\n",
    "    variable_index = jnp.arange(structure_shape[0].shape[0], bias.shape[1])\n",
    "    phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    grad_E = -layered_force(0, phase, WL, bias, target, structure_shape, beta)[variable_index]\n",
    "    \n",
    "    return grad_E\n",
    "\n",
    "@jax.jit\n",
    "def opt_layered_energy_with_grad(variable_phase, input_data, WL, bias, target, structure_shape, beta):\n",
    "    E = opt_layered_energy(variable_phase, input_data, WL, bias, target, structure_shape, beta)\n",
    "    grad_E = opt_layered_grad(variable_phase, input_data, WL, bias, target, structure_shape, beta)\n",
    "    \n",
    "    return E, grad_E\n",
    "\n",
    "@jax.jit\n",
    "def opt_single_layer_run_network(phase_0, WL, bias, target, structure_shape, beta):\n",
    "    # Run network for single set of input data\n",
    "    \n",
    "    input_index = jnp.arange(0, structure_shape[0].shape[0])\n",
    "    variable_index = jnp.arange(structure_shape[0].shape[0], bias.shape[1])\n",
    "    \n",
    "    input_data = phase_0[input_index]\n",
    "    variable_phase_0 = phase_0[variable_index]\n",
    "    \n",
    "    optfunc = lambda variable_phase: opt_layered_energy_with_grad(variable_phase, input_data, WL, bias, target, structure_shape, beta)\n",
    "    \n",
    "    solver = jaxopt.LBFGS(optfunc, value_and_grad=True, maxiter=10000, tol=1e-6)\n",
    "    #solver = jaxopt.GradientDescent(optfunc, value_and_grad=True, maxiter=10000, tol=1e-6)\n",
    "\n",
    "    #args = input_data, WL, bias, target, structure_shape, beta\n",
    "\n",
    "    res = solver.run(variable_phase_0)\n",
    "\n",
    "    variable_phase = res.params\n",
    "    phase = jnp.concatenate((input_data, variable_phase))\n",
    "\n",
    "    return phase\n",
    "\n",
    "@jax.jit\n",
    "def opt_run_layered_network(phase_0, WL, bias, target, structure_shape, beta):\n",
    "    runfunc = lambda phase_0, target: opt_single_layer_run_network(phase_0, WL, bias, target, structure_shape, beta)\n",
    "    phase = jax.tree_map(runfunc, list(phase_0), list(target))\n",
    "    return jnp.asarray(phase)\n",
    "\n",
    "@jax.jit\n",
    "def v_opt_run_layered_network(phase_0, WL, bias, target, structure_shape, beta):\n",
    "    runfunc = lambda phase_0, target: opt_single_layer_run_network(phase_0, WL, bias, target, structure_shape, beta)\n",
    "    phase = jax.vmap(runfunc, (0,0))(phase_0, target)\n",
    "    return phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825ae807",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#====================================Run the network with lattice structure=================================\n",
    "#--------------------------------------Evolve the system with solving ODE-----------------------------------\n",
    "# moving phase\n",
    "@jax.jit\n",
    "def move_phase(phase):\n",
    "    phase_shape = phase.shape\n",
    "    sub_row = jnp.zeros([1, phase_shape[1]])\n",
    "    sub_col = jnp.zeros([phase_shape[0], 1])\n",
    "    \n",
    "    #phase_u[i,j] = phase[i+1, j]\n",
    "    phase_u = jnp.delete(phase, 0, axis=0)\n",
    "    #phase_u = jnp.concatenate((phase_u, sub_row), axis=0)\n",
    "    phase_u = jnp.concatenate((phase_u, phase[-1,:][None,:]), axis=0)\n",
    "    \n",
    "    #phase_d[i,j] = phase[i-1, j]\n",
    "    phase_d = jnp.delete(phase, phase_shape[0]-1, axis=0)\n",
    "    #phase_d = jnp.concatenate((sub_row, phase_d), axis=0)\n",
    "    phase_d = jnp.concatenate((phase[0,:][None,:], phase_d), axis=0)\n",
    "    \n",
    "    #phase_l[i,j] = phase[i, j+1]\n",
    "    phase_l = jnp.delete(phase, 0, axis=1)\n",
    "    #phase_l = jnp.concatenate((phase_l, sub_col), axis=1)\n",
    "    phase_l = jnp.concatenate((phase_l, phase[:,-1][:,None]), axis=1)\n",
    "    \n",
    "    #phase_r[i,j] = phase[i, j-1]\n",
    "    phase_r = jnp.delete(phase, phase_shape[1]-1, axis=1)\n",
    "    #phase_r = jnp.concatenate((sub_col, phase_r), axis=1)\n",
    "    phase_r = jnp.concatenate((phase[:,0][:,None], phase_r), axis=1)\n",
    "    \n",
    "    return jnp.asarray([phase_u, phase_d, phase_l, phase_r])\n",
    "\n",
    "@jax.jit\n",
    "def SquareLattice_total_energy(flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    dimension = lattice_form.shape\n",
    "    phase = jnp.reshape(flatten_phase, dimension)\n",
    "    moved_phase = move_phase(phase)\n",
    "    E0 = -0.5 * jnp.sum(W * jnp.cos(phase - moved_phase))\n",
    "    \n",
    "    h = bias[0,...]\n",
    "    psi = bias[1,...]\n",
    "    \n",
    "    E1 = -jnp.sum(h*jnp.cos(phase - psi))\n",
    "    \n",
    "    phase_out = phase[-1, output_index]\n",
    "    E2 = -jnp.sum(jnp.log(1+jnp.cos(phase_out - target)))\n",
    "    \n",
    "    return E0 + E1 + beta*E2\n",
    "\n",
    "@jax.jit\n",
    "def SquareLattice_total_force(t, flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    dimension = lattice_form.shape\n",
    "    phase = jnp.reshape(flatten_phase, dimension)\n",
    "    moved_phase = move_phase(phase)\n",
    "    F0 = -jnp.sum(W * jnp.sin(phase - moved_phase), axis=0)\n",
    "    \n",
    "    h = bias[0,...]\n",
    "    psi = bias[1,...]\n",
    "    F1 = -h*jnp.sin(phase - psi)\n",
    "    \n",
    "    row_output_index = output_index - (dimension[0]-1)*dimension[1]\n",
    "    \n",
    "    phase_out = phase[-1, row_output_index]\n",
    "    F2 = -jnp.sin(phase_out - target)/(1+jnp.cos(phase_out - target))\n",
    "    \n",
    "    F = F1 + F0\n",
    "    \n",
    "    F_out = F[-1, row_output_index]\n",
    "    F_out = F_out + F2*beta\n",
    "    \n",
    "    F = F.at[0,input_index].set(0)\n",
    "    F = F.at[-1,row_output_index].set(F_out)\n",
    "    \n",
    "    return jnp.concatenate(F)\n",
    "\n",
    "@jax.jit\n",
    "def single_lattice_run_network(phase_0, T, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    #run network with layer structure for single inputs\n",
    "    # Solve the equation with diffrax\n",
    "\n",
    "    # Transform a lattice configuration into a sequence\n",
    "    flatten_phase_0 = jnp.concatenate(phase_0)\n",
    "    \n",
    "    # Set parameter for diffrax\n",
    "    rtol = 1e-8\n",
    "    atol = 1e-8\n",
    "    t_span = [0,T]\n",
    "    #saveat = np.linspace(*t_span, 10).tolist()\n",
    "    odefunc = lambda t, flatten_phase_0, args: SquareLattice_total_force(t, flatten_phase_0, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    eqs = diffrax.ODETerm(odefunc)\n",
    "    \n",
    "    # Use 4th Runge Kutta\n",
    "    solver = diffrax.Tsit5()\n",
    "    \n",
    "    #Use 5th Kvaerno for stiff case\n",
    "    #solver = diffrax.Kvaerno4()\n",
    "    \n",
    "    stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
    "    #t = diffrax.SaveAt(ts=saveat)\n",
    "    \n",
    "    # Solve the ODE\n",
    "    solution = diffrax.diffeqsolve(eqs, solver, t0=t_span[0], t1=t_span[1], dt0 = None, y0=flatten_phase_0,\n",
    "                               stepsize_controller=stepsize_controller, max_steps=10000000)\n",
    "    \n",
    "    L = len(solution.ts)\n",
    "    \n",
    "    phase = solution.ys[L-1,:].reshape(phase_0.shape)\n",
    "\n",
    "    return phase\n",
    "\n",
    "v_run_lattice_network = jax.vmap(single_lattice_run_network, (0,None,None,None,0,None,None,None,None))\n",
    "\n",
    "#--------------------------------------Evolve the system via optimization methods-----------------------------------------\n",
    "@jax.jit\n",
    "def opt_SquareLattice_total_energy(variable_phase, input_data, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    flatten_phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    E = SquareLattice_total_energy(flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    return E\n",
    "\n",
    "@jax.jit\n",
    "def opt_SquareLattice_total_force(variable_phase, input_data, W, bias, target, lattice_form, input_index, output_index, beta):\n",
    "    flatten_phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    F = SquareLattice_total_force(0, flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    return F\n",
    "\n",
    "@jax.jit\n",
    "def opt_SquareLattice_energy_with_grad(variable_phase, input_data, W, bias, target, lattice_form, input_index, variable_index, output_index, beta):\n",
    "    flatten_phase = jnp.concatenate((input_data, variable_phase), axis=0)\n",
    "    E = SquareLattice_total_energy(flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    F = SquareLattice_total_force(0, flatten_phase, W, bias, target, lattice_form, input_index, output_index, beta)\n",
    "    return E, -F[variable_index]\n",
    "\n",
    "@jax.jit\n",
    "def opt_single_SquareLattice_run_network(phase_0, W, bias, target, lattice_form, input_index, variable_index, output_index, beta):\n",
    "    # Run network for single set of input data\n",
    "    \n",
    "    flatten_phase_0 = jnp.concatenate(phase_0)\n",
    "    \n",
    "    input_data = flatten_phase_0[input_index]\n",
    "    variable_phase_0 = flatten_phase_0[variable_index]\n",
    "    \n",
    "    optfunc = lambda variable_phase: opt_SquareLattice_energy_with_grad(variable_phase, input_data, W, bias, target, lattice_form, input_index, variable_index, output_index, beta)\n",
    "    \n",
    "    solver = jaxopt.LBFGS(optfunc, value_and_grad=True, maxiter=10000, tol=1e-6)\n",
    "    #solver = jaxopt.GradientDescent(optfunc, value_and_grad=True, maxiter=10000, tol=1e-6)\n",
    "\n",
    "    #args = input_data, WL, bias, target, structure_shape, beta\n",
    "\n",
    "    res = solver.run(variable_phase_0)\n",
    "\n",
    "    variable_phase = res.params\n",
    "    flatten_phase = jnp.concatenate((input_data, variable_phase))\n",
    "\n",
    "    return jnp.reshape(flatten_phase, phase_0.shape)\n",
    "\n",
    "v_opt_run_SquareLattice_network = jax.vmap(opt_single_SquareLattice_run_network, (0,None,None,0,None,None,None,None,None))\n",
    "\n",
    "#-----------------------------------Calculate the gradient over parameters with EP-----------------------------------------\n",
    "\n",
    "@jax.jit\n",
    "def single_SquareLattice_paras_gradient(equi_nudge, equi_zero, bias, beta):\n",
    "    #Here equi_nudge and equi_zero are all configuration for a single phase\n",
    "    # Calculate gradient over weights\n",
    "    moved_equi_zero = move_phase(equi_zero)\n",
    "    dp_zero = equi_zero - moved_equi_zero\n",
    "    \n",
    "    moved_equi_nudge = move_phase(equi_nudge)\n",
    "    dp_nudge = equi_nudge - moved_equi_nudge\n",
    "    \n",
    "    gW = -jnp.cos(dp_nudge) + jnp.cos(dp_zero)\n",
    "    \n",
    "    # Calculate the gradient over bias\n",
    "    h = bias[0,...]\n",
    "    psi = bias[1,...]\n",
    "    \n",
    "    g_h = jnp.cos(equi_zero-psi) - jnp.cos(equi_nudge-psi)\n",
    "    g_psi = h*jnp.sin(equi_zero-psi) - h*jnp.sin(equi_nudge-psi)\n",
    "    \n",
    "    gh = jnp.asarray([g_h, g_psi])\n",
    "    \n",
    "    return gW/beta, gh/beta\n",
    "\n",
    "\n",
    "def SquareLattice_EP_param_gradient(W_0, bias_0, phase_0, training_target, training_paras, model_paras):\n",
    "    \n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, lattice_form, input_index, variable_index, output_index = model_paras\n",
    "    \n",
    "    N = bias_0.shape[1]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    batch_size = phase_0.shape[0]\n",
    "    batch_training_target = jnp.tensordot(jnp.ones(batch_size), training_target, 0)\n",
    "    \n",
    "    #equi_zero = v_opt_run_SquareLattice_network(jnp.concatenate(phase_0), W_0, bias_0, jnp.concatenate(batch_training_target), lattice_form, input_index, variable_index, output_index, 0)\n",
    "    #equi_nudge = v_opt_run_SquareLattice_network(equi_zero, W_0, bias_0, jnp.concatenate(batch_training_target), lattice_form, input_index, variable_index, output_index, beta)\n",
    "\n",
    "    equi_zero = v_run_lattice_network(jnp.concatenate(phase_0), T, W_0, bias_0, jnp.concatenate(batch_training_target), lattice_form, input_index, output_index, 0)\n",
    "    equi_nudge = v_run_lattice_network(equi_zero, T, W_0, bias_0, jnp.concatenate(batch_training_target), lattice_form, input_index, output_index, beta)\n",
    "\n",
    "    gWL, ghL = jax.vmap(single_SquareLattice_paras_gradient, in_axes=(0,0,None,None), out_axes=(0,0))(equi_nudge, equi_zero, bias_0, beta)\n",
    "    \n",
    "    gW = jnp.mean(gWL, axis=0)\n",
    "    gh = jnp.mean(ghL, axis=0)\n",
    "    \n",
    "    flatten_equi_zero = jax.vmap(jnp.concatenate)(equi_zero)\n",
    "    \n",
    "    cost_list = cost_function(flatten_equi_zero, jnp.concatenate(batch_training_target), output_index)\n",
    "    cost = jnp.mean(cost_list)\n",
    "    \n",
    "    q_cost_list = qualitative_cost(flatten_equi_zero, jnp.concatenate(batch_training_target), output_index, 0.1)\n",
    "    q_cost = jnp.mean(q_cost_list)\n",
    "    \n",
    "    return gW, gh, cost, q_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbcf2537",
   "metadata": {
    "id": "fbcf2537",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#===================Step by step update===============\n",
    "#--------------------------------initialize the state------------------------------\n",
    "def init_phase(N, input_index, input_data, batch_size):\n",
    "    N_data = input_data.shape[0]\n",
    "    phase_0 = np.random.rand(np.max([batch_size,1]), N_data, N) * (batch_size > 0)\n",
    "    phase_0[:,:,input_index] = input_data\n",
    "    \n",
    "    return jnp.asarray(phase_0)\n",
    "\n",
    "@jax.jit\n",
    "def jax_init_phase(phase_form, input_index, input_data, batch_form, seed, from_zero=False):\n",
    "    N_data, N = phase_form.shape\n",
    "    batch_size = batch_form.shape[0]\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    BS = jnp.max(jnp.asarray([1, batch_size]))\n",
    "    phase_0 = jax.random.uniform(key, shape=(batch_size, N_data, N), dtype=jnp.float64, minval=-jnp.pi, maxval=jnp.pi) * (1 - (from_zero==True))\n",
    "    #phase_0 = np.random.rand(np.max([batch_size,1]), N_data, N) * (batch_size > 0)\n",
    "    phase_0 = phase_0.at[:,:,input_index].set(input_data)\n",
    "    \n",
    "    return jnp.asarray(phase_0)\n",
    "\n",
    "@jax.jit\n",
    "def lattice_jax_init_phase(phase_form, input_index, input_data, batch_form, seed, from_zero=False):\n",
    "    N_data, *dim = phase_form.shape\n",
    "    batch_size = batch_form.shape[0]\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    phase_0 = jax.random.uniform(key, shape=(batch_size, N_data, *dim), dtype=jnp.float64, minval=-jnp.pi, maxval=jnp.pi) * (1 - (from_zero==True))\n",
    "    #phase_0 = np.random.rand(np.max([batch_size,1]), N_data, N) * (batch_size > 0)\n",
    "    phase_0 = phase_0.at[...,0,input_index].set(input_data)\n",
    "    \n",
    "    return jnp.asarray(phase_0)\n",
    "\n",
    "#------------------------------Calculate gradient with EP---------------------------\n",
    "@jax.jit\n",
    "def EP_param_gradient(W_0, bias_0, phase_0, training_target, training_paras, model_paras):\n",
    "    # Initialize the system\n",
    "    #batch_size = batch_form.shape[0]\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, input_index, variable_index, output_index = model_paras\n",
    "    N_data = jnp.shape(training_target)[0]\n",
    "    N = jnp.shape(W_0)[1]\n",
    "    T = N_ev * dt\n",
    "    \n",
    "    #initial phase\n",
    "    batch_size = phase_0.shape[0]\n",
    "    batch_training_target = jnp.tensordot(jnp.ones(batch_size), training_target, 0)\n",
    "    \n",
    "    '''\n",
    "    equi_zero = v_opt_run_network(jnp.concatenate(phase_0), W_0, bias_0, jnp.concatenate(batch_training_target), 0, input_index, variable_index, output_index)\n",
    "    if jnp.isnan(jnp.sum(equi_zero)):\n",
    "        equi_zero = v_run_network(jnp.concatenate(phase_0), T, W_0, bias_0, jnp.concatenate(batch_training_target), 0, input_index, output_index)\n",
    "        \n",
    "    equi_nudge = v_opt_run_network(equi_zero, W_0, bias_0, jnp.concatenate(batch_training_target), beta, input_index, variable_index, output_index)\n",
    "    if jnp.isnan(jnp.sum(equi_nudge)):\n",
    "        equi_nudge = v_run_network(equi_zero, T, W_0, bias_0, jnp.concatenate(batch_training_target), beta, input_index, output_index)\n",
    "    '''\n",
    "    equi_zero = v_run_network(jnp.concatenate(phase_0), T, W_0, bias_0, jnp.concatenate(batch_training_target), 0, input_index, output_index)\n",
    "    equi_nudge = v_run_network(equi_zero, T, W_0, bias_0, jnp.concatenate(batch_training_target), beta, input_index, output_index)\n",
    "    \n",
    "    cost_list = cost_function(equi_zero, jnp.concatenate(batch_training_target), output_index)\n",
    "    cost = jnp.mean(cost_list)\n",
    "    \n",
    "    q_cost_list = qualitative_cost(equi_zero, jnp.concatenate(batch_training_target), output_index, 0.1)\n",
    "    q_cost = jnp.mean(q_cost_list)\n",
    "    \n",
    "    gW, gh = paras_gradient(equi_nudge, equi_zero, bias_0, beta)\n",
    "    \n",
    "\n",
    "    # Formulate the output\n",
    "\n",
    "\n",
    "    return gW/beta, gh/beta, cost, q_cost\n",
    "\n",
    "#---------------------------------Calculate parameter gradient with EP with layer structure-------------------------------\n",
    "@jax.jit\n",
    "def weights_deriv_in_layer(phase1, phase2):\n",
    "    gW_loc = -jnp.cos(phase2[:,None,:] - jnp.transpose(phase1[:,None,:], axes=(0,2,1)))\n",
    "    \n",
    "    return jnp.mean(gW_loc, axis=0)\n",
    "\n",
    "@jax.jit\n",
    "def weights_gradient_in_layer(phase, structure_shape):\n",
    "    split_points = jax.tree_map(jnp.size, structure_shape)\n",
    "    phase_list = jnp.split(phase, split_points, axis=1)\n",
    "    \n",
    "    phase1_list = phase_list.copy()\n",
    "    phase2_list = phase_list.copy()\n",
    "    \n",
    "    del phase1_list[-1]\n",
    "    del phase2_list[0]\n",
    "    \n",
    "    return jax.tree_map(weights_deriv_in_layer, phase1_list, phase2_list)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def layer_paras_gradient(equi_nudge, equi_zero, bias, structure_shape, beta):\n",
    "    \n",
    "    #Calculate gradient over bias\n",
    "    h = bias[0,:]\n",
    "    psi = bias[1,:]\n",
    "\n",
    "    h = jnp.tensordot(jnp.ones(jnp.shape(equi_zero)[0]), h, 0)\n",
    "    psi = jnp.tensordot(jnp.ones(jnp.shape(equi_zero)[0]), psi, 0)\n",
    "\n",
    "    g_h = jnp.cos(equi_zero-psi) - jnp.cos(equi_nudge-psi)\n",
    "    g_psi = h*jnp.sin(equi_zero-psi) - h*jnp.sin(equi_nudge-psi)\n",
    "\n",
    "    g_h = jnp.mean(g_h,axis=0)\n",
    "    g_psi = jnp.mean(g_psi,axis=0)\n",
    "\n",
    "    gh = jnp.asarray([g_h, g_psi])\n",
    "    \n",
    "    #Calculate gradient over coupling\n",
    "    dEdW_zero = weights_gradient_in_layer(equi_zero, structure_shape)\n",
    "    dEdW_nudge = weights_gradient_in_layer(equi_nudge, structure_shape)\n",
    "    \n",
    "    gW = jax.tree_map(jnp.subtract, dEdW_nudge, dEdW_zero)\n",
    "    \n",
    "    return jax.tree_map(jnp.divide, gW, list(beta*jnp.ones(len(gW)))), gh/beta\n",
    "        \n",
    "\n",
    "#@jax.jit\n",
    "def layered_EP_param_gradient(WL_0, bias_0, phase_0, training_target, training_paras, model_paras):\n",
    "    # Initialize the system\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, structure_list, split_points, structure_shape = model_paras\n",
    "    \n",
    "    N = bias_0.shape[1]\n",
    "    T = N_ev * dt\n",
    "\n",
    "    input_index = jnp.arange(0, structure_shape[0].shape[0])\n",
    "    output_index = jnp.arange(structure_shape[-1].shape[0], N)\n",
    "    \n",
    "    batch_size = phase_0.shape[0]\n",
    "    batch_training_target = jnp.tensordot(jnp.ones(batch_size), training_target, 0)\n",
    "    \n",
    "    '''\n",
    "    equi_zero = v_opt_run_layered_network(jnp.concatenate(phase_0), WL_0, bias_0, jnp.concatenate(batch_training_target), structure_shape, 0)\n",
    "    if jnp.isnan(jnp.sum(equi_zero)):\n",
    "        equi_zero = v_run_layered_network(jnp.concatenate(phase_0), T, WL_0, bias_0, jnp.concatenate(batch_training_target), structure_shape, 0)\n",
    "    \n",
    "    equi_nudge = v_opt_run_layered_network(equi_zero, WL_0, bias_0, jnp.concatenate(batch_training_target), structure_shape, beta)\n",
    "    if jnp.isnan(jnp.sum(equi_nudge)):\n",
    "        equi_nudge = v_run_layered_network(equi_zero, T, WL_0, bias_0, jnp.concatenate(batch_training_target), structure_shape, beta)\n",
    "\n",
    "    # Calculate the loss\n",
    "    cost_list = cost_function(equi_zero, batch_training_target, output_index)\n",
    "    cost = jnp.mean(cost_list)\n",
    "\n",
    "    q_cost_list = qualitative_cost(equi_zero, batch_training_target, output_index, tol=0.1)\n",
    "    q_cost = jnp.mean(q_cost_list)\n",
    "\n",
    "    '''\n",
    "    zero_run = lambda phase_0: v_run_layered_network(phase_0, T, WL_0, bias_0, training_target, structure_shape, 0)\n",
    "    equi_zero = jax.vmap(zero_run,(0))(phase_0)\n",
    "    \n",
    "    nudge_run = lambda phase_0: v_run_layered_network(phase_0, T, WL_0, bias_0, training_target, structure_shape, beta)\n",
    "    equi_nudge = jax.vmap(nudge_run, (0))(equi_zero)\n",
    "    \n",
    "    cost_list = jax.vmap(cost_function, (0, None, None))(equi_zero, training_target, output_index)\n",
    "    cost = jnp.mean(cost_list)\n",
    "\n",
    "    q_cost_list = jax.vmap(qualitative_cost, (0, None, None, None))(equi_zero, training_target, output_index, 0.1)\n",
    "    q_cost = jnp.mean(q_cost_list)\n",
    "    \n",
    "    equi_zero = jnp.concatenate(equi_zero, axis=0)\n",
    "    equi_nudge = jnp.concatenate(equi_nudge, axis=0)\n",
    "    \n",
    "\n",
    "    gW, gh = layer_paras_gradient(equi_nudge, equi_zero, bias_0, structure_shape, beta)\n",
    "    \n",
    "    return gW, gh, cost, q_cost\n",
    "\n",
    "\n",
    "#---------------------------------Calculate parameter gradient with EP with lattice structure-------------------------------\n",
    "\n",
    "    \n",
    "#================================Optimization Updates========================\n",
    "@jax.jit\n",
    "def gradient_descent_update(paras, g_paras, study_rate_0, itr_time, study_rate_f=0.001, decay=0):\n",
    "    eta = jnp.max(jnp.asarray([study_rate_0 - decay*itr_time, study_rate_f]))\n",
    "    return paras - eta * g_paras\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Adam_update(paras, g_paras, study_rate_0, itr_time, s, r, r1=0.9, r2=0.999, delta=1e-8, study_rate_f=0.001, decay=0):\n",
    "    '''\n",
    "        This implement update stem with adam. s is the momentum, r realizes adaptive stepsize\n",
    "    '''\n",
    "    # Get the study rate, epsilon in Adam\n",
    "    eta = jnp.max(jnp.asarray([study_rate_0 - decay*itr_time, study_rate_f]))\n",
    "    \n",
    "    new_t = itr_time + 1\n",
    "    new_s = r1 * s + (1 - r1) * g_paras\n",
    "    new_r = r2 * r + (1 - r2) * (g_paras * g_paras)\n",
    "    \n",
    "    s_hat = new_s/(1 - r1**new_t)\n",
    "    r_hat = new_r/(1 - r2**new_t)\n",
    "    \n",
    "    new_paras = paras - eta * s_hat / (delta + jnp.sqrt(r_hat))\n",
    "    \n",
    "    return new_paras, new_s, new_r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3fa1e",
   "metadata": {},
   "source": [
    "$Test the network with XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e010118",
   "metadata": {},
   "source": [
    "Calculate the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3408082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_US_model(model, N_epoch, batch_size, study_rate, training_data, training_target):\n",
    "    #test all-to-all connected network\n",
    "    from_zero = False\n",
    "    if batch_size == 0:\n",
    "        batch_size = 1\n",
    "        from_zero = True\n",
    "\n",
    "    N_data = training_data.shape[0]\n",
    "    batch_form = jnp.zeros(batch_size)\n",
    "    phase_form = jnp.zeros([N_data, model.N])\n",
    "\n",
    "    training_paras = model.beta, study_rate\n",
    "    model_paras = model.N_ev, model.dt, model.input_index, model.variable_index, model.output_index\n",
    "\n",
    "    phase_0 = init_phase(model.N, model.input_index, training_data, batch_size)\n",
    "    W_0 = model.weights_0\n",
    "    bias_0 = model.bias_0\n",
    "\n",
    "    WL = np.zeros([N_epoch+1, model.N, model.N])\n",
    "    biasL = np.zeros([N_epoch+1, 2, model.N])\n",
    "    costL = np.zeros(N_epoch)\n",
    "\n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    WL[0,...] = W_0\n",
    "    biasL[0,...] = bias_0\n",
    "\n",
    "    #prepare for Adam\n",
    "    s_W = 0*W\n",
    "    r_W = 0*W\n",
    "    s_h = 0*bias\n",
    "    r_h = 0*bias\n",
    "    \n",
    "    for k in range(0, N_epoch):\n",
    "        phase_0 = jax_init_phase(phase_form, model.input_index, training_data, batch_form, k, from_zero)\n",
    "        #phase_0 = init_phase(UA_model.N, UA_model.input_index, training_data, batch_size)\n",
    "        gW, gh, cost, q_cost = EP_param_gradient(W, bias, phase_0, training_target, training_paras, model_paras)\n",
    "        \n",
    "        W = gradient_descent_update(W, gW, study_rate, k)\n",
    "        bias = gradient_descent_update(bias, gh, study_rate, k)\n",
    "        \n",
    "        #W, s_W, r_W = Adam_update(W, gW, study_rate, k, s_W, r_W)\n",
    "        #bias, s_h, r_h = Adam_update(bias, gh, study_rate, k, s_h, r_h)\n",
    "        \n",
    "        costL[k] = cost\n",
    "    \n",
    "    return WL, biasL, costL\n",
    "\n",
    "\n",
    "def train_layered_model(model, N_epoch, batch_size, study_rate, training_data, training_target):\n",
    "    #test layered network with XOR\n",
    "\n",
    "    from_zero = False\n",
    "    if batch_size == 0:\n",
    "        batch_size = 1\n",
    "        from_zero = True\n",
    "\n",
    "    batch_form = jnp.zeros(batch_size)\n",
    "\n",
    "    training_paras = model.beta, study_rate\n",
    "    model_paras = model.N_ev, model.dt, model.structure_list, model.split_points, model.structure_shape\n",
    "\n",
    "    N_data = training_data.shape[0]\n",
    "    phase_form = jnp.zeros([N_data, model.N])\n",
    "    WL_0 = model.WL\n",
    "    bias_0 = model.bias_0\n",
    "\n",
    "    WLL_layer = []\n",
    "    biasL_layer = np.zeros([N_epoch+1, 2, model.N])\n",
    "    costL_layer = np.zeros(N_epoch)\n",
    "\n",
    "    WL = WL_0\n",
    "    bias = bias_0\n",
    "\n",
    "    WLL_layer = [WL_0]\n",
    "    biasL_layer[0,...] = bias_0\n",
    "\n",
    "    #prepare for Adam\n",
    "    WL_shape = jax.tree_map(jnp.shape, WL_0)\n",
    "\n",
    "    s_W = []\n",
    "    for shape in WL_shape:\n",
    "        s_W.append(jnp.zeros(shape))\n",
    "\n",
    "    r_W = s_W.copy()\n",
    "\n",
    "    s_h = jnp.zeros(bias_0.shape)\n",
    "    r_h = s_h\n",
    "    \n",
    "    depth = len(WL)\n",
    "    study_rate_list = [study_rate]\n",
    "    gWL = np.zeros([N_epoch, depth])\n",
    "    ngWL = np.zeros([N_epoch, depth])\n",
    "    \n",
    "    for k in range(1, depth-1):\n",
    "        study_rate_list.append(study_rate_list[-1]*1)\n",
    "        \n",
    "    study_rate_list.append(study_rate_list[-1] * 0.1)\n",
    "\n",
    "    for k in range(0, N_epoch):\n",
    "        phase_0 = jax_init_phase(phase_form, model.input_index, training_data, batch_form, k, from_zero)\n",
    "        gW, gh, cost, q_cost = layered_EP_param_gradient(WL, bias, phase_0, training_target, training_paras, model_paras)\n",
    "        \n",
    "        abs_gW = jax.tree_util.tree_map(jnp.linalg.norm, gW)\n",
    "        ngW = jax.tree_util.tree_map(jnp.divide, abs_gW, jax.tree_util.tree_map(jnp.linalg.norm, WL))\n",
    "        \n",
    "        ugW = jax.tree_util.tree_map(jnp.divide, gW, jax.tree_util.tree_map(jnp.linalg.norm, WL))\n",
    "        #ugW = jax.tree_util.tree_map(jnp.multiply, ugW, jax.tree_util.tree_map(jnp.sqrt, jax.tree_util.tree_map(jnp.size, WL) ))\n",
    "        \n",
    "        \n",
    "        #update_func = lambda paras, g_paras, study_rate_list: gradient_descent_update(paras, g_paras, study_rate_list, k)\n",
    "        #WL = jax.tree_util.tree_map(update_func, WL, gW, study_rate_list)\n",
    "        \n",
    "        '''\n",
    "        bias_list = jnp.split(bias, layer_model.split_points, axis=1)\n",
    "        gh_list = jnp.split(gh, layer_model.split_points, axis=1)\n",
    "        bias0 = bias_list[0]\n",
    "        del bias_list[0]\n",
    "        del gh_list[0]\n",
    "        ugh_list = jax.tree_util.tree_map(jnp.divide, gh_list, jax.tree_util.tree_map(jnp.linalg.norm, bias_list))\n",
    "        \n",
    "        bias_list = jax.tree_util.tree_map(update_func, bias_list, gh_list, study_rate_list)\n",
    "        bias_list.insert(0,bias0)\n",
    "        bias = jnp.concatenate(bias_list, axis=1)\n",
    "        '''\n",
    "        #bias = gradient_descent_update(bias, gh, study_rate, k)\n",
    "\n",
    "        '''\n",
    "        ugW = jax.tree_util.tree_map(jnp.divide, gW, jax.tree_util.tree_map(jnp.linalg.norm, WL))\n",
    "        bias_list = jnp.split(bias, layer_model.split_points, axis=1)\n",
    "        gh_list = jnp.split(gh, layer_model.split_points, axis=1)\n",
    "        bias0 = bias_list[0]\n",
    "        del bias_list[0]\n",
    "        del gh_list[0]\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        update_func = lambda paras, g_paras, s, r: Adam_update(paras, g_paras, study_rate, k, s, r)\n",
    "        bias, s_h, r_h = update_func(bias, gh, s_h, r_h)\n",
    "        res = jax.tree_map(update_func, WL, gW, s_W, r_W)\n",
    "        WL, s_W, r_W = tuple(zip(*res))\n",
    "        WL, s_W, r_W = list(WL), list(s_W), list(r_W)\n",
    "        \n",
    "        \n",
    "        WLL_layer.append(WL)\n",
    "        biasL_layer[k+1,...] = bias\n",
    "        costL_layer[k] = cost\n",
    "        gWL[k,:] = jnp.asarray(abs_gW)\n",
    "        ngWL[k,:] = jnp.asarray(ngW)\n",
    "         \n",
    "    return WLL_layer, biasL_layer, costL_layer, gWL, ngWL\n",
    "\n",
    "def train_squarelattice_model(model, N_epoch, batch_size, study_rate, training_data, training_target):\n",
    "    #test Square lattice network with XOR\n",
    "    from_zero = False\n",
    "    if batch_size == 0:\n",
    "        batch_size = 1\n",
    "        from_zero = True\n",
    "\n",
    "    batch_form = jnp.zeros(batch_size)\n",
    "\n",
    "    lattice_form = jnp.zeros(model.dimension)\n",
    "\n",
    "    training_paras = beta, study_rate\n",
    "    model_paras = N_ev, dt, lattice_form, model.input_index, model.variable_index, model.output_index\n",
    "\n",
    "    N_data = training_data.shape[0]\n",
    "    phase_form = jnp.zeros([N_data, *model.dimension])\n",
    "\n",
    "    W_0 = model.weights_0\n",
    "    bias_0 = model.bias_0\n",
    "\n",
    "    WL_lattice = []\n",
    "    biasL_lattice = []\n",
    "    costL_lattice = np.zeros(N_epoch)\n",
    "\n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    WL_lattice = [W_0]\n",
    "    biasL_lattice.append(bias_0)\n",
    "\n",
    "    #prepare for Adam\n",
    "    #prepare for Adam\n",
    "    s_W = 0*W\n",
    "    r_W = 0*W\n",
    "    s_h = 0*bias\n",
    "    r_h = 0*bias\n",
    "\n",
    "    for k in range(0, N_epoch):\n",
    "        phase_0 = lattice_jax_init_phase(phase_form, model.input_index, training_data, batch_form, k**2, from_zero)\n",
    "        gW, gh, cost, q_cost = SquareLattice_EP_param_gradient(W, bias, phase_0, training_target, training_paras, model_paras)\n",
    "        \n",
    "        W = gradient_descent_update(W, gW, study_rate/100, k)\n",
    "        bias = gradient_descent_update(bias, gh, study_rate/100, k)\n",
    "        \n",
    "        #W, s_W, r_W = Adam_update(W, gW, study_rate, k, s_W, r_W)\n",
    "        #bias, s_h, r_h = Adam_update(bias, gh, study_rate, k, s_h, r_h)\n",
    "        \n",
    "        \n",
    "        WL_lattice.append(W)\n",
    "        biasL_lattice.append(bias)\n",
    "        costL_lattice[k] = cost\n",
    "\n",
    "    return WL_lattice, biasL_lattice, costL_lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ee123fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.5, dtype=float64, weak_type=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.divide(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca3bc2",
   "metadata": {},
   "source": [
    "#Start doing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cecb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = jnp.pi/2 * jnp.asarray([[-1,-1],[-1,1],[1,-1],[1,1]])\n",
    "training_target = jnp.pi/2 * jnp.asarray([[-1],[1],[1],[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c86c27ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 13:30:22.939165: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1207] Failed to get stream capture info: out of memory\n",
      "2023-08-23 13:30:23.030828: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2432] Execution of replica 0 failed: INVALID_ARGUMENT: stream is uninitialized or in an error state\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INVALID_ARGUMENT: stream is uninitialized or in an error state",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m structure_list \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray([\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     24\u001b[0m N \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(structure_list)\n\u001b[0;32m---> 25\u001b[0m layer_model \u001b[38;5;241m=\u001b[39m \u001b[43mSP_XY_Layer_Network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m layer_model\u001b[38;5;241m.\u001b[39mrandom_state_initiation()\n",
      "Cell \u001b[0;32mIn[5], line 703\u001b[0m, in \u001b[0;36mSP_XY_Layer_Network.__init__\u001b[0;34m(self, N, N_ev, dt, structure_list)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_points[k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mstructure_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_points, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_points[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/array.py:343\u001b[0m, in \u001b[0;36mArrayImpl.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax_numpy\u001b[38;5;241m.\u001b[39m_rewriting_take(\u001b[38;5;28mself\u001b[39m, idx)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax_numpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rewriting_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3892\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mdynamic_slice_in_dim(arr, start, _max(\u001b[38;5;241m0\u001b[39m, stop \u001b[38;5;241m-\u001b[39m start), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3890\u001b[0m   \u001b[38;5;66;03m# Use dynamic rather than static slice here to avoid slow repeated execution:\u001b[39;00m\n\u001b[1;32m   3891\u001b[0m   \u001b[38;5;66;03m# See https://github.com/google/jax/issues/12198\u001b[39;00m\n\u001b[0;32m-> 3892\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamic_slice_in_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_max\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3894\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mslice_in_dim(arr, start, stop, step)\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/lax/slicing.py:698\u001b[0m, in \u001b[0;36mdynamic_slice_in_dim\u001b[0;34m(operand, start_index, slice_size, axis)\u001b[0m\n\u001b[1;32m    696\u001b[0m start_indices[axis] \u001b[38;5;241m=\u001b[39m start_index\n\u001b[1;32m    697\u001b[0m slice_sizes[axis] \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39m_canonicalize_dimension(slice_size)\n\u001b[0;32m--> 698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdynamic_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_sizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/lax/slicing.py:110\u001b[0m, in \u001b[0;36mdynamic_slice\u001b[0;34m(operand, start_indices, slice_sizes)\u001b[0m\n\u001b[1;32m    108\u001b[0m   dynamic_sizes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    109\u001b[0m   static_sizes \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mcanonicalize_shape(slice_sizes)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdynamic_slice_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstart_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdynamic_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mslice_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstatic_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/core.py:360\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    358\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    359\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 360\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/core.py:363\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 363\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/core.py:817\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 817\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/dispatch.py:129\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    125\u001b[0m   msg \u001b[38;5;241m=\u001b[39m pjit\u001b[38;5;241m.\u001b[39m_device_assignment_mismatch_error(\n\u001b[1;32m    126\u001b[0m       prim\u001b[38;5;241m.\u001b[39mname, fails, args, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m, arg_names)\n\u001b[1;32m    127\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/dispatch.py:211\u001b[0m, in \u001b[0;36mxla_primitive_callable.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    208\u001b[0m compiled \u001b[38;5;241m=\u001b[39m _xla_callable_uncached(lu\u001b[38;5;241m.\u001b[39mwrap_init(prim_fun), prim\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    209\u001b[0m                                   donated_invars, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39marg_specs)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prim\u001b[38;5;241m.\u001b[39mmultiple_results:\n\u001b[0;32m--> 211\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[43mcompiled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m compiled\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/conda-envs/EqProp2/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:1916\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1911\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(\n\u001b[1;32m   1912\u001b[0m       results\u001b[38;5;241m.\u001b[39mdisassemble_prefix_into_single_device_arrays(\n\u001b[1;32m   1913\u001b[0m           \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_effects)),\n\u001b[1;32m   1914\u001b[0m       results\u001b[38;5;241m.\u001b[39mconsume_token())\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1916\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1918\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INVALID_ARGUMENT: stream is uninitialized or in an error state"
     ]
    }
   ],
   "source": [
    "#Define the network\n",
    "N_ev = 1000\n",
    "dt = 0.1\n",
    "\n",
    "study_rate = 0.1\n",
    "beta = 0.1\n",
    "\n",
    "#Define the network with square lattice architecture\n",
    "dimension = [3,3]\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "lattice_model = SP_XY_SquareLattice_Network(dimension, N_ev, dt, input_size, output_size)\n",
    "lattice_model.random_state_initiation()\n",
    "\n",
    "#Define the network with all-to-all connection\n",
    "N = 5\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "\n",
    "#Define the network with layer stryctyre\n",
    "structure_list = jnp.asarray([2,5,5,5,1])\n",
    "N = jnp.sum(structure_list)\n",
    "layer_model = SP_XY_Layer_Network(N, N_ev, dt, structure_list)\n",
    "layer_model.random_state_initiation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a71c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experimental parameters\n",
    "N_task = 100\n",
    "N_epoch = 1000\n",
    "study_rate = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "UA_model.get_beta(beta)\n",
    "layer_model.get_beta(beta)\n",
    "lattice_model.get_beta(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205a786-b50b-4823-ac74-fbc6e258534c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target):\n",
    "    #test all-to-all connected network\n",
    "    from_zero = False\n",
    "    if batch_size == 0:\n",
    "        batch_size = 1\n",
    "        from_zero = True\n",
    "\n",
    "    beta, study_rate = training_paras\n",
    "    N_ev, dt, input_index, variable_index, output_index = model_paras\n",
    "    \n",
    "    N = input_index.shape[0] + variable_index.shape[0]\n",
    "    \n",
    "    N_data = training_data.shape[0]\n",
    "    batch_form = jnp.zeros(batch_size)\n",
    "    phase_form = jnp.zeros([N_data, N])\n",
    "\n",
    "    WL = jnp.zeros([N_epoch+1, N, N])\n",
    "    biasL = jnp.zeros([N_epoch+1, 2, N])\n",
    "    costL = jnp.zeros(N_epoch)\n",
    "\n",
    "    W = W_0\n",
    "    bias = bias_0\n",
    "\n",
    "    WL = WL.at[0,...].set(W_0)\n",
    "    biasL = biasL.at[0,...].set(bias_0)\n",
    "\n",
    "    #prepare for Adam\n",
    "    s_W = 0*W\n",
    "    r_W = 0*W\n",
    "    s_h = 0*bias\n",
    "    r_h = 0*bias\n",
    "    \n",
    "    for k in range(0, N_epoch):\n",
    "        phase_0 = jax_init_phase(phase_form, input_index, training_data, batch_form, k, from_zero)\n",
    "        #phase_0 = init_phase(UA_model.N, UA_model.input_index, training_data, batch_size)\n",
    "        gW, gh, cost, q_cost = EP_param_gradient(W, bias, phase_0, training_target, training_paras, model_paras)\n",
    "        \n",
    "        W = gradient_descent_update(W, gW, study_rate, k)\n",
    "        bias = gradient_descent_update(bias, gh, study_rate, k)\n",
    "        \n",
    "        #W, s_W, r_W = Adam_update(W, gW, study_rate, k, s_W, r_W)\n",
    "        #bias, s_h, r_h = Adam_update(bias, gh, study_rate, k, s_h, r_h)\n",
    "        \n",
    "        costL = costL.at[k].set(cost)\n",
    "    \n",
    "    return WL, biasL, costL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6598998-ed05-4b25-9aee-3f1c1d25f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experimental parameters\n",
    "N_task = 100\n",
    "N_epoch = 1000\n",
    "study_rate = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "UA_model.get_beta(beta)\n",
    "layer_model.get_beta(beta)\n",
    "lattice_model.get_beta(beta)\n",
    "\n",
    "W_0 = UA_model.weights_0\n",
    "bias_0 = UA_model.bias_0\n",
    "\n",
    "variable_index = UA_model.variable_index\n",
    "\n",
    "training_paras = beta, study_rate\n",
    "model_paras = N_ev, dt, input_index, variable_index, output_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95370e2b-f6ae-41c2-afa2-86dd70bd7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "\n",
    "v_train = jax.vmap(train_fun, (0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e260a7f-7754-4c91-88f4-84f50a79e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_task = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7f77b-de0e-47df-b1c9-61a9e5b10812",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "UA_model.get_beta(beta)\n",
    "\n",
    "all_cost_US3 = np.zeros([N_task, N_epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3037a9b-1088-4aff-b539-4b1ffb967eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W0L = np.zeros([N_task, N, N])\n",
    "bias0L = np.zeros([N_task, 2, N])\n",
    "\n",
    "W_0 = UA_model.weights_0\n",
    "bias_0 = UA_model.bias_0\n",
    "\n",
    "variable_index = UA_model.variable_index\n",
    "\n",
    "training_paras = beta, study_rate\n",
    "model_paras = N_ev, dt, input_index, variable_index, output_index\n",
    "\n",
    "for k in range(0, N_task):\n",
    "    UA_model.random_state_initiation()\n",
    "    W0L[k] = UA_model.weights_0\n",
    "    bias0L[k] = UA_model.bias_0\n",
    "\n",
    "W0L = jnp.asarray(W0L)\n",
    "bias0L = jnp.asarray(bias0L)\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "t0 = time.time()\n",
    "A, B, all_cost_US1 = v_train(W0L, bias0L)\n",
    "t1 = time.time()\n",
    "\n",
    "print('N=',N, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64437122-4ca9-4dd2-a1c0-9201fc3bbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "UA_model.get_beta(beta)\n",
    "\n",
    "all_cost_US2 = np.zeros([N_task, N_epoch])\n",
    "\n",
    "W0L = np.zeros([N_task, N, N])\n",
    "bias0L = np.zeros([N_task, 2, N])\n",
    "\n",
    "W_0 = UA_model.weights_0\n",
    "bias_0 = UA_model.bias_0\n",
    "\n",
    "variable_index = UA_model.variable_index\n",
    "\n",
    "training_paras = beta, study_rate\n",
    "model_paras = N_ev, dt, input_index, variable_index, output_index\n",
    "\n",
    "for k in range(0, N_task):\n",
    "    UA_model.random_state_initiation()\n",
    "    W0L[k] = UA_model.weights_0\n",
    "    bias0L[k] = UA_model.bias_0\n",
    "\n",
    "W0L = jnp.asarray(W0L)\n",
    "bias0L = jnp.asarray(bias0L)\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "t0 = time.time()\n",
    "A, B, all_cost_US2 = v_train(W0L, bias0L)\n",
    "t1 = time.time()\n",
    "\n",
    "print('N=',N, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a4f6a-7ed7-442a-afc7-164419c8aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "UA_model.get_beta(beta)\n",
    "\n",
    "all_cost_US3 = np.zeros([N_task, N_epoch])\n",
    "\n",
    "W0L = np.zeros([N_task, N, N])\n",
    "bias0L = np.zeros([N_task, 2, N])\n",
    "\n",
    "W_0 = UA_model.weights_0\n",
    "bias_0 = UA_model.bias_0\n",
    "\n",
    "variable_index = UA_model.variable_index\n",
    "\n",
    "training_paras = beta, study_rate\n",
    "model_paras = N_ev, dt, input_index, variable_index, output_index\n",
    "\n",
    "for k in range(0, N_task):\n",
    "    UA_model.random_state_initiation()\n",
    "    W0L[k] = UA_model.weights_0\n",
    "    bias0L[k] = UA_model.bias_0\n",
    "\n",
    "W0L = jnp.asarray(W0L)\n",
    "bias0L = jnp.asarray(bias0L)\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "t0 = time.time()\n",
    "A, B, all_cost_US3 = v_train(W0L, bias0L)\n",
    "t1 = time.time()\n",
    "\n",
    "print('N=',N, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720c3a0-18a5-4764-9bf2-7e93c81ad1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "UA_model.get_beta(beta)\n",
    "\n",
    "all_cost_US5 = np.zeros([N_task, N_epoch])\n",
    "\n",
    "W0L = np.zeros([N_task, N, N])\n",
    "bias0L = np.zeros([N_task, 2, N])\n",
    "\n",
    "W_0 = UA_model.weights_0\n",
    "bias_0 = UA_model.bias_0\n",
    "\n",
    "variable_index = UA_model.variable_index\n",
    "\n",
    "training_paras = beta, study_rate\n",
    "model_paras = N_ev, dt, input_index, variable_index, output_index\n",
    "\n",
    "for k in range(0, N_task):\n",
    "    UA_model.random_state_initiation()\n",
    "    W0L[k] = UA_model.weights_0\n",
    "    bias0L[k] = UA_model.bias_0\n",
    "\n",
    "W0L = jnp.asarray(W0L)\n",
    "bias0L = jnp.asarray(bias0L)\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "t0 = time.time()\n",
    "A, B, all_cost_US5 = v_train(W0L, bias0L)\n",
    "t1 = time.time()\n",
    "\n",
    "print('N=',N, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac215e6d-aaf6-4a20-b47e-d69e67fbed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "UA_model.get_beta(beta)\n",
    "\n",
    "all_cost_US10 = np.zeros([N_task, N_epoch])\n",
    "\n",
    "W0L = np.zeros([N_task, N, N])\n",
    "bias0L = np.zeros([N_task, 2, N])\n",
    "\n",
    "W_0 = UA_model.weights_0\n",
    "bias_0 = UA_model.bias_0\n",
    "\n",
    "variable_index = UA_model.variable_index\n",
    "\n",
    "training_paras = beta, study_rate\n",
    "model_paras = N_ev, dt, input_index, variable_index, output_index\n",
    "\n",
    "for k in range(0, N_task):\n",
    "    UA_model.random_state_initiation()\n",
    "    W0L[k] = UA_model.weights_0\n",
    "    bias0L[k] = UA_model.bias_0\n",
    "\n",
    "W0L = jnp.asarray(W0L)\n",
    "bias0L = jnp.asarray(bias0L)\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "t0 = time.time()\n",
    "A, B, all_cost_US10 = v_train(W0L, bias0L)\n",
    "t1 = time.time()\n",
    "\n",
    "print('N=',N, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae39b67-68db-45b2-9161-3ba603f11926",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "UA_model.get_beta(beta)\n",
    "\n",
    "all_cost_US20 = np.zeros([N_task, N_epoch])\n",
    "\n",
    "W0L = np.zeros([N_task, N, N])\n",
    "bias0L = np.zeros([N_task, 2, N])\n",
    "\n",
    "W_0 = UA_model.weights_0\n",
    "bias_0 = UA_model.bias_0\n",
    "\n",
    "variable_index = UA_model.variable_index\n",
    "\n",
    "training_paras = beta, study_rate\n",
    "model_paras = N_ev, dt, input_index, variable_index, output_index\n",
    "\n",
    "for k in range(0, N_task):\n",
    "    UA_model.random_state_initiation()\n",
    "    W0L[k] = UA_model.weights_0\n",
    "    bias0L[k] = UA_model.bias_0\n",
    "\n",
    "W0L = jnp.asarray(W0L)\n",
    "bias0L = jnp.asarray(bias0L)\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "t0 = time.time()\n",
    "A, B, all_cost_US20 = v_train(W0L, bias0L)\n",
    "t1 = time.time()\n",
    "\n",
    "print('N=',N, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551bc35-e637-4d37-8a8f-144a4442645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "input_index = jnp.asarray([0,1])\n",
    "output_index = jnp.asarray([N-1])\n",
    "UA_model = SP_XY_Network(N, N_ev, dt, input_index, output_index)\n",
    "UA_model.random_state_initiation()\n",
    "UA_model.get_beta(beta)\n",
    "\n",
    "all_cost_US30 = np.zeros([N_task, N_epoch])\n",
    "\n",
    "W0L = np.zeros([N_task, N, N])\n",
    "bias0L = np.zeros([N_task, 2, N])\n",
    "\n",
    "W_0 = UA_model.weights_0\n",
    "bias_0 = UA_model.bias_0\n",
    "\n",
    "variable_index = UA_model.variable_index\n",
    "\n",
    "training_paras = beta, study_rate\n",
    "model_paras = N_ev, dt, input_index, variable_index, output_index\n",
    "\n",
    "for k in range(0, N_task):\n",
    "    UA_model.random_state_initiation()\n",
    "    W0L[k] = UA_model.weights_0\n",
    "    bias0L[k] = UA_model.bias_0\n",
    "\n",
    "W0L = jnp.asarray(W0L)\n",
    "bias0L = jnp.asarray(bias0L)\n",
    "\n",
    "train_fun = lambda W_0, bias_0: single_train_US_model(W_0, bias_0, model_paras, training_paras, N_epoch, batch_size, study_rate, training_data, training_target)\n",
    "v_train = jax.vmap(train_fun, (0,0))\n",
    "\n",
    "t0 = time.time()\n",
    "A, B, all_cost_US30 = v_train(W0L, bias0L)\n",
    "t1 = time.time()\n",
    "\n",
    "print('N=',N, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6d928-61cc-4b64-aae0-238f0d52fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [1,2,3,5,10,20,30]\n",
    "all_cost = np.asarray([all_cost_US1, all_cost_US2, all_cost_US3, all_cost_US5, all_cost_US10, \n",
    "                       all_cost_US20, all_cost_US30])\n",
    "\n",
    "np.savetxt('N={0}_all_cost'.format(N), all_cost, delimiter = ',')\n",
    "np.savetxt('batch_size_list', batch_size_list, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc05da-98c4-425a-aa5b-6d6fbf9b7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, N_epoch)*np.sqrt(1), np.mean(all_cost_US1, axis=0), linewidth=1, label='1')\n",
    "plt.plot(np.arange(0, N_epoch)*np.sqrt(2), np.mean(all_cost_US2, axis=0), linewidth=1, label='2')\n",
    "plt.plot(np.arange(0, N_epoch)*np.sqrt(3), np.mean(all_cost_US3, axis=0), linewidth=1, label='3')\n",
    "plt.plot(np.arange(0, N_epoch)*np.sqrt(5), np.mean(all_cost_US5, axis=0), linewidth=1, label='5')\n",
    "plt.plot(np.arange(0, N_epoch)*np.sqrt(10), np.mean(all_cost_US10, axis=0), linewidth=1, label='10')\n",
    "plt.plot(np.arange(0, N_epoch)*np.sqrt(20), np.mean(all_cost_US20, axis=0), linewidth=1, label='20')\n",
    "\n",
    "plt.xlim(0,1000)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a754c0f-15a9-49ad-99b5-6eb2724fce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, N_epoch) * 1, np.mean(all_cost_US1, axis=0), linewidth=1, label='1')\n",
    "plt.plot(np.arange(0, N_epoch) * 2, np.mean(all_cost_US2, axis=0), linewidth=1, label='2')\n",
    "plt.plot(np.arange(0, N_epoch) * 3, np.mean(all_cost_US3, axis=0), linewidth=1, label='3')\n",
    "plt.plot(np.arange(0, N_epoch) * 5, np.mean(all_cost_US5, axis=0), linewidth=1, label='5')\n",
    "plt.plot(np.arange(0, N_epoch) * 10, np.mean(all_cost_US10, axis=0), linewidth=1, label='10')\n",
    "plt.plot(np.arange(0, N_epoch) * 20, np.mean(all_cost_US20, axis=0), linewidth=1, label='20')\n",
    "\n",
    "plt.xlim(0,1000)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f184d5f-bbc2-4fe5-925f-36f75e7accfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US1, axis=0)), linewidth=1, label='1')\n",
    "plt.plot(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US2, axis=0)), linewidth=1, label='2')\n",
    "plt.plot(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US3, axis=0)), linewidth=1, label='3')\n",
    "plt.plot(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US5, axis=0)), linewidth=1, label='5')\n",
    "plt.plot(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US10, axis=0)), linewidth=1, label='10')\n",
    "plt.plot(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US20, axis=0)), linewidth=1, label='20')\n",
    "\n",
    "plt.xlim(0,1000)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cebef-f536-4ec1-ac2f-81cb8eaadca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d5217-44bf-4c84-9f5e-9ff8d399dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = stats.linregress(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US1, axis=0)))\n",
    "res2 = stats.linregress(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US2, axis=0)))\n",
    "res3 = stats.linregress(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US3, axis=0)))\n",
    "res5 = stats.linregress(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US5, axis=0)))\n",
    "res10 = stats.linregress(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US10, axis=0)))\n",
    "res20 = stats.linregress(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US20, axis=0)))\n",
    "res30 = stats.linregress(np.arange(0, N_epoch), np.log10(np.mean(all_cost_US30, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9008e0-38ce-4a85-bcf2-363855c47f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86882a8d-e41a-4659-87b9-8c63a86ee17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([-res1[0], -res2[0], -res3[0], -res5[0], -res10[0], -res20[0], -res30[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428066f9-6011-468c-9fed-6df6b1eed776",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.asarray([[1,2,3,5,10,20,30], [-res1[0], -res2[0], -res3[0], -res5[0], -res10[0], -res20[0], -res30[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de358c-0f96-4e60-89fc-89dbaae4334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('N=5', output, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee61df-1aa8-4819-8898-daa9b612bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay5 = [2.118332785439846089e-03,3.485313508407480478e-03,2.725419781522187869e-03,\n",
    "          3.772021258650624324e-03,3.575820172821611142e-03,3.376713731825767457e-03,3.431455878637905410e-03]\n",
    "\n",
    "decay20 = [7.427673204958776384e-04,1.688504573651377011e-03,2.192812334178094631e-03,2.860170920171466354e-03,\n",
    "           3.231884957736510730e-03,2.943565754468945635e-03,3.313970231557502623e-03]\n",
    "\n",
    "decay30 = [4.685640741984679349e-04,1.358629395691381006e-03,1.513321193117669406e-03,1.857330323048346672e-03,\n",
    "           2.734086534248033554e-03,2.245442585579424197e-03,2.867965794294211231e-03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf8b21-2cfe-4ab1-8593-2017744daa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1,2,3,5,10,20,30], decay5)\n",
    "plt.plot([1,2,3,5,10,20,30], decay20)\n",
    "plt.plot([1,2,3,5,10,20,30], decay30)\n",
    "\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ae91b-f3fc-4306-926f-24e46b0ba52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([1,2,3,5,10,20,30])\n",
    "plt.plot([1,2,3,5,10,20,30], np.asarray(decay5)/X)\n",
    "plt.plot([1,2,3,5,10,20,30], np.asarray(decay20)/X)\n",
    "plt.plot([1,2,3,5,10,20,30], np.asarray(decay30)/X)\n",
    "\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed87f4e-e83b-4d12-8157-8e7ffadf48b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "EqProp2",
   "language": "python",
   "name": "eqprop2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
